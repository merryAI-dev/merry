# ê°•í™”í•™ìŠµ (Reinforcement Learning) í™œìš© ê°€ì´ë“œ

## ê°œìš”

VC Investment AgentëŠ” ì‚¬ìš©ì í”¼ë“œë°±ì„ ìˆ˜ì§‘í•˜ì—¬ **ì§€ì†ì ìœ¼ë¡œ ê°œì„ **ë˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ì™œ ê°•í™”í•™ìŠµì¸ê°€?

1. **ì‚¬ìš©ì ë§ì¶¤í˜• í•™ìŠµ**: ì‹¤ì œ ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì„±ëŠ¥ í–¥ìƒ
2. **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ì–´ë–¤ ì§ˆë¬¸/ì‘ë‹µ íŒ¨í„´ì´ ì¢‹ì€ í‰ê°€ë¥¼ ë°›ëŠ”ì§€ í•™ìŠµ
3. **ë„êµ¬ ì‚¬ìš© ìµœì í™”**: ì–´ë–¤ ë„êµ¬ ì¡°í•©ì´ ìµœì ì˜ ê²°ê³¼ë¥¼ ë‚´ëŠ”ì§€ íŒŒì•…
4. **ìë™í™”ëœ ê°œì„ **: í”¼ë“œë°± ë°ì´í„°ê°€ ìŒ“ì¼ìˆ˜ë¡ ìë™ìœ¼ë¡œ ë” ë˜‘ë˜‘í•´ì§

---

## í”¼ë“œë°± ìˆ˜ì§‘ íë¦„

```
ì‚¬ìš©ì ì§ˆë¬¸
    â†“
ë©”ë¦¬ ì‘ë‹µ ìƒì„±
    â†“
ì‚¬ìš©ì í”¼ë“œë°± (ğŸ‘/ğŸ‘)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. JSONL íŒŒì¼ ì €ì¥             â”‚
â”‚  2. SQLite DB ì €ì¥ (í†µí•© ê´€ë¦¬) â”‚
â”‚  3. ë³´ìƒ ì ìˆ˜ ê³„ì‚° (-1 ~ 1)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ê°•í™”í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
    â†“
í”„ë¡¬í”„íŠ¸/ì‘ë‹µ íŒ¨í„´ ë¶„ì„
```

---

## ë°ì´í„° êµ¬ì¡°

### 1. í”¼ë“œë°± ë°ì´í„° (`feedback/feedback_data.jsonl`)

```json
{
  "id": "20251215_143000_123456",
  "timestamp": "2025-12-15T14:30:00",
  "user_message": "temp/íˆ¬ìê²€í† .xlsxë¥¼ 2030ë…„ PER 10ë°°ë¡œ ë¶„ì„í•´ì¤˜",
  "assistant_response": "ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. IRR 35.2%...",
  "feedback_type": "thumbs_up",
  "context": {
    "tools_used": ["read_excel_as_text", "analyze_and_generate_projection"]
  },
  "reward": 1.0
}
```

### 2. ê°•í™”í•™ìŠµ ë°ì´í„°ì…‹ (`feedback/rl_dataset.jsonl`)

```json
{
  "prompt": "temp/íˆ¬ìê²€í† .xlsxë¥¼ 2030ë…„ PER 10ë°°ë¡œ ë¶„ì„í•´ì¤˜",
  "response": "ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. IRR 35.2%...",
  "reward": 1.0,
  "tools_used": ["read_excel_as_text", "analyze_and_generate_projection"],
  "timestamp": "2025-12-15T14:30:00"
}
```

### 3. SQLite ë°ì´í„°ë² ì´ìŠ¤ (`feedback/feedback.db`)

**í…Œì´ë¸” êµ¬ì¡°:**

- `feedbacks`: ëª¨ë“  í”¼ë“œë°± ê¸°ë¡
- `session_stats`: ì„¸ì…˜ë³„ í†µê³„
- `rl_dataset`: ê°•í™”í•™ìŠµ í›ˆë ¨ìš© ë°ì´í„°

**ì™œ DBë¥¼ ì¶”ê°€í–ˆë‚˜?**
- JSONLì€ ë¹ ë¥´ê²Œ ìŒ“ì´ì§€ë§Œ ì¿¼ë¦¬ê°€ ëŠë¦¼
- DBëŠ” ë³µì¡í•œ ë¶„ì„ (í†µê³„, íŒ¨í„´ ë¶„ì„) ê°€ëŠ¥
- ì „ì²´ ì¡°ì§ì˜ í”¼ë“œë°±ì„ í†µí•© ê´€ë¦¬

---

## ê°•í™”í•™ìŠµ í™œìš© ë°©ë²•

### 1. í”„ë¡¬í”„íŠ¸ ê°œì„  (Prompt Engineering)

#### ëª©í‘œ
ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°œì„ í•˜ì—¬ ë” ë‚˜ì€ ì‘ë‹µ ìƒì„±

#### ë°©ë²•

```python
from agent.feedback_db import FeedbackDatabase, RLTrainingPipeline

db = FeedbackDatabase()
pipeline = RLTrainingPipeline(db)

# ë‚®ì€ í‰ê°€ë¥¼ ë°›ì€ íŒ¨í„´ ë¶„ì„
low_patterns = db.get_low_performing_patterns(min_occurrences=3)

for pattern in low_patterns:
    print(f"ë¬¸ì œ ì§ˆë¬¸: {pattern['user_message']}")
    print(f"ë°œìƒ íšŸìˆ˜: {pattern['occurrences']}")
    print(f"í‰ê·  ë³´ìƒ: {pattern['avg_reward']}")
    # â†’ ì´ëŸ° ì§ˆë¬¸ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
```

#### ì‹¤ì œ í™œìš© ì˜ˆì‹œ

**ë¬¸ì œ ë°œê²¬:**
```
ì§ˆë¬¸: "ì´ íŒŒì¼ ë¶„ì„í•´ì¤˜"
í‰ê·  ë³´ìƒ: -0.8
ë°œìƒ íšŸìˆ˜: 15íšŒ
```

**ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°œì„ :**
```python
# vc_agent.py ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
"""
ì‚¬ìš©ìê°€ "ì´ íŒŒì¼" ê°™ì€ ì• ë§¤í•œ í‘œí˜„ì„ ì“°ë©´:
1. ì–´ë–¤ íŒŒì¼ì¸ì§€ ëª…í™•íˆ í™•ì¸
2. ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ì œì•ˆ
3. ì‚¬ìš©ì í™•ì¸ í›„ ì§„í–‰
"""
```

---

### 2. ì‘ë‹µ í’ˆì§ˆ í–¥ìƒ (Response Quality)

#### ëª©í‘œ
ë†’ì€ í‰ê°€ë¥¼ ë°›ì€ ì‘ë‹µ íŒ¨í„´ì„ í•™ìŠµ

#### ë°©ë²•

```python
# ìš°ìˆ˜ íŒ¨í„´ ë¶„ì„
high_patterns = db.get_high_performing_patterns(min_occurrences=3)

for pattern in high_patterns:
    print(f"ìš°ìˆ˜ ì§ˆë¬¸: {pattern['user_message']}")
    print(f"í‰ê·  ì‘ë‹µ ê¸¸ì´: {pattern['avg_response_length']}ì")
    print(f"í‰ê·  ë³´ìƒ: {pattern['avg_reward']}")
    # â†’ ì´ëŸ° ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ë‹¤ë¥¸ ì˜ì—­ì—ë„ ì ìš©
```

#### ë°œê²¬ ì˜ˆì‹œ

**ìš°ìˆ˜ íŒ¨í„´:**
```
ì§ˆë¬¸: "2030ë…„ PER 10,20,30ë°°ë¡œ Exit í”„ë¡œì ì…˜ ìƒì„±í•´ì¤˜"
í‰ê·  ë³´ìƒ: 1.0
ë°œìƒ íšŸìˆ˜: 20íšŒ
í‰ê·  ì‘ë‹µ ê¸¸ì´: 450ì
ë„êµ¬ ì‚¬ìš©: read_excel_as_text â†’ analyze_and_generate_projection
```

**í•™ìŠµ ë‚´ìš©:**
- ëª…í™•í•œ íŒŒë¼ë¯¸í„° (ì—°ë„, PER ë°°ìˆ˜) â†’ ë†’ì€ ë§Œì¡±ë„
- ë„êµ¬ 2ê°œ ì¡°í•©ì´ íš¨ê³¼ì 
- ì‘ë‹µì€ 450ì ì •ë„ê°€ ì ì •

---

### 3. ë„êµ¬ ì‚¬ìš© ìµœì í™” (Tool Usage Optimization)

#### ëª©í‘œ
ì–´ë–¤ ë„êµ¬ ì¡°í•©ì´ ìµœì ì˜ ê²°ê³¼ë¥¼ ë‚´ëŠ”ì§€ í•™ìŠµ

#### ë°©ë²•

```python
# ë„êµ¬ ì‚¬ìš© íŒ¨í„´ ë¶„ì„
tool_analysis = pipeline.analyze_tool_usage_patterns()

print(tool_analysis['recommendation'])
# ì¶œë ¥: "ê¶Œì¥ ë„êµ¬ ì¡°í•©: read_excel_as_text, analyze_and_generate_projection (í‰ê·  ë³´ìƒ: 0.95)"
```

#### í™œìš© ì˜ˆì‹œ

**ë°œê²¬:**
- `analyze_excel` ë‹¨ë… ì‚¬ìš©: í‰ê·  ë³´ìƒ 0.3
- `read_excel_as_text` â†’ `analyze_and_generate_projection`: í‰ê·  ë³´ìƒ 0.95

**ê°œì„ :**
```python
# vc_agent.py ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
"""
ì—‘ì…€ íŒŒì¼ ë¶„ì„ ì‹œ:
1. ë¨¼ì € read_excel_as_textë¡œ êµ¬ì¡° íŒŒì•… (ê¶Œì¥)
2. ê·¸ ë‹¤ìŒ analyze_and_generate_projection ì‹¤í–‰
"""
```

---

### 4. Claude API íŒŒì¸íŠœë‹ (Fine-tuning)

#### ëª©í‘œ
ì‹¤ì œ ë°ì´í„°ë¡œ Claude ëª¨ë¸ ìì²´ë¥¼ í•™ìŠµ (í–¥í›„ ê°€ëŠ¥)

#### ì¤€ë¹„

```python
# í›ˆë ¨ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
db = FeedbackDatabase()
training_file = db.export_rl_training_data(min_reward=0.5)

print(f"í›ˆë ¨ ë°ì´í„° ìƒì„±: {training_file}")
# ì¶œë ¥: feedback/rl_training_data.jsonl
```

#### JSONL í˜•ì‹ (Anthropic RLHF í˜•ì‹)

```json
{
  "prompt": "ì‚¬ìš©ì ì§ˆë¬¸",
  "response": "ì—ì´ì „íŠ¸ ì‘ë‹µ",
  "reward": 1.0
}
```

#### í–¥í›„ í™œìš© (Anthropic Fine-tuning API ì‚¬ìš©)

```python
# Anthropic Fine-tuning API (í–¥í›„)
from anthropic import Anthropic

client = Anthropic(api_key="...")

# íŒŒì¸íŠœë‹ ì‘ì—… ìƒì„±
fine_tune = client.fine_tuning.create(
    model="claude-opus-4-5-20251101",
    training_file="feedback/rl_training_data.jsonl",
    validation_file="feedback/rl_validation.jsonl"
)

# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš©
agent = VCAgent(model=fine_tune.model_id)
```

---

## ë¦¬í¬íŠ¸ ìƒì„±

### í”„ë¡¬í”„íŠ¸ ê°œì„  ë¦¬í¬íŠ¸

```python
from agent.feedback_db import FeedbackDatabase

db = FeedbackDatabase()
report_path = db.generate_prompt_improvement_report()

print(f"ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")
```

**ë¦¬í¬íŠ¸ ë‚´ìš©:**
- ì „ì²´ í†µê³„ (ë§Œì¡±ë„, í”¼ë“œë°± ìˆ˜, í‰ê·  ë³´ìƒ)
- ê°œì„  í•„ìš” íŒ¨í„´ (ë¶€ì •ì  í”¼ë“œë°± Top 5)
- ìš°ìˆ˜ íŒ¨í„´ (ê¸ì •ì  í”¼ë“œë°± Top 5)
- êµ¬ì²´ì  ê°œì„  ì œì•ˆ

---

## ì‹¤ì „ ì›Œí¬í”Œë¡œìš°

### ë§¤ì£¼ ê¸ˆìš”ì¼: í”¼ë“œë°± ë¶„ì„ ë° ê°œì„ 

```python
# 1. í†µê³„ í™•ì¸
db = FeedbackDatabase()
stats = db.get_global_stats()

print(f"ì´ë²ˆ ì£¼ í”¼ë“œë°±: {stats['total_feedback']}ê°œ")
print(f"ë§Œì¡±ë„: {stats['satisfaction_rate']*100:.1f}%")

# 2. ë¦¬í¬íŠ¸ ìƒì„±
report = db.generate_prompt_improvement_report()
print(f"ë¦¬í¬íŠ¸: {report}")

# 3. íŒ¨í„´ ë¶„ì„
pipeline = RLTrainingPipeline(db)
low_patterns = db.get_low_performing_patterns()
high_patterns = db.get_high_performing_patterns()

# 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
improvements = pipeline.generate_system_prompt_improvements()
with open("prompt_improvements.md", "w") as f:
    f.write(improvements)

# 5. vc_agent.py ìˆ˜ì •
# - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
# - ë„êµ¬ ì‚¬ìš© ìˆœì„œ ì¡°ì •
# - ì‘ë‹µ ìŠ¤íƒ€ì¼ ê°œì„ 
```

---

## í†µê³„ ì¿¼ë¦¬ ì˜ˆì‹œ

### ì „ì²´ í†µê³„

```python
stats = db.get_global_stats()
# {
#   "total_feedback": 150,
#   "positive_feedback": 120,
#   "negative_feedback": 30,
#   "satisfaction_rate": 0.80,
#   "average_reward": 0.65,
#   "total_sessions": 45,
#   "total_users": 12
# }
```

### ì‚¬ìš©ìë³„ í†µê³„

```python
user_stats = db.get_user_stats("í™ê¸¸ë™")
# {
#   "total_feedback": 25,
#   "positive_feedback": 22,
#   "satisfaction_rate": 0.88
# }
```

### íŒ¨í„´ ë¶„ì„

```python
# ê°œì„  í•„ìš”
low = db.get_low_performing_patterns(min_occurrences=3)

# ìš°ìˆ˜ ì‚¬ë¡€
high = db.get_high_performing_patterns(min_occurrences=3)
```

---

## ì§€ì†ì  ê°œì„  ì‚¬ì´í´

```
1ì£¼ì°¨
    â†“
í”¼ë“œë°± ìˆ˜ì§‘ (20ê°œ)
    â†“
íŒ¨í„´ ë¶„ì„ â†’ ë¬¸ì œ ë°œê²¬: "íŒŒì¼ ê²½ë¡œ ì• ë§¤í•¨"
    â†“
í”„ë¡¬í”„íŠ¸ ê°œì„ 
    â†“
2ì£¼ì°¨
    â†“
í”¼ë“œë°± ìˆ˜ì§‘ (30ê°œ)
    â†“
ë§Œì¡±ë„ ìƒìŠ¹ (70% â†’ 85%)
    â†“
ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬: "ì‘ë‹µ ë„ˆë¬´ ì§§ìŒ"
    â†“
ì‘ë‹µ ê¸¸ì´ ì¡°ì •
    â†“
3ì£¼ì°¨...
```

---

## í•µì‹¬ ë©”íŠ¸ë¦­

### ì¶”ì í•´ì•¼ í•  ì§€í‘œ

1. **ë§Œì¡±ë„ (Satisfaction Rate)**
   - ëª©í‘œ: 80% ì´ìƒ
   - ê³„ì‚°: ğŸ‘ / (ğŸ‘ + ğŸ‘)

2. **í‰ê·  ë³´ìƒ (Average Reward)**
   - ëª©í‘œ: 0.6 ì´ìƒ
   - ë²”ìœ„: -1.0 ~ 1.0

3. **ê°œì„  ì†ë„**
   - ì£¼ì°¨ë³„ ë§Œì¡±ë„ ì¦ê°€ìœ¨
   - ëª©í‘œ: ë§¤ì£¼ 2-5% ìƒìŠ¹

4. **ì‚¬ìš©ì ì°¸ì—¬ë„**
   - í”¼ë“œë°± ë¹„ìœ¨ (ì‘ë‹µ ë‹¹ í”¼ë“œë°± ìˆ˜)
   - ëª©í‘œ: 30% ì´ìƒ

---

## CLI ë„êµ¬

### í”¼ë“œë°± í†µê³„ í™•ì¸

```bash
# ì „ì²´ í†µê³„
python -c "from agent.feedback_db import FeedbackDatabase; db = FeedbackDatabase(); print(db.get_global_stats())"

# ë¦¬í¬íŠ¸ ìƒì„±
python -c "from agent.feedback_db import FeedbackDatabase; db = FeedbackDatabase(); print(db.generate_prompt_improvement_report())"

# í›ˆë ¨ ë°ì´í„° ë‚´ë³´ë‚´ê¸°
python -c "from agent.feedback_db import FeedbackDatabase; db = FeedbackDatabase(); print(db.export_rl_training_data())"
```

---

## ê²°ë¡ 

### ê°•í™”í•™ìŠµìœ¼ë¡œ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ê²ƒ

1. âœ… **ìë™í™”ëœ í’ˆì§ˆ ê°œì„ **: ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œ ìë™ í•™ìŠµ
2. âœ… **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ë°ì´í„° ê¸°ë°˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
3. âœ… **ë„êµ¬ ì‚¬ìš© ìµœì í™”**: ìµœì ì˜ ë„êµ¬ ì¡°í•© ë°œê²¬
4. âœ… **ê°œì¸í™”**: ì‚¬ìš©ì/ì¡°ì§ë³„ ë§ì¶¤í˜• ì‘ë‹µ
5. âœ… **ì§€ì†ì  ê°œì„ **: ë°ì´í„°ê°€ ìŒ“ì¼ìˆ˜ë¡ ë” ë˜‘ë˜‘í•´ì§

### ë‹¤ìŒ ë‹¨ê³„

1. **1ê°œì›” ë°ì´í„° ìˆ˜ì§‘**: ìµœì†Œ 100ê°œ í”¼ë“œë°± í™•ë³´
2. **íŒ¨í„´ ë¶„ì„**: ê°œì„ /ìš°ìˆ˜ íŒ¨í„´ ì‹ë³„
3. **í”„ë¡¬í”„íŠ¸ ê°œì„ **: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—…ë°ì´íŠ¸
4. **A/B í…ŒìŠ¤íŠ¸**: ê°œì„  ì „/í›„ ë¹„êµ
5. **ë°˜ë³µ**: ì§€ì†ì  ê°œì„  ì‚¬ì´í´ í™•ë¦½

**ëª©í‘œ: 3ê°œì›” ë‚´ ë§Œì¡±ë„ 90% ë‹¬ì„±! ğŸ¯**
