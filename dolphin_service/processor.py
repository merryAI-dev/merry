"""
Claude Vision PDF Processor

Claude Opusë¥¼ ì‚¬ìš©í•˜ì—¬ PDFë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
Dolphin ëŒ€ì‹  Claude Vision APIë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import base64
import hashlib
import json
import logging
import os
import time
from datetime import datetime, timedelta
from io import BytesIO
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ .env íŒŒì¼ ë¡œë“œ (ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©)
PROJECT_ROOT = Path(__file__).resolve().parent.parent
load_dotenv(PROJECT_ROOT / ".env")

from .config import DOLPHIN_CONFIG
from . import classifier as doc_classifier
from . import chunker
from . import prompts as prompt_registry
from .strategy import get_strategy, ProcessingStrategy
from shared.training_logger import log_training_data

logger = logging.getLogger(__name__)


def _rows_to_markdown(rows: List[List]) -> str:
    """2D ë¦¬ìŠ¤íŠ¸ë¥¼ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ ë³€í™˜."""
    if not rows:
        return ""
    lines = []
    for i, row in enumerate(rows):
        cells = [str(c) if c is not None else "" for c in row]
        lines.append("| " + " | ".join(cells) + " |")
        if i == 0:
            lines.append("| " + " | ".join(["---"] * len(cells)) + " |")
    return "\n".join(lines)

# ìºì‹œ ë””ë ‰í† ë¦¬
CACHE_DIR = Path(os.getenv("PDF_CACHE_DIR", "/tmp/claude_pdf_cache"))


def _get_cache_path(pdf_path: str, max_pages: int, output_mode: str) -> Path:
    """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
    # íŒŒì¼ í•´ì‹œ ìƒì„±
    file_stat = Path(pdf_path).stat()
    cache_key = f"{pdf_path}:{file_stat.st_size}:{file_stat.st_mtime}:{max_pages}:{output_mode}"
    hash_key = hashlib.md5(cache_key.encode()).hexdigest()
    return CACHE_DIR / f"{hash_key}.json"


def _get_cached_result(cache_path: Path) -> Optional[Dict[str, Any]]:
    """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
    if not DOLPHIN_CONFIG.get("cache_enabled", True):
        return None

    if not cache_path.exists():
        return None

    try:
        # TTL ì²´í¬
        ttl_days = DOLPHIN_CONFIG.get("cache_ttl_days", 7)
        file_age = datetime.now() - datetime.fromtimestamp(cache_path.stat().st_mtime)
        if file_age > timedelta(days=ttl_days):
            cache_path.unlink()  # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
            return None

        with open(cache_path, "r", encoding="utf-8") as f:
            result = json.load(f)
            result["cache_hit"] = True
            return result
    except Exception as e:
        logger.warning(f"ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
        return None


def _save_to_cache(cache_path: Path, result: Dict[str, Any]) -> None:
    """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ìž¥"""
    if not DOLPHIN_CONFIG.get("cache_enabled", True):
        return

    try:
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"ìºì‹œ ì €ìž¥ ì‹¤íŒ¨: {e}")


class ClaudeVisionProcessor:
    """Claude Vision ê¸°ë°˜ PDF ì²˜ë¦¬ê¸°

    - PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    - Claude Opus Visionìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
    - í…Œì´ë¸”, ìž¬ë¬´ì œí‘œ ìžë™ ì¸ì‹
    """

    def __init__(self, api_key: str = None):
        self.api_key = api_key

    def process_pdf(
        self,
        pdf_path: str,
        max_pages: int = None,
        output_mode: str = "structured",
        progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
    ) -> Dict[str, Any]:
        """PDF íŒŒì¼ì„ ë¶„ë¥˜ í›„ ìµœì  ì „ëžµìœ¼ë¡œ ì²˜ë¦¬

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ íŽ˜ì´ì§€ ìˆ˜
            output_mode: ì¶œë ¥ ëª¨ë“œ (text_only, structured, tables_only)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        max_pages = max_pages or DOLPHIN_CONFIG["default_max_pages"]

        # ìºì‹œ í™•ì¸
        cache_path = _get_cache_path(pdf_path, max_pages, output_mode)
        cached = _get_cached_result(cache_path)
        if cached:
            self._emit_progress(progress_callback, "complete", "ìºì‹œì—ì„œ ë¡œë“œë¨")
            return cached

        self._emit_progress(progress_callback, "loading", "PDF ë¡œë”© ì¤‘...")

        try:
            # 1. ë¬¸ì„œ ë¶„ë¥˜
            self._emit_progress(progress_callback, "classifying", "ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜ ì¤‘...")
            classification = doc_classifier.classify(pdf_path, max_pages)
            strategy = get_strategy(classification)

            logger.info(
                f"ë¬¸ì„œ ë¶„ë¥˜: {classification.doc_type.value}, "
                f"ì „ëžµ: vision={strategy.use_vision}, model={strategy.model}"
            )

            # 2. ì „ëžµì— ë”°ë¼ ë¶„ê¸°
            if not strategy.use_vision:
                # PyMuPDF ì§ì ‘ ì²˜ë¦¬ (PURE_TEXT, TEXT_WITH_TABLES, SMALL_TABLE)
                self._emit_progress(
                    progress_callback,
                    "processing",
                    f"PyMuPDFë¡œ {classification.total_pages}íŽ˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...",
                )
                result = self._process_with_pymupdf(pdf_path, classification)
            else:
                # Vision API ì²˜ë¦¬
                self._emit_progress(
                    progress_callback,
                    "converting",
                    f"PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘ (DPI={strategy.dpi})...",
                )
                images_base64, total_pages = self._pdf_to_base64_images(
                    pdf_path, max_pages, dpi_override=strategy.dpi
                )

                if not images_base64:
                    return {
                        "success": False,
                        "error": "PDFì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    }

                # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
                chunks = chunker.create_chunks(images_base64, strategy)
                self._emit_progress(
                    progress_callback,
                    "processing",
                    f"{strategy.model}ë¡œ {len(images_base64)}íŽ˜ì´ì§€ ë¶„ì„ ì¤‘ "
                    f"({len(chunks)}ì²­í¬)...",
                )

                if len(chunks) == 1:
                    # ë‹¨ì¼ ì²­í¬: ê¸°ì¡´ ë°©ì‹
                    result = self._process_with_claude(
                        chunks[0], output_mode, progress_callback,
                        model_override=strategy.model,
                        prompt_type=strategy.prompt_type,
                        max_tokens_override=strategy.max_tokens,
                    )
                else:
                    # ë‹¤ì¤‘ ì²­í¬: ë³‘ë ¬ ì²˜ë¦¬
                    result = self._process_chunks_parallel(
                        chunks, output_mode, progress_callback,
                        strategy=strategy,
                    )

            # 3. ê²°ê³¼ ì¡°í•©
            processing_time = time.time() - start_time

            total_pages = classification.total_pages
            pages_read = min(total_pages, max_pages)

            final_result = {
                "success": True,
                "file_path": pdf_path,
                "total_pages": total_pages,
                "pages_read": pages_read,
                "content": result.get("content", ""),
                "char_count": len(result.get("content", "")),
                "structured_content": result.get("structured_content", {}),
                "financial_tables": result.get("financial_tables", {}),
                "doc_type": classification.doc_type.value,
                "processing_method": (
                    "pymupdf_direct" if not strategy.use_vision
                    else f"vision_{strategy.model or 'unknown'}"
                ),
                "processing_time_seconds": processing_time,
                "cache_hit": False,
                "cached_at": datetime.utcnow().isoformat(),
            }

            # ìºì‹œì— ì €ìž¥
            _save_to_cache(cache_path, final_result)

            self._emit_progress(
                progress_callback,
                "complete",
                f"ì™„ë£Œ ({processing_time:.1f}ì´ˆ)",
                {"duration": processing_time},
            )

            return final_result

        except Exception as e:
            logger.error(f"Claude Vision ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            return self._fallback_to_pymupdf(pdf_path, max_pages, str(e))

    def _pdf_to_base64_images(
        self, pdf_path: str, max_pages: int, dpi_override: int = 0
    ) -> tuple:
        """PDFë¥¼ base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ íŽ˜ì´ì§€ ìˆ˜
            dpi_override: 0ì´ë©´ config ê¸°ë³¸ê°’, >0ì´ë©´ í•´ë‹¹ DPI ì‚¬ìš©

        Returns:
            (images_base64: List[str], total_pages: int)
        """
        try:
            import fitz  # PyMuPDF
        except ImportError as e:
            raise ImportError(f"PDF ë³€í™˜ì— PyMuPDFê°€ í•„ìš”í•©ë‹ˆë‹¤: {e}")

        doc = None
        images_base64 = []

        try:
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            pages_to_read = min(total_pages, max_pages)

            dpi = dpi_override if dpi_override > 0 else DOLPHIN_CONFIG.get("image_dpi", 150)
            zoom = dpi / 72

            for i in range(pages_to_read):
                page = doc[i]
                mat = fitz.Matrix(zoom, zoom)
                pix = page.get_pixmap(matrix=mat)

                # PNGë¡œ ë³€í™˜ í›„ base64 ì¸ì½”ë”©
                img_bytes = pix.tobytes("png")
                img_base64 = base64.standard_b64encode(img_bytes).decode("utf-8")
                images_base64.append(img_base64)

            return images_base64, total_pages

        finally:
            if doc:
                doc.close()

    def _get_system_prompt(self) -> str:
        """VC íˆ¬ìž ë¶„ì„ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)."""
        return prompt_registry.FINANCIAL_SYSTEM

    def _process_with_claude(
        self,
        images_base64: List[str],
        output_mode: str,
        progress_callback: Optional[Callable] = None,
        model_override: Optional[str] = None,
        prompt_type: str = "financial_structured",
        max_tokens_override: int = 0,
        page_offset: int = 0,
    ) -> Dict[str, Any]:
        """Claude Vision APIë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬

        Args:
            images_base64: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸
            output_mode: ì¶œë ¥ ëª¨ë“œ
            progress_callback: ì§„í–‰ ì½œë°±
            model_override: ëª¨ë¸ ì˜¤ë²„ë¼ì´ë“œ (Noneì´ë©´ ê¸°ë³¸ Sonnet)
            prompt_type: prompts.py í‚¤
            max_tokens_override: max_tokens ì˜¤ë²„ë¼ì´ë“œ (0ì´ë©´ ê¸°ë³¸ê°’)
            page_offset: ì²­í¬ì˜ ì‹œìž‘ íŽ˜ì´ì§€ ë²ˆí˜¸ (íŽ˜ì´ì§€ ë¼ë²¨ì— ì‚¬ìš©)
        """
        import httpx
        import anthropic

        # íƒ€ìž„ì•„ì›ƒ ì„¤ì • (5ë¶„)
        timeout_config = httpx.Timeout(
            timeout=DOLPHIN_CONFIG.get("timeout_seconds", 300),
            connect=30.0,
        )
        client = anthropic.Anthropic(timeout=timeout_config)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (prompts.py ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‚¬ìš©)
        system_prompt, user_prompt = prompt_registry.get_prompts(prompt_type, output_mode)

        # ì´ë¯¸ì§€ ì½˜í…ì¸  êµ¬ì„±
        content = []
        for i, img_b64 in enumerate(images_base64):
            content.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/png",
                    "data": img_b64,
                },
            })
            content.append({
                "type": "text",
                "text": f"[íŽ˜ì´ì§€ {page_offset + i + 1}]"
            })

        content.append({
            "type": "text",
            "text": user_prompt
        })

        model = model_override or "claude-sonnet-4-5-20250929"
        max_tokens = max_tokens_override if max_tokens_override > 0 else 16384

        # Claude API í˜¸ì¶œ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— cache_control ì ìš©)
        response = client.messages.create(
            model=model,
            max_tokens=max_tokens,
            system=[
                {
                    "type": "text",
                    "text": system_prompt,
                    "cache_control": {"type": "ephemeral"},
                }
            ],
            messages=[
                {
                    "role": "user",
                    "content": content
                }
            ]
        )

        # ì‘ë‹µ íŒŒì‹±
        response_text = response.content[0].text if response.content else ""
        return self._parse_claude_response(response_text, output_mode)

    def _process_chunks_parallel(
        self,
        chunks: List[List[str]],
        output_mode: str,
        progress_callback: Optional[Callable] = None,
        strategy: Optional[ProcessingStrategy] = None,
    ) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬ í›„ ê²°ê³¼ ë³‘í•©."""
        from concurrent.futures import ThreadPoolExecutor, as_completed

        page_offsets = chunker.compute_page_offsets(chunks)
        chunk_results: List[Optional[Dict[str, Any]]] = [None] * len(chunks)

        def process_chunk(idx: int) -> Dict[str, Any]:
            return self._process_with_claude(
                chunks[idx],
                output_mode,
                progress_callback=None,  # ì²­í¬ë³„ ì½œë°± ë¹„í™œì„±í™”
                model_override=strategy.model if strategy else None,
                prompt_type=strategy.prompt_type if strategy else "financial_structured",
                max_tokens_override=strategy.max_tokens if strategy else 0,
                page_offset=page_offsets[idx],
            )

        completed = 0
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {executor.submit(process_chunk, i): i for i in range(len(chunks))}
            for future in as_completed(futures):
                idx = futures[future]
                try:
                    chunk_results[idx] = future.result()
                except Exception as e:
                    logger.error(f"ì²­í¬ {idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    chunk_results[idx] = {"content": "", "error": str(e)}
                completed += 1
                self._emit_progress(
                    progress_callback,
                    "processing",
                    f"ì²­í¬ {completed}/{len(chunks)} ì™„ë£Œ",
                )

        # None ì œê±° ë° ë³‘í•©
        valid_results = [r for r in chunk_results if r is not None]
        return chunker.merge_chunk_results(valid_results, page_offsets)

    def _process_with_pymupdf(
        self,
        pdf_path: str,
        classification: "doc_classifier.ClassificationResult",
    ) -> Dict[str, Any]:
        """Vision API ì—†ì´ PyMuPDFë§Œìœ¼ë¡œ ì²˜ë¦¬ (PURE_TEXT, TEXT_WITH_TABLES, SMALL_TABLE)."""
        import fitz

        doc = fitz.open(pdf_path)
        try:
            text_parts: List[str] = []
            pages: List[Dict[str, Any]] = []
            all_tables: List[Dict[str, Any]] = []

            for i in range(len(doc)):
                page = doc[i]
                text = page.get_text("text")
                text_parts.append(
                    f"\n{'='*60}\níŽ˜ì´ì§€ {i+1}\n{'='*60}\n{text}"
                )

                page_data: Dict[str, Any] = {
                    "page_num": i + 1,
                    "elements": [{"type": "text", "content": text}],
                }

                # PyMuPDF í…Œì´ë¸” ê°ì§€
                try:
                    found_tables = page.find_tables()
                    for t in found_tables.tables:
                        rows = t.extract()
                        table_entry = {
                            "type": "table",
                            "content": {
                                "rows": rows,
                                "markdown": _rows_to_markdown(rows),
                            },
                        }
                        page_data["elements"].append(table_entry)
                        all_tables.append({
                            "page": i + 1,
                            "content": table_entry["content"],
                        })
                except Exception:
                    pass

                pages.append(page_data)

            content = "".join(text_parts)

            return {
                "content": content,
                "structured_content": {"pages": pages},
                "financial_tables": {},  # table_extractorê°€ ë‚˜ì¤‘ì— ì²˜ë¦¬
            }
        finally:
            doc.close()

    def _get_text_only_prompt(self) -> str:
        """í…ìŠ¤íŠ¸ ì „ìš© í”„ë¡¬í”„íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)."""
        return prompt_registry.TEXT_ONLY_USER

    def _get_structured_prompt(self) -> str:
        """êµ¬ì¡°í™” ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)."""
        return prompt_registry.FINANCIAL_STRUCTURED_USER

    def _get_tables_only_prompt(self) -> str:
        """í…Œì´ë¸” ì „ìš© í”„ë¡¬í”„íŠ¸ (í•˜ìœ„ í˜¸í™˜ì„±)."""
        return prompt_registry.TABLE_EXTRACTION_USER

    def _parse_claude_response(
        self, response_text: str, output_mode: str
    ) -> Dict[str, Any]:
        """Claude ì‘ë‹µ íŒŒì‹±"""
        import json
        import re

        if output_mode == "text_only":
            return {"content": response_text}

        # JSON ì¶”ì¶œ ì‹œë„
        try:
            # ```json ... ``` ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # ì „ì²´ê°€ JSONì¸ ê²½ìš°
                json_str = response_text.strip()

            parsed = json.loads(json_str)

            return {
                "content": parsed.get("content", response_text),
                "structured_content": parsed.get("structured_content", {}),
                "financial_tables": parsed.get("financial_tables", {}),
            }

        except json.JSONDecodeError:
            logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜")
            return {"content": response_text}

    def _fallback_to_pymupdf(
        self, pdf_path: str, max_pages: int, reason: str
    ) -> Dict[str, Any]:
        """PyMuPDFë¡œ í´ë°±"""
        logger.info(f"PyMuPDFë¡œ í´ë°±: {reason}")

        try:
            import fitz

            doc = None
            try:
                doc = fitz.open(pdf_path)
                total_pages = len(doc)
                pages_to_read = min(total_pages, max_pages)

                text_content = []
                for i in range(pages_to_read):
                    page = doc[i]
                    text = page.get_text()
                    text_content.append(
                        f"\n{'='*60}\níŽ˜ì´ì§€ {i+1}\n{'='*60}\n{text}"
                    )

                content = "".join(text_content)

                return {
                    "success": True,
                    "file_path": pdf_path,
                    "total_pages": total_pages,
                    "pages_read": pages_to_read,
                    "content": content,
                    "char_count": len(content),
                    "processing_method": "pymupdf_fallback",
                    "fallback_used": True,
                    "fallback_reason": reason,
                    "cache_hit": False,
                    "cached_at": datetime.utcnow().isoformat(),
                }

            finally:
                if doc:
                    doc.close()

        except Exception as e:
            logger.error(f"í´ë°± ì²˜ë¦¬ë„ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "fallback_attempted": True,
                "original_error": reason,
            }

    def _emit_progress(
        self,
        callback: Optional[Callable],
        stage: str,
        message: str,
        data: Optional[Dict[str, Any]] = None,
    ) -> None:
        """ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ ë°œìƒ"""
        if callback:
            event = {
                "type": "info",
                "content": message,
                "data": {"stage": stage, **(data or {})},
            }
            try:
                callback(event)
            except Exception as e:
                logger.warning(f"ì§„í–‰ ì½œë°± ì‹¤íŒ¨: {e}")


# ê¸°ì¡´ DolphinProcessorë¥¼ ClaudeVisionProcessorë¡œ ëŒ€ì²´
DolphinProcessor = ClaudeVisionProcessor


@log_training_data(task_type="pdf_extraction", model_name="claude-sonnet-4-5-20250929")
def process_pdf_with_claude(
    pdf_path: str,
    max_pages: int = None,
    output_mode: str = "structured",
    progress_callback: Optional[Callable] = None,
) -> Dict[str, Any]:
    """PDFë¥¼ Claude Visionìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” íŽ¸ì˜ í•¨ìˆ˜"""
    processor = ClaudeVisionProcessor()
    return processor.process_pdf(
        pdf_path=pdf_path,
        max_pages=max_pages,
        output_mode=output_mode,
        progress_callback=progress_callback,
    )


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
process_pdf_with_dolphin = process_pdf_with_claude


class InteractiveAnalysisSession:
    """ëŒ€í™”í˜• íˆ¬ìž ë¶„ì„ ì„¸ì…˜

    ì—¬ëŸ¬ íŒŒì¼ê³¼ í…ìŠ¤íŠ¸ ìž…ë ¥ì„ ë°›ì•„ì„œ ì ì§„ì ìœ¼ë¡œ ë¶„ì„ì„ ì™„ì„±í•©ë‹ˆë‹¤.
    """

    def __init__(self, session_id: str = None):
        self.session_id = session_id or datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        self.processor = ClaudeVisionProcessor()
        self.accumulated_data = {
            "company_info": {},
            "investment_terms": {"found": False},
            "financial_tables": {
                "income_statement": {"found": False},
                "balance_sheet": {"found": False},
                "cash_flow": {"found": False},
                "cap_table": {"found": False},
            },
            "valuation_metrics": {},
            "source_files": [],
            "text_inputs": [],
        }
        self.missing_data = []

    def add_pdf(self, pdf_path: str, max_pages: int = 30) -> Dict[str, Any]:
        """PDF íŒŒì¼ ì¶”ê°€ ë° ë¶„ì„"""
        result = self.processor.process_pdf(
            pdf_path=pdf_path,
            max_pages=max_pages,
            output_mode="structured",
        )

        if result.get("success"):
            self._merge_data(result)
            self.accumulated_data["source_files"].append(pdf_path)

        return self._get_status()

    def add_text_input(self, text: str, data_type: str = "general") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìž…ë ¥ ì¶”ê°€ (ìž¬ë¬´ ë°ì´í„°, Cap Table ë“±)

        Args:
            text: ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ í…ìŠ¤íŠ¸
            data_type: "financial", "cap_table", "investment_terms", "general"
        """
        import json
        import re
        import anthropic

        try:
            client = anthropic.Anthropic()
            prompt = self._get_text_parsing_prompt(text, data_type)

            response = client.messages.create(
                model="claude-haiku-4-5-20251001",  # í…ìŠ¤íŠ¸ íŒŒì‹±ì€ Haikuë¡œ ì¶©ë¶„ (ë¹„ìš© 95% ì ˆê°)
                max_tokens=4096,
                system=[
                    {
                        "type": "text",
                        "text": self.processor._get_system_prompt(),
                        "cache_control": {"type": "ephemeral"},
                    }
                ],
                messages=[{"role": "user", "content": prompt}]
            )

            response_text = response.content[0].text
            json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(1))
            else:
                parsed = json.loads(response_text.strip())

            self._merge_parsed_text(parsed, data_type)
            self.accumulated_data["text_inputs"].append({
                "type": data_type,
                "content": text[:500],  # ìš”ì•½ ì €ìž¥
            })

        except anthropic.APIConnectionError as e:
            logger.error(f"API ì—°ê²° ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": "API ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
            }
        except anthropic.RateLimitError as e:
            logger.error(f"Rate limit: {e}")
            return {
                "success": False,
                "error": "API í˜¸ì¶œ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            }
        except anthropic.APIStatusError as e:
            logger.error(f"API ì˜¤ë¥˜: {e}")
            return {
                "success": False,
                "error": f"API ì˜¤ë¥˜: {e.message}",
            }
        except json.JSONDecodeError as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": "ì‘ë‹µ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            }
        except Exception as e:
            logger.exception("í…ìŠ¤íŠ¸ ìž…ë ¥ ì²˜ë¦¬ ì‹¤íŒ¨")
            return {
                "success": False,
                "error": f"ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
            }

        return self._get_status()

    def _get_text_parsing_prompt(self, text: str, data_type: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì‹±ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ íˆ¬ìž ë¶„ì„ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

"""
        if data_type == "financial":
            base_prompt += """
ìž¬ë¬´ ì •ë³´ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ:
```json
{
  "income_statement": {
    "found": true,
    "years": ["2023", "2024E"],
    "metrics": {
      "revenue": [ê°’ë“¤],
      "operating_income": [ê°’ë“¤],
      "net_income": [ê°’ë“¤]
    }
  }
}
```"""
        elif data_type == "cap_table":
            base_prompt += """
Cap Tableì„ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ:
```json
{
  "cap_table": {
    "found": true,
    "total_shares_issued": ì´ì£¼ì‹ìˆ˜,
    "shareholders": [
      {"name": "ì£¼ì£¼ëª…", "shares": ì£¼ì‹ìˆ˜, "percentage": ì§€ë¶„ìœ¨}
    ]
  }
}
```"""
        elif data_type == "investment_terms":
            base_prompt += """
íˆ¬ìž ì¡°ê±´ì„ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ:
```json
{
  "investment_terms": {
    "found": true,
    "investment_amount": íˆ¬ìžê¸ˆì•¡,
    "pre_money_valuation": Pre-money,
    "price_per_share": ì£¼ë‹¹ê°€ê²©,
    "investment_type": "ë³´í†µì£¼/ìš°ì„ ì£¼/CB"
  }
}
```"""
        else:
            base_prompt += """
ê´€ë ¨ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
```json
{
  "extracted_info": {...}
}
```"""

        return base_prompt

    def _merge_data(self, new_data: Dict[str, Any]) -> None:
        """ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©"""
        # íšŒì‚¬ ì •ë³´
        if new_data.get("company_info"):
            self.accumulated_data["company_info"].update(new_data["company_info"])

        # íˆ¬ìž ì¡°ê±´
        if new_data.get("investment_terms", {}).get("found"):
            self.accumulated_data["investment_terms"] = new_data["investment_terms"]

        # ìž¬ë¬´ í…Œì´ë¸”
        financial = new_data.get("financial_tables", {})
        for table_type in ["income_statement", "balance_sheet", "cash_flow", "cap_table"]:
            if financial.get(table_type, {}).get("found"):
                self.accumulated_data["financial_tables"][table_type] = financial[table_type]

        # ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ
        if new_data.get("valuation_metrics"):
            self.accumulated_data["valuation_metrics"].update(new_data["valuation_metrics"])

    def _merge_parsed_text(self, parsed: Dict[str, Any], data_type: str) -> None:
        """íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ë°ì´í„° ë³‘í•©"""
        if data_type == "financial" and parsed.get("income_statement"):
            self.accumulated_data["financial_tables"]["income_statement"] = parsed["income_statement"]
        elif data_type == "cap_table" and parsed.get("cap_table"):
            self.accumulated_data["financial_tables"]["cap_table"] = parsed["cap_table"]
        elif data_type == "investment_terms" and parsed.get("investment_terms"):
            self.accumulated_data["investment_terms"] = parsed["investment_terms"]

    def _get_status(self) -> Dict[str, Any]:
        """í˜„ìž¬ ë¶„ì„ ìƒíƒœ ë°˜í™˜"""
        missing = []
        critical_missing = []

        # í•„ìˆ˜ ë°ì´í„° ì²´í¬
        if not self.accumulated_data["financial_tables"]["income_statement"].get("found"):
            critical_missing.append({
                "field": "income_statement",
                "name": "ì†ìµê³„ì‚°ì„œ",
                "suggestion": "ìž¬ë¬´ì œí‘œ PDFë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, ë§¤ì¶œ/ì˜ì—…ì´ìµ/ìˆœì´ìµì„ í…ìŠ¤íŠ¸ë¡œ ìž…ë ¥í•´ì£¼ì„¸ìš”.\nì˜ˆ: '2024ë…„ ë§¤ì¶œ 100ì–µ, ì˜ì—…ì´ìµ 20ì–µ, ìˆœì´ìµ 15ì–µ'",
            })

        if not self.accumulated_data["financial_tables"]["cap_table"].get("found"):
            critical_missing.append({
                "field": "cap_table",
                "name": "Cap Table (ì£¼ì£¼í˜„í™©)",
                "suggestion": "Cap Tableì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, ì£¼ì£¼í˜„í™©ì„ í…ìŠ¤íŠ¸ë¡œ ìž…ë ¥í•´ì£¼ì„¸ìš”.\nì˜ˆ: 'ëŒ€í‘œì´ì‚¬ 60%, íˆ¬ìžìžA 20%, ìŠ¤í†¡ì˜µì…˜í’€ 20%'",
            })

        if not self.accumulated_data["investment_terms"].get("found"):
            missing.append({
                "field": "investment_terms",
                "name": "íˆ¬ìž ì¡°ê±´",
                "suggestion": "íˆ¬ìžê¸ˆì•¡, ë°¸ë¥˜ì—ì´ì…˜, ì£¼ë‹¹ê°€ê²© ë“±ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.\nì˜ˆ: 'íˆ¬ìžê¸ˆì•¡ 30ì–µ, Pre-money 100ì–µ, ì£¼ë‹¹ê°€ê²© 10,000ì›'",
            })

        # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
        if critical_missing:
            status = "incomplete"
            message = "ðŸ”´ í•„ìˆ˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤:\n"
            for item in critical_missing:
                message += f"\n**{item['name']}**\n{item['suggestion']}\n"
        elif missing:
            status = "partial"
            message = "ðŸŸ¡ ì¶”ê°€ ë°ì´í„°ê°€ ìžˆìœ¼ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:\n"
            for item in missing:
                message += f"\n**{item['name']}**\n{item['suggestion']}\n"
        else:
            status = "complete"
            message = "ðŸŸ¢ ëª¨ë“  í•„ìˆ˜ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê°€ëŠ¥í•©ë‹ˆë‹¤."

        return {
            "success": True,
            "session_id": self.session_id,
            "status": status,
            "message": message,
            "critical_missing": critical_missing,
            "optional_missing": missing,
            "accumulated_data": self.accumulated_data,
            "source_count": {
                "files": len(self.accumulated_data["source_files"]),
                "text_inputs": len(self.accumulated_data["text_inputs"]),
            },
        }

    def get_final_analysis(self) -> Dict[str, Any]:
        """ìµœì¢… ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        status = self._get_status()

        if status["status"] == "incomplete":
            return {
                "success": False,
                "error": "í•„ìˆ˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                "missing": status["critical_missing"],
                "message": status["message"],
            }

        return {
            "success": True,
            "session_id": self.session_id,
            "analysis": self.accumulated_data,
            "data_quality": "complete" if status["status"] == "complete" else "partial",
            "warnings": status.get("optional_missing", []),
        }


# ì„¸ì…˜ ê´€ë¦¬ (í´ë°±ìš© - Streamlit ì™¸ë¶€ì—ì„œ ì‚¬ìš©)
_active_sessions: Dict[str, InteractiveAnalysisSession] = {}


def _get_streamlit_session_storage() -> Dict[str, InteractiveAnalysisSession]:
    """Streamlit session_state ê¸°ë°˜ ì„¸ì…˜ ì €ìž¥ì†Œ ë°˜í™˜ (ê°€ëŠ¥í•œ ê²½ìš°)"""
    try:
        import streamlit as st
        if "analysis_sessions" not in st.session_state:
            st.session_state.analysis_sessions = {}
        return st.session_state.analysis_sessions
    except (ImportError, RuntimeError):
        # Streamlit ì™¸ë¶€ì—ì„œ ì‹¤í–‰ ì¤‘ì´ê±°ë‚˜ context ì—†ìŒ
        return _active_sessions


def get_or_create_session(session_id: str = None) -> InteractiveAnalysisSession:
    """ë¶„ì„ ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±

    Streamlit í™˜ê²½ì—ì„œëŠ” st.session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì €ë³„ ì„¸ì…˜ ë¶„ë¦¬.
    ê·¸ ì™¸ í™˜ê²½ì—ì„œëŠ” ëª¨ë“ˆ ë ˆë²¨ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©.
    """
    storage = _get_streamlit_session_storage()

    if session_id and session_id in storage:
        return storage[session_id]

    session = InteractiveAnalysisSession(session_id)
    storage[session.session_id] = session
    return session


def clear_session(session_id: str) -> None:
    """ì„¸ì…˜ ì‚­ì œ"""
    storage = _get_streamlit_session_storage()
    if session_id in storage:
        del storage[session_id]
