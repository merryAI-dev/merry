"""
Claude Vision PDF Processor

Claude Opusë¥¼ ì‚¬ìš©í•˜ì—¬ PDFë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
Dolphin ëŒ€ì‹  Claude Vision APIë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
"""

import base64
import hashlib
import json
import logging
import os
import time
from datetime import datetime, timedelta
from io import BytesIO
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

from .config import DOLPHIN_CONFIG

logger = logging.getLogger(__name__)

# ìºì‹œ ë””ë ‰í† ë¦¬
CACHE_DIR = Path(os.getenv("PDF_CACHE_DIR", "/tmp/claude_pdf_cache"))


def _get_cache_path(pdf_path: str, max_pages: int, output_mode: str) -> Path:
    """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
    # íŒŒì¼ í•´ì‹œ ìƒì„±
    file_stat = Path(pdf_path).stat()
    cache_key = f"{pdf_path}:{file_stat.st_size}:{file_stat.st_mtime}:{max_pages}:{output_mode}"
    hash_key = hashlib.md5(cache_key.encode()).hexdigest()
    return CACHE_DIR / f"{hash_key}.json"


def _get_cached_result(cache_path: Path) -> Optional[Dict[str, Any]]:
    """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
    if not DOLPHIN_CONFIG.get("cache_enabled", True):
        return None

    if not cache_path.exists():
        return None

    try:
        # TTL ì²´í¬
        ttl_days = DOLPHIN_CONFIG.get("cache_ttl_days", 7)
        file_age = datetime.now() - datetime.fromtimestamp(cache_path.stat().st_mtime)
        if file_age > timedelta(days=ttl_days):
            cache_path.unlink()  # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
            return None

        with open(cache_path, "r", encoding="utf-8") as f:
            result = json.load(f)
            result["cache_hit"] = True
            return result
    except Exception as e:
        logger.warning(f"ìºì‹œ ì½ê¸° ì‹¤íŒ¨: {e}")
        return None


def _save_to_cache(cache_path: Path, result: Dict[str, Any]) -> None:
    """ê²°ê³¼ë¥¼ ìºì‹œì— ì €ìž¥"""
    if not DOLPHIN_CONFIG.get("cache_enabled", True):
        return

    try:
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.warning(f"ìºì‹œ ì €ìž¥ ì‹¤íŒ¨: {e}")


class ClaudeVisionProcessor:
    """Claude Vision ê¸°ë°˜ PDF ì²˜ë¦¬ê¸°

    - PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
    - Claude Opus Visionìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì¶”ì¶œ
    - í…Œì´ë¸”, ìž¬ë¬´ì œí‘œ ìžë™ ì¸ì‹
    """

    def __init__(self, api_key: str = None):
        self.api_key = api_key

    def process_pdf(
        self,
        pdf_path: str,
        max_pages: int = None,
        output_mode: str = "structured",
        progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
    ) -> Dict[str, Any]:
        """PDF íŒŒì¼ì„ Claude Visionìœ¼ë¡œ ì²˜ë¦¬

        Args:
            pdf_path: PDF íŒŒì¼ ê²½ë¡œ
            max_pages: ìµœëŒ€ ì²˜ë¦¬ íŽ˜ì´ì§€ ìˆ˜
            output_mode: ì¶œë ¥ ëª¨ë“œ (text_only, structured, tables_only)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜

        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        start_time = time.time()
        max_pages = max_pages or DOLPHIN_CONFIG["default_max_pages"]

        # ìºì‹œ í™•ì¸
        cache_path = _get_cache_path(pdf_path, max_pages, output_mode)
        cached = _get_cached_result(cache_path)
        if cached:
            self._emit_progress(progress_callback, "complete", "ìºì‹œì—ì„œ ë¡œë“œë¨")
            return cached

        self._emit_progress(progress_callback, "loading", "PDF ë¡œë”© ì¤‘...")

        try:
            # 1. PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            self._emit_progress(progress_callback, "converting", "PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ ì¤‘...")
            images_base64, total_pages = self._pdf_to_base64_images(pdf_path, max_pages)

            if not images_base64:
                return {
                    "success": False,
                    "error": "PDFì—ì„œ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                }

            # ì´ë¯¸ì§€ í¬ê¸° ì²´í¬ (ì´ 50MB ì œí•œ)
            total_size = sum(len(img) for img in images_base64)
            if total_size > 50_000_000:  # 50MB
                logger.warning(f"ì´ë¯¸ì§€ í¬ê¸° ì´ˆê³¼: {total_size / 1_000_000:.1f}MB")
                # íŽ˜ì´ì§€ ìˆ˜ ì¤„ì´ê¸°
                reduced_pages = max(5, len(images_base64) // 2)
                images_base64 = images_base64[:reduced_pages]
                logger.info(f"íŽ˜ì´ì§€ ìˆ˜ë¥¼ {reduced_pages}ê°œë¡œ ì¤„ìž„")

            # 2. Claude APIë¡œ ì²˜ë¦¬
            self._emit_progress(
                progress_callback,
                "processing",
                f"Claude Opusë¡œ {len(images_base64)}íŽ˜ì´ì§€ ë¶„ì„ ì¤‘...",
            )

            result = self._process_with_claude(
                images_base64, output_mode, progress_callback
            )

            # 3. ê²°ê³¼ ì¡°í•©
            processing_time = time.time() - start_time

            final_result = {
                "success": True,
                "file_path": pdf_path,
                "total_pages": total_pages,
                "pages_read": len(images_base64),
                "content": result.get("content", ""),
                "char_count": len(result.get("content", "")),
                "structured_content": result.get("structured_content", {}),
                "financial_tables": result.get("financial_tables", {}),
                "processing_method": "claude_opus",
                "processing_time_seconds": processing_time,
                "cache_hit": False,
                "cached_at": datetime.utcnow().isoformat(),
            }

            # ìºì‹œì— ì €ìž¥
            _save_to_cache(cache_path, final_result)

            self._emit_progress(
                progress_callback,
                "complete",
                f"ì™„ë£Œ ({processing_time:.1f}ì´ˆ)",
                {"duration": processing_time},
            )

            return final_result

        except Exception as e:
            logger.error(f"Claude Vision ì²˜ë¦¬ ì‹¤íŒ¨: {e}", exc_info=True)
            return self._fallback_to_pymupdf(pdf_path, max_pages, str(e))

    def _pdf_to_base64_images(
        self, pdf_path: str, max_pages: int
    ) -> tuple:
        """PDFë¥¼ base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Returns:
            (images_base64: List[str], total_pages: int)
        """
        try:
            import fitz  # PyMuPDF
        except ImportError as e:
            raise ImportError(f"PDF ë³€í™˜ì— PyMuPDFê°€ í•„ìš”í•©ë‹ˆë‹¤: {e}")

        doc = None
        images_base64 = []

        try:
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            pages_to_read = min(total_pages, max_pages)

            dpi = DOLPHIN_CONFIG.get("image_dpi", 150)
            zoom = dpi / 72

            for i in range(pages_to_read):
                page = doc[i]
                mat = fitz.Matrix(zoom, zoom)
                pix = page.get_pixmap(matrix=mat)

                # PNGë¡œ ë³€í™˜ í›„ base64 ì¸ì½”ë”©
                img_bytes = pix.tobytes("png")
                img_base64 = base64.standard_b64encode(img_bytes).decode("utf-8")
                images_base64.append(img_base64)

            return images_base64, total_pages

        finally:
            if doc:
                doc.close()

    def _get_system_prompt(self) -> str:
        """VC íˆ¬ìž ë¶„ì„ ì „ë¬¸ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ 10ë…„ ì´ìƒ ê²½ë ¥ì˜ VC(ë²¤ì²˜ìºí”¼íƒˆ) íˆ¬ìžì‹¬ì‚¬ì—­ìž…ë‹ˆë‹¤. ìˆ˜ë°± ê±´ì˜ ìŠ¤íƒ€íŠ¸ì—… íˆ¬ìžë¥¼ ê²€í† í•œ ê²½í—˜ì´ ìžˆìŠµë‹ˆë‹¤.

## í•µì‹¬ ì—­ëŸ‰

### 1. ìž¬ë¬´ì œí‘œ ë¶„ì„ ì „ë¬¸ê°€
- ì†ìµê³„ì‚°ì„œ(P&L, IS): ë§¤ì¶œì•¡, ë§¤ì¶œì›ê°€, ë§¤ì¶œì´ì´ìµ, íŒê´€ë¹„, ì˜ì—…ì´ìµ, EBITDA, ë‹¹ê¸°ìˆœì´ìµ
- ìž¬ë¬´ìƒíƒœí‘œ(BS): ìœ ë™ìžì‚°, ë¹„ìœ ë™ìžì‚°, ì´ìžì‚°, ìœ ë™ë¶€ì±„, ë¹„ìœ ë™ë¶€ì±„, ì´ë¶€ì±„, ìžë³¸ì´ê³„
- í˜„ê¸ˆíë¦„í‘œ(CF): ì˜ì—…í™œë™CF, íˆ¬ìží™œë™CF, ìž¬ë¬´í™œë™CF, ê¸°ë§í˜„ê¸ˆ

### 2. íˆ¬ìžì¡°ê±´ ë¶„ì„
- Pre-money/Post-money ë°¸ë¥˜ì—ì´ì…˜
- íˆ¬ìžê¸ˆì•¡, íˆ¬ìžë‹¨ê°€(ì£¼ë‹¹ê°€ê²©), ì·¨ë“ì£¼ì‹ìˆ˜
- íˆ¬ìžìœ í˜•: ë³´í†µì£¼, ìš°ì„ ì£¼(RCPS), ì „í™˜ì‚¬ì±„(CB), SAFE
- íˆ¬ìžì¡°ê±´: ì²­ì‚°ìš°ì„ ê¶Œ, í¬ì„ë°©ì§€, ë™ë°˜ë§¤ê°ê¶Œ, ì´ì‚¬ì„ ìž„ê¶Œ

### 3. Cap Table ë¶„ì„
- ì£¼ì£¼ëª…, ë³´ìœ ì£¼ì‹ìˆ˜, ì§€ë¶„ìœ¨
- ì´ë°œí–‰ì£¼ì‹ìˆ˜, ì£¼ì‹ì¢…ë¥˜ë³„ êµ¬ë¶„
- íˆ¬ìž ë¼ìš´ë“œë³„ ë³€ë™ì‚¬í•­

### 4. ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ
- PER (Price to Earnings Ratio)
- PSR (Price to Sales Ratio)
- EV/EBITDA, EV/Revenue
- PBR (Price to Book Ratio)

## ì¶”ì¶œ ê·œì¹™

### ìˆ«ìž ì²˜ë¦¬
1. ë‹¨ìœ„ë¥¼ ë°˜ë“œì‹œ í™•ì¸í•˜ê³  ì›í™” ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
   - "100ì–µ" â†’ 10000000000
   - "50ë°±ë§Œì›" â†’ 50000000
   - "1.5ì¡°" â†’ 1500000000000
2. ì²œë‹¨ìœ„ ì½¤ë§ˆ ì œê±°: "1,234,567" â†’ 1234567
3. ìŒìˆ˜ëŠ” ê´„í˜¸ ë˜ëŠ” ë§ˆì´ë„ˆìŠ¤ë¡œ í‘œì‹œëœ ê²ƒ ëª¨ë‘ ì¸ì‹: (100) = -100
4. ë¹„ìœ¨/í¼ì„¼íŠ¸ëŠ” ì†Œìˆ˜ë¡œ ë³€í™˜: "15%" â†’ 0.15 (ë‹¨, metricsì—ì„œëŠ” ìˆ«ìž ê·¸ëŒ€ë¡œ)

### ì—°ë„ ì²˜ë¦¬
1. ì¶”ì •ì¹˜ êµ¬ë¶„: 2024E, 2025E, 2025(E), 2025ì˜ˆìƒ â†’ ì—°ë„ì— "E" í‘œì‹œ
2. ì‹¤ì ê³¼ ì¶”ì •ì¹˜ê°€ í˜¼ìž¬ëœ í…Œì´ë¸”ì€ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
3. ë°˜ê¸°/ë¶„ê¸° ë°ì´í„°ë„ ì¸ì‹: 1H24, 2Q24 ë“±

### í…Œì´ë¸” ì²˜ë¦¬
1. ë³‘í•©ëœ ì…€ì€ ë…¼ë¦¬ì ìœ¼ë¡œ ë¶„ë¦¬
2. ì†Œê³„/í•©ê³„ í–‰ì€ ë³„ë„ í‘œì‹œ
3. í—¤ë”ê°€ ì—¬ëŸ¬ ì¤„ì¸ ê²½ìš° í†µí•©í•˜ì—¬ ì¸ì‹

### íŠ¹ìˆ˜ ì¼€ì´ìŠ¤
1. "í‘ìžì „í™˜", "ì ìžì§€ì†" ë“± í…ìŠ¤íŠ¸ ì£¼ì„ë„ í•¨ê»˜ ì¶”ì¶œ
2. YoY ì„±ìž¥ë¥ ì´ ìžˆìœ¼ë©´ í•¨ê»˜ ì¶”ì¶œ
3. ì»¨ì„¼ì„œìŠ¤ vs íšŒì‚¬ì œì‹œ êµ¬ë¶„ì´ ìžˆìœ¼ë©´ í‘œì‹œ

## í’ˆì§ˆ ê¸°ì¤€
- ìˆ«ìž í•˜ë‚˜ë¼ë„ í‹€ë¦¬ë©´ íˆ¬ìž ì˜ì‚¬ê²°ì •ì— ì¹˜ëª…ì 
- ë¶ˆí™•ì‹¤í•œ ê²½ìš° í•´ë‹¹ í•„ë“œë¥¼ nullë¡œ ë‘ê³  warningsì— ê¸°ë¡
- í…Œì´ë¸”ì´ ìž˜ë ¤ìžˆê±°ë‚˜ ë¶ˆì™„ì „í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ ê²½ê³ """

    def _process_with_claude(
        self,
        images_base64: List[str],
        output_mode: str,
        progress_callback: Optional[Callable] = None,
    ) -> Dict[str, Any]:
        """Claude Vision APIë¡œ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        import httpx
        import anthropic

        # íƒ€ìž„ì•„ì›ƒ ì„¤ì • (5ë¶„)
        timeout_config = httpx.Timeout(
            timeout=DOLPHIN_CONFIG.get("timeout_seconds", 300),
            connect=30.0,
        )
        client = anthropic.Anthropic(timeout=timeout_config)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if output_mode == "tables_only":
            prompt = self._get_tables_only_prompt()
        elif output_mode == "structured":
            prompt = self._get_structured_prompt()
        else:  # text_only
            prompt = self._get_text_only_prompt()

        # ì´ë¯¸ì§€ ì½˜í…ì¸  êµ¬ì„±
        content = []
        for i, img_b64 in enumerate(images_base64):
            content.append({
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/png",
                    "data": img_b64,
                },
            })
            content.append({
                "type": "text",
                "text": f"[íŽ˜ì´ì§€ {i + 1}]"
            })

        content.append({
            "type": "text",
            "text": prompt
        })

        # Claude API í˜¸ì¶œ (Opus + ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
        response = client.messages.create(
            model="claude-opus-4-20250514",  # ìµœê³  ì„±ëŠ¥ Opus ì‚¬ìš©
            max_tokens=16384,  # ê¸´ ìž¬ë¬´ì œí‘œë„ ì²˜ë¦¬ ê°€ëŠ¥
            system=self._get_system_prompt(),
            messages=[
                {
                    "role": "user",
                    "content": content
                }
            ]
        )

        # ì‘ë‹µ íŒŒì‹±
        response_text = response.content[0].text if response.content else ""
        return self._parse_claude_response(response_text, output_mode)

    def _get_text_only_prompt(self) -> str:
        return """ìœ„ PDF íŽ˜ì´ì§€ë“¤ì˜ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ê·œì¹™:
1. ê° íŽ˜ì´ì§€ë¥¼ "============ íŽ˜ì´ì§€ N ============" í˜•ì‹ìœ¼ë¡œ êµ¬ë¶„
2. í…Œì´ë¸”ì€ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
3. ì›ë³¸ ë ˆì´ì•„ì›ƒê³¼ ìˆœì„œë¥¼ ìµœëŒ€í•œ ë³´ì¡´
4. ìˆ«ìžì™€ ë‹¨ìœ„ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œ (ì–µì›, ë°±ë§Œì› ë“±)

í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

    def _get_structured_prompt(self) -> str:
        return """ìœ„ PDF íŽ˜ì´ì§€ë“¤ì„ íˆ¬ìžì‹¬ì‚¬ ê´€ì ì—ì„œ ì² ì €ížˆ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

```json
{
  "content": "ì „ì²´ í…ìŠ¤íŠ¸ (íŽ˜ì´ì§€ë³„ ===íŽ˜ì´ì§€ N=== êµ¬ë¶„)",

  "company_info": {
    "name": "íšŒì‚¬ëª…",
    "industry": "ì‚°ì—…/ì—…ì¢…",
    "founded": "ì„¤ë¦½ì—°ë„",
    "employees": ì§ì›ìˆ˜,
    "business_model": "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìš”ì•½"
  },

  "investment_terms": {
    "found": true/false,
    "page": íŽ˜ì´ì§€ë²ˆí˜¸,
    "investment_amount": íˆ¬ìžê¸ˆì•¡(ì›),
    "pre_money_valuation": Pre-money(ì›),
    "post_money_valuation": Post-money(ì›),
    "price_per_share": ì£¼ë‹¹ê°€ê²©(ì›),
    "shares_acquired": ì·¨ë“ì£¼ì‹ìˆ˜,
    "ownership_percentage": ì·¨ë“ì§€ë¶„ìœ¨,
    "investment_type": "ë³´í†µì£¼/ìš°ì„ ì£¼/CB/SAFE",
    "investment_round": "ì‹œë“œ/í”„ë¦¬A/ì‹œë¦¬ì¦ˆA/B/C",
    "special_terms": ["ì²­ì‚°ìš°ì„ ê¶Œ", "í¬ì„ë°©ì§€", "ë™ë°˜ë§¤ê°ê¶Œ"]
  },

  "financial_tables": {
    "income_statement": {
      "found": true/false,
      "page": íŽ˜ì´ì§€ë²ˆí˜¸,
      "unit": "ì–µì›/ë°±ë§Œì›/ì²œì›",
      "source": "íšŒì‚¬ì œì‹œ/ì‹¬ì‚¬ì—­ì¶”ì •/ì»¨ì„¼ì„œìŠ¤",
      "years": ["2023", "2024E", "2025E", "2026E"],
      "metrics": {
        "revenue": [ë§¤ì¶œì•¡ë“¤],
        "revenue_growth_yoy": [YoYì„±ìž¥ë¥ ë“¤],
        "gross_profit": [ë§¤ì¶œì´ì´ìµë“¤],
        "gross_margin": [ë§¤ì¶œì´ì´ìµë¥ ë“¤],
        "operating_income": [ì˜ì—…ì´ìµë“¤],
        "operating_margin": [ì˜ì—…ì´ìµë¥ ë“¤],
        "ebitda": [EBITDAë“¤],
        "net_income": [ë‹¹ê¸°ìˆœì´ìµë“¤]
      }
    },
    "balance_sheet": {
      "found": true/false,
      "page": íŽ˜ì´ì§€ë²ˆí˜¸,
      "unit": "ì–µì›/ë°±ë§Œì›/ì²œì›",
      "years": ["2023", "2024E"],
      "metrics": {
        "total_assets": [ì´ìžì‚°ë“¤],
        "current_assets": [ìœ ë™ìžì‚°ë“¤],
        "total_liabilities": [ì´ë¶€ì±„ë“¤],
        "total_equity": [ìžë³¸ì´ê³„ë“¤],
        "cash_and_equivalents": [í˜„ê¸ˆë°í˜„ê¸ˆì„±ìžì‚°ë“¤],
        "debt": [ì°¨ìž…ê¸ˆë“¤]
      }
    },
    "cash_flow": {
      "found": true/false,
      "page": íŽ˜ì´ì§€ë²ˆí˜¸,
      "metrics": {
        "operating_cf": [ì˜ì—…í™œë™CFë“¤],
        "investing_cf": [íˆ¬ìží™œë™CFë“¤],
        "financing_cf": [ìž¬ë¬´í™œë™CFë“¤],
        "free_cash_flow": [FCFë“¤]
      }
    },
    "cap_table": {
      "found": true/false,
      "page": íŽ˜ì´ì§€ë²ˆí˜¸,
      "total_shares_issued": ì´ë°œí–‰ì£¼ì‹ìˆ˜,
      "shareholders": [
        {
          "name": "ì£¼ì£¼ëª…",
          "shares": ë³´ìœ ì£¼ì‹ìˆ˜,
          "percentage": ì§€ë¶„ìœ¨,
          "share_type": "ë³´í†µì£¼/ìš°ì„ ì£¼"
        }
      ],
      "option_pool": {
        "allocated": ë¶€ì—¬ëœìŠ¤í†¡ì˜µì…˜ìˆ˜,
        "remaining": ìž”ì—¬í’€
      }
    }
  },

  "valuation_metrics": {
    "per": PERë°°ìˆ˜,
    "psr": PSRë°°ìˆ˜,
    "ev_ebitda": EV/EBITDAë°°ìˆ˜,
    "ev_revenue": EV/Revenueë°°ìˆ˜
  },

  "data_validation": {
    "yoy_growth_check": [
      {
        "metric": "revenue",
        "year_from": "2023",
        "year_to": "2024E",
        "value_from": ì´ì „ê°’,
        "value_to": ì´í›„ê°’,
        "calculated_growth": ê³„ì‚°ëœì„±ìž¥ë¥ ,
        "stated_growth": IRìžë£Œì—ëª…ì‹œëœì„±ìž¥ë¥ _ë˜ëŠ”_null,
        "match": true/false,
        "discrepancy": "ì°¨ì´ê°€ ìžˆìœ¼ë©´ ì„¤ëª…"
      }
    ],
    "margin_consistency": [
      {
        "metric": "operating_margin",
        "year": "2024E",
        "calculated": ì˜ì—…ì´ìµ/ë§¤ì¶œ*100,
        "stated": IRìžë£Œì—ëª…ì‹œëœê°’,
        "match": true/false
      }
    ],
    "cap_table_check": {
      "sum_of_shares": ì£¼ì£¼ë³„ë³´ìœ ì£¼ì‹í•©ê³„,
      "total_shares_stated": ì´ë°œí–‰ì£¼ì‹ìˆ˜,
      "match": true/false
    },
    "valuation_check": {
      "pre_money_stated": Pre-money(IRìžë£Œ),
      "calculated_from_per": ë‹¹ê¸°ìˆœì´ìµ*PER,
      "calculated_from_psr": ë§¤ì¶œ*PSR,
      "reasonable": true/false,
      "notes": "ë°¸ë¥˜ì—ì´ì…˜ ì •í•©ì„± ì½”ë©˜íŠ¸"
    }
  },

  "key_risks": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2"],

  "warnings": ["ë¶ˆì™„ì „í•˜ê±°ë‚˜ ë¶ˆí™•ì‹¤í•œ ë°ì´í„°ì— ëŒ€í•œ ê²½ê³ "],

  "data_source_labels": {
    "financial_data": "íšŒì‚¬ì œì‹œ/ì‹¬ì‚¬ì—­ì¶”ì •/ì™¸ë¶€ìžë£Œ",
    "valuation": "íšŒì‚¬ì œì‹œ/ì‹œìž¥ê°€ê²©",
    "cap_table": "íšŒì‚¬ì œì‹œ/ë“±ê¸°ë¶€ë“±ë³¸"
  },

  "missing_data": {
    "has_missing": true/false,
    "critical_missing": [
      {
        "field": "income_statement",
        "reason": "ì†ìµê³„ì‚°ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
        "suggestion": "ìž¬ë¬´ì œí‘œê°€ í¬í•¨ëœ íŽ˜ì´ì§€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”",
        "priority": "high"
      },
      {
        "field": "cap_table",
        "reason": "ì£¼ì£¼í˜„í™© ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
        "suggestion": "Cap Table ë˜ëŠ” ì£¼ì£¼ëª…ë¶€ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”",
        "priority": "high"
      },
      {
        "field": "investment_terms",
        "reason": "íˆ¬ìžì¡°ê±´ì´ ëª…ì‹œë˜ì–´ ìžˆì§€ ì•ŠìŠµë‹ˆë‹¤",
        "suggestion": "í…€ì‹¯(Term Sheet) ë˜ëŠ” íˆ¬ìžê³„ì•½ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”",
        "priority": "medium"
      }
    ],
    "optional_missing": [
      {
        "field": "cash_flow",
        "reason": "í˜„ê¸ˆíë¦„í‘œê°€ ì—†ìŠµë‹ˆë‹¤ (ì„ íƒì‚¬í•­)",
        "priority": "low"
      }
    ],
    "request_message": "íˆ¬ìž ë¶„ì„ì„ ìœ„í•´ ë‹¤ìŒ ìžë£Œê°€ ì¶”ê°€ë¡œ í•„ìš”í•©ë‹ˆë‹¤:\n1. [í•„ìˆ˜] ìž¬ë¬´ì œí‘œ (ì†ìµê³„ì‚°ì„œ)\n2. [í•„ìˆ˜] Cap Table\n\níŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ë¡œ ìž…ë ¥í•´ì£¼ì„¸ìš”."
  }
}
```

## ì¶”ì¶œ ê·œì¹™

1. **ìˆ«ìž ë³€í™˜**: ëª¨ë“  ê¸ˆì•¡ì€ ì›í™” ê¸°ì¤€ ì •ìˆ˜ë¡œ ë³€í™˜
   - "100ì–µ" â†’ 10000000000
   - "5ì²œë§Œì›" â†’ 50000000
   - í‘œì— ë‹¨ìœ„ê°€ ëª…ì‹œë˜ì–´ ìžˆìœ¼ë©´ (ë‹¨ìœ„: ë°±ë§Œì›) í•´ë‹¹ ë‹¨ìœ„ ì ìš©

2. **ì—°ë„ í‘œê¸°**: ì¶”ì •ì¹˜ëŠ” "E" ë¶™ì—¬ì„œ í‘œì‹œ (2024E, 2025E)

3. **ë¹„ìœ¨**: í¼ì„¼íŠ¸ëŠ” ìˆ«ìž ê·¸ëŒ€ë¡œ (15.5% â†’ 15.5)

4. **ëˆ„ë½ ë°ì´í„°**: ì°¾ì„ ìˆ˜ ì—†ëŠ” í•„ë“œëŠ” null, í…Œì´ë¸” ìžì²´ê°€ ì—†ìœ¼ë©´ found: false

5. **ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤**: íšŒì‚¬ì œì‹œ/ì‹¬ì‚¬ì—­ì¶”ì •ì´ ë‹¤ë¥´ë©´ source í•„ë“œë¡œ êµ¬ë¶„

6. **ì •í•©ì„± ê²€ì¦ (ë§¤ìš° ì¤‘ìš”)**:
   - YoY ì„±ìž¥ë¥ : ì§ì ‘ ê³„ì‚°í•œ ê°’ê³¼ IRìžë£Œì— ëª…ì‹œëœ ê°’ ë¹„êµ
   - ë§ˆì§„ìœ¨: ì˜ì—…ì´ìµÃ·ë§¤ì¶œ ê³„ì‚°ê°’ê³¼ ëª…ì‹œëœ ê°’ ë¹„êµ
   - Cap Table: ì£¼ì£¼ë³„ ì§€ë¶„ í•©ê³„ = 100% í™•ì¸
   - ë°¸ë¥˜ì—ì´ì…˜: PER/PSR ì—­ì‚°ê°’ê³¼ ì œì‹œ ë°¸ë¥˜ì—ì´ì…˜ ë¹„êµ

7. **ë°ì´í„° ì¶œì²˜ ëª…ì‹œ**: ê° ë°ì´í„°ê°€ íšŒì‚¬ì œì‹œ/ì‹¬ì‚¬ì—­ì¶”ì •/ì™¸ë¶€ìžë£Œ ì¤‘ ì–´ë””ì„œ ì™”ëŠ”ì§€ í‘œì‹œ

8. **JSONë§Œ ì¶œë ¥**: ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ìˆœìˆ˜ JSONë§Œ ë°˜í™˜"""

    def _get_tables_only_prompt(self) -> str:
        return """ìœ„ PDF íŽ˜ì´ì§€ë“¤ì—ì„œ í…Œì´ë¸”ë§Œ ì¶”ì¶œí•˜ì—¬ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”:

```json
{
  "tables": [
    {
      "page": íŽ˜ì´ì§€ë²ˆí˜¸,
      "title": "í…Œì´ë¸” ì œëª© (ìžˆìœ¼ë©´)",
      "markdown": "| ì—´1 | ì—´2 |\\n|---|---|\\n| ê°’1 | ê°’2 |",
      "rows": [["í—¤ë”1", "í—¤ë”2"], ["ê°’1", "ê°’2"]]
    }
  ],
  "financial_tables": {
    "income_statement": {...},
    "balance_sheet": {...},
    "cash_flow": {...},
    "cap_table": {...}
  }
}
```

ê·œì¹™:
1. ëª¨ë“  í…Œì´ë¸”ì„ ë¹ ì§ì—†ì´ ì¶”ì¶œ
2. ìž¬ë¬´ì œí‘œëŠ” financial_tablesì— ë³„ë„ êµ¬ì¡°í™”
3. ìˆ«ìžì™€ ë‹¨ìœ„ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì¶œ
4. JSONë§Œ ì¶œë ¥"""

    def _parse_claude_response(
        self, response_text: str, output_mode: str
    ) -> Dict[str, Any]:
        """Claude ì‘ë‹µ íŒŒì‹±"""
        import json
        import re

        if output_mode == "text_only":
            return {"content": response_text}

        # JSON ì¶”ì¶œ ì‹œë„
        try:
            # ```json ... ``` ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # ì „ì²´ê°€ JSONì¸ ê²½ìš°
                json_str = response_text.strip()

            parsed = json.loads(json_str)

            return {
                "content": parsed.get("content", response_text),
                "structured_content": parsed.get("structured_content", {}),
                "financial_tables": parsed.get("financial_tables", {}),
            }

        except json.JSONDecodeError:
            logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜")
            return {"content": response_text}

    def _fallback_to_pymupdf(
        self, pdf_path: str, max_pages: int, reason: str
    ) -> Dict[str, Any]:
        """PyMuPDFë¡œ í´ë°±"""
        logger.info(f"PyMuPDFë¡œ í´ë°±: {reason}")

        try:
            import fitz

            doc = None
            try:
                doc = fitz.open(pdf_path)
                total_pages = len(doc)
                pages_to_read = min(total_pages, max_pages)

                text_content = []
                for i in range(pages_to_read):
                    page = doc[i]
                    text = page.get_text()
                    text_content.append(
                        f"\n{'='*60}\níŽ˜ì´ì§€ {i+1}\n{'='*60}\n{text}"
                    )

                content = "".join(text_content)

                return {
                    "success": True,
                    "file_path": pdf_path,
                    "total_pages": total_pages,
                    "pages_read": pages_to_read,
                    "content": content,
                    "char_count": len(content),
                    "processing_method": "pymupdf_fallback",
                    "fallback_used": True,
                    "fallback_reason": reason,
                    "cache_hit": False,
                    "cached_at": datetime.utcnow().isoformat(),
                }

            finally:
                if doc:
                    doc.close()

        except Exception as e:
            logger.error(f"í´ë°± ì²˜ë¦¬ë„ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "fallback_attempted": True,
                "original_error": reason,
            }

    def _emit_progress(
        self,
        callback: Optional[Callable],
        stage: str,
        message: str,
        data: Optional[Dict[str, Any]] = None,
    ) -> None:
        """ì§„í–‰ ìƒí™© ì´ë²¤íŠ¸ ë°œìƒ"""
        if callback:
            event = {
                "type": "info",
                "content": message,
                "data": {"stage": stage, **(data or {})},
            }
            try:
                callback(event)
            except Exception as e:
                logger.warning(f"ì§„í–‰ ì½œë°± ì‹¤íŒ¨: {e}")


# ê¸°ì¡´ DolphinProcessorë¥¼ ClaudeVisionProcessorë¡œ ëŒ€ì²´
DolphinProcessor = ClaudeVisionProcessor


def process_pdf_with_claude(
    pdf_path: str,
    max_pages: int = None,
    output_mode: str = "structured",
    progress_callback: Optional[Callable] = None,
) -> Dict[str, Any]:
    """PDFë¥¼ Claude Visionìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” íŽ¸ì˜ í•¨ìˆ˜"""
    processor = ClaudeVisionProcessor()
    return processor.process_pdf(
        pdf_path=pdf_path,
        max_pages=max_pages,
        output_mode=output_mode,
        progress_callback=progress_callback,
    )


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
process_pdf_with_dolphin = process_pdf_with_claude


class InteractiveAnalysisSession:
    """ëŒ€í™”í˜• íˆ¬ìž ë¶„ì„ ì„¸ì…˜

    ì—¬ëŸ¬ íŒŒì¼ê³¼ í…ìŠ¤íŠ¸ ìž…ë ¥ì„ ë°›ì•„ì„œ ì ì§„ì ìœ¼ë¡œ ë¶„ì„ì„ ì™„ì„±í•©ë‹ˆë‹¤.
    """

    def __init__(self, session_id: str = None):
        self.session_id = session_id or datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        self.processor = ClaudeVisionProcessor()
        self.accumulated_data = {
            "company_info": {},
            "investment_terms": {"found": False},
            "financial_tables": {
                "income_statement": {"found": False},
                "balance_sheet": {"found": False},
                "cash_flow": {"found": False},
                "cap_table": {"found": False},
            },
            "valuation_metrics": {},
            "source_files": [],
            "text_inputs": [],
        }
        self.missing_data = []

    def add_pdf(self, pdf_path: str, max_pages: int = 30) -> Dict[str, Any]:
        """PDF íŒŒì¼ ì¶”ê°€ ë° ë¶„ì„"""
        result = self.processor.process_pdf(
            pdf_path=pdf_path,
            max_pages=max_pages,
            output_mode="structured",
        )

        if result.get("success"):
            self._merge_data(result)
            self.accumulated_data["source_files"].append(pdf_path)

        return self._get_status()

    def add_text_input(self, text: str, data_type: str = "general") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìž…ë ¥ ì¶”ê°€ (ìž¬ë¬´ ë°ì´í„°, Cap Table ë“±)

        Args:
            text: ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ í…ìŠ¤íŠ¸
            data_type: "financial", "cap_table", "investment_terms", "general"
        """
        import anthropic

        client = anthropic.Anthropic()

        prompt = self._get_text_parsing_prompt(text, data_type)

        response = client.messages.create(
            model="claude-opus-4-20250514",
            max_tokens=4096,
            system=self.processor._get_system_prompt(),
            messages=[{"role": "user", "content": prompt}]
        )

        try:
            import json
            import re

            response_text = response.content[0].text
            json_match = re.search(r"```json\s*(.*?)\s*```", response_text, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(1))
            else:
                parsed = json.loads(response_text.strip())

            self._merge_parsed_text(parsed, data_type)
            self.accumulated_data["text_inputs"].append({
                "type": data_type,
                "content": text[:500],  # ìš”ì•½ ì €ìž¥
            })

        except Exception as e:
            return {
                "success": False,
                "error": f"í…ìŠ¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {str(e)}",
            }

        return self._get_status()

    def _get_text_parsing_prompt(self, text: str, data_type: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì‹±ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        base_prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ íˆ¬ìž ë¶„ì„ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

"""
        if data_type == "financial":
            base_prompt += """
ìž¬ë¬´ ì •ë³´ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ:
```json
{
  "income_statement": {
    "found": true,
    "years": ["2023", "2024E"],
    "metrics": {
      "revenue": [ê°’ë“¤],
      "operating_income": [ê°’ë“¤],
      "net_income": [ê°’ë“¤]
    }
  }
}
```"""
        elif data_type == "cap_table":
            base_prompt += """
Cap Tableì„ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ:
```json
{
  "cap_table": {
    "found": true,
    "total_shares_issued": ì´ì£¼ì‹ìˆ˜,
    "shareholders": [
      {"name": "ì£¼ì£¼ëª…", "shares": ì£¼ì‹ìˆ˜, "percentage": ì§€ë¶„ìœ¨}
    ]
  }
}
```"""
        elif data_type == "investment_terms":
            base_prompt += """
íˆ¬ìž ì¡°ê±´ì„ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ:
```json
{
  "investment_terms": {
    "found": true,
    "investment_amount": íˆ¬ìžê¸ˆì•¡,
    "pre_money_valuation": Pre-money,
    "price_per_share": ì£¼ë‹¹ê°€ê²©,
    "investment_type": "ë³´í†µì£¼/ìš°ì„ ì£¼/CB"
  }
}
```"""
        else:
            base_prompt += """
ê´€ë ¨ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
```json
{
  "extracted_info": {...}
}
```"""

        return base_prompt

    def _merge_data(self, new_data: Dict[str, Any]) -> None:
        """ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©"""
        # íšŒì‚¬ ì •ë³´
        if new_data.get("company_info"):
            self.accumulated_data["company_info"].update(new_data["company_info"])

        # íˆ¬ìž ì¡°ê±´
        if new_data.get("investment_terms", {}).get("found"):
            self.accumulated_data["investment_terms"] = new_data["investment_terms"]

        # ìž¬ë¬´ í…Œì´ë¸”
        financial = new_data.get("financial_tables", {})
        for table_type in ["income_statement", "balance_sheet", "cash_flow", "cap_table"]:
            if financial.get(table_type, {}).get("found"):
                self.accumulated_data["financial_tables"][table_type] = financial[table_type]

        # ë°¸ë¥˜ì—ì´ì…˜ ì§€í‘œ
        if new_data.get("valuation_metrics"):
            self.accumulated_data["valuation_metrics"].update(new_data["valuation_metrics"])

    def _merge_parsed_text(self, parsed: Dict[str, Any], data_type: str) -> None:
        """íŒŒì‹±ëœ í…ìŠ¤íŠ¸ ë°ì´í„° ë³‘í•©"""
        if data_type == "financial" and parsed.get("income_statement"):
            self.accumulated_data["financial_tables"]["income_statement"] = parsed["income_statement"]
        elif data_type == "cap_table" and parsed.get("cap_table"):
            self.accumulated_data["financial_tables"]["cap_table"] = parsed["cap_table"]
        elif data_type == "investment_terms" and parsed.get("investment_terms"):
            self.accumulated_data["investment_terms"] = parsed["investment_terms"]

    def _get_status(self) -> Dict[str, Any]:
        """í˜„ìž¬ ë¶„ì„ ìƒíƒœ ë°˜í™˜"""
        missing = []
        critical_missing = []

        # í•„ìˆ˜ ë°ì´í„° ì²´í¬
        if not self.accumulated_data["financial_tables"]["income_statement"].get("found"):
            critical_missing.append({
                "field": "income_statement",
                "name": "ì†ìµê³„ì‚°ì„œ",
                "suggestion": "ìž¬ë¬´ì œí‘œ PDFë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜, ë§¤ì¶œ/ì˜ì—…ì´ìµ/ìˆœì´ìµì„ í…ìŠ¤íŠ¸ë¡œ ìž…ë ¥í•´ì£¼ì„¸ìš”.\nì˜ˆ: '2024ë…„ ë§¤ì¶œ 100ì–µ, ì˜ì—…ì´ìµ 20ì–µ, ìˆœì´ìµ 15ì–µ'",
            })

        if not self.accumulated_data["financial_tables"]["cap_table"].get("found"):
            critical_missing.append({
                "field": "cap_table",
                "name": "Cap Table (ì£¼ì£¼í˜„í™©)",
                "suggestion": "Cap Tableì„ ì—…ë¡œë“œí•˜ê±°ë‚˜, ì£¼ì£¼í˜„í™©ì„ í…ìŠ¤íŠ¸ë¡œ ìž…ë ¥í•´ì£¼ì„¸ìš”.\nì˜ˆ: 'ëŒ€í‘œì´ì‚¬ 60%, íˆ¬ìžìžA 20%, ìŠ¤í†¡ì˜µì…˜í’€ 20%'",
            })

        if not self.accumulated_data["investment_terms"].get("found"):
            missing.append({
                "field": "investment_terms",
                "name": "íˆ¬ìž ì¡°ê±´",
                "suggestion": "íˆ¬ìžê¸ˆì•¡, ë°¸ë¥˜ì—ì´ì…˜, ì£¼ë‹¹ê°€ê²© ë“±ì„ ìž…ë ¥í•´ì£¼ì„¸ìš”.\nì˜ˆ: 'íˆ¬ìžê¸ˆì•¡ 30ì–µ, Pre-money 100ì–µ, ì£¼ë‹¹ê°€ê²© 10,000ì›'",
            })

        # ìƒíƒœ ë©”ì‹œì§€ ìƒì„±
        if critical_missing:
            status = "incomplete"
            message = "ðŸ”´ í•„ìˆ˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤:\n"
            for item in critical_missing:
                message += f"\n**{item['name']}**\n{item['suggestion']}\n"
        elif missing:
            status = "partial"
            message = "ðŸŸ¡ ì¶”ê°€ ë°ì´í„°ê°€ ìžˆìœ¼ë©´ ë” ì •í™•í•œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:\n"
            for item in missing:
                message += f"\n**{item['name']}**\n{item['suggestion']}\n"
        else:
            status = "complete"
            message = "ðŸŸ¢ ëª¨ë“  í•„ìˆ˜ ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê°€ëŠ¥í•©ë‹ˆë‹¤."

        return {
            "success": True,
            "session_id": self.session_id,
            "status": status,
            "message": message,
            "critical_missing": critical_missing,
            "optional_missing": missing,
            "accumulated_data": self.accumulated_data,
            "source_count": {
                "files": len(self.accumulated_data["source_files"]),
                "text_inputs": len(self.accumulated_data["text_inputs"]),
            },
        }

    def get_final_analysis(self) -> Dict[str, Any]:
        """ìµœì¢… ë¶„ì„ ê²°ê³¼ ë°˜í™˜"""
        status = self._get_status()

        if status["status"] == "incomplete":
            return {
                "success": False,
                "error": "í•„ìˆ˜ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                "missing": status["critical_missing"],
                "message": status["message"],
            }

        return {
            "success": True,
            "session_id": self.session_id,
            "analysis": self.accumulated_data,
            "data_quality": "complete" if status["status"] == "complete" else "partial",
            "warnings": status.get("optional_missing", []),
        }


# ì„¸ì…˜ ê´€ë¦¬ (í´ë°±ìš© - Streamlit ì™¸ë¶€ì—ì„œ ì‚¬ìš©)
_active_sessions: Dict[str, InteractiveAnalysisSession] = {}


def _get_streamlit_session_storage() -> Dict[str, InteractiveAnalysisSession]:
    """Streamlit session_state ê¸°ë°˜ ì„¸ì…˜ ì €ìž¥ì†Œ ë°˜í™˜ (ê°€ëŠ¥í•œ ê²½ìš°)"""
    try:
        import streamlit as st
        if "analysis_sessions" not in st.session_state:
            st.session_state.analysis_sessions = {}
        return st.session_state.analysis_sessions
    except (ImportError, RuntimeError):
        # Streamlit ì™¸ë¶€ì—ì„œ ì‹¤í–‰ ì¤‘ì´ê±°ë‚˜ context ì—†ìŒ
        return _active_sessions


def get_or_create_session(session_id: str = None) -> InteractiveAnalysisSession:
    """ë¶„ì„ ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±

    Streamlit í™˜ê²½ì—ì„œëŠ” st.session_stateë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì €ë³„ ì„¸ì…˜ ë¶„ë¦¬.
    ê·¸ ì™¸ í™˜ê²½ì—ì„œëŠ” ëª¨ë“ˆ ë ˆë²¨ ë”•ì…”ë„ˆë¦¬ ì‚¬ìš©.
    """
    storage = _get_streamlit_session_storage()

    if session_id and session_id in storage:
        return storage[session_id]

    session = InteractiveAnalysisSession(session_id)
    storage[session.session_id] = session
    return session


def clear_session(session_id: str) -> None:
    """ì„¸ì…˜ ì‚­ì œ"""
    storage = _get_streamlit_session_storage()
    if session_id in storage:
        del storage[session_id]
