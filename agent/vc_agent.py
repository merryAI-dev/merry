"""
Unified VC Investment Agent - Single Agent Architecture

í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ê°€ ëª¨ë“  ìž‘ì—…ì„ ìˆ˜í–‰:
- ëŒ€í™”í˜• ëª¨ë“œ (chat)
- ìžìœ¨ ì‹¤í–‰ ëª¨ë“œ (goal)
- ë„êµ¬ ì‹¤í–‰
"""

import os
import json
import re
from datetime import date, timedelta
from typing import Any, AsyncIterator, Dict, List, Optional
from dotenv import load_dotenv

from anthropic import Anthropic, AsyncAnthropic
from .tools import register_tools, execute_tool
from .memory import ChatMemory
from .feedback import FeedbackSystem
from shared.logging_config import get_logger

load_dotenv()

logger = get_logger("vc_agent")

# ì•ˆì „ìž¥ì¹˜: ìµœëŒ€ ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜
MAX_TOOL_STEPS = 15


class VCAgent:
    """
    í†µí•© VC íˆ¬ìž ë¶„ì„ ì—ì´ì „íŠ¸

    ë‹¨ì¼ ì—ì´ì „íŠ¸ë¡œ ëª¨ë“  ìž‘ì—… ìˆ˜í–‰:
    - chat(message): ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤
    - achieve_goal(goal): ìžìœ¨ ì‹¤í–‰
    - execute_tool(tool, params): ì§ì ‘ ë„êµ¬ ì‹¤í–‰
    """

    def __init__(
        self,
        api_key: str = None,
        model: str = "claude-opus-4-5-20251101",
        user_id: str = None,
        member_name: str = None,
        team_id: str = None,
    ):
        """
        Args:
            api_key: Anthropic API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜)
            model: Claude ëª¨ë¸ (ê¸°ë³¸: Opus 4.5)
            user_id: ì‚¬ìš©ìž ê³ ìœ  ID (ê°™ì€ IDë¼ë¦¬ ì„¸ì…˜/í”¼ë“œë°± ê³µìœ )
        """
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.user_id = user_id or "anonymous"
        self.member_name = member_name
        self.team_id = team_id or self.user_id

        if not self.api_key:
            raise ValueError(
                "ANTHROPIC_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                ".env íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •í•˜ì„¸ìš”."
            )

        # Anthropic SDK
        self.client = Anthropic(api_key=self.api_key)
        self.async_client = AsyncAnthropic(api_key=self.api_key)
        self.model = model

        # ë„êµ¬ ë“±ë¡
        self.tools = register_tools()

        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ (ê¸°ë³¸/ë³´ì´ìŠ¤ ë¶„ë¦¬)
        self.conversation_history: List[Dict[str, Any]] = []
        self.voice_conversation_history: List[Dict[str, Any]] = []

        # ìž‘ì—… ì»¨í…ìŠ¤íŠ¸
        self.context = {
            "analyzed_files": [],
            "cached_results": {},
            "last_analysis": None
        }

        # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ (user_id ê¸°ë°˜)
        self.memory = ChatMemory(user_id=self.user_id)

        # í”¼ë“œë°± ì‹œìŠ¤í…œ (user_id ê¸°ë°˜)
        self.feedback = FeedbackSystem(user_id=self.user_id)

        # ë§ˆì§€ë§‰ ì‘ë‹µ ì €ìž¥ (í”¼ë“œë°±ìš©)
        self.last_interaction = {
            "user_message": None,
            "assistant_response": None,
            "context": {}
        }

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.token_usage = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "session_calls": 0
        }

        # ë„êµ¬ í˜¸ì¶œ ì¹´ìš´í„° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        self._tool_step_count = 0

        # ë³´ê³ ì„œ ëª¨ë“œ: í•­ìƒ ì‹¬í™” ì˜ê²¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
        self.report_deep_mode = True

    # ========================================
    # System Prompt
    # ========================================

    def _build_system_prompt(self, mode: str = "exit", context_text: Optional[str] = None) -> str:
        """ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            mode: "exit" (Exit í”„ë¡œì ì…˜), "peer" (Peer PER ë¶„ì„), "diagnosis", "report"
        """

        analyzed_files = ", ".join(self.context["analyzed_files"]) if self.context["analyzed_files"] else "ì—†ìŒ"

        if mode.startswith("voice_"):
            submode = mode.split("_", 1)[1] if "_" in mode else "chat"
            return self._build_voice_system_prompt(submode, context_text)

        # Peer PER ë¶„ì„ ëª¨ë“œ
        if mode == "peer":
            return self._build_peer_system_prompt(analyzed_files)

        # ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ ëª¨ë“œ
        if mode == "diagnosis":
            return self._build_diagnosis_system_prompt(analyzed_files)

        # íˆ¬ìžì‹¬ì‚¬ ë³´ê³ ì„œ/ì¸ìˆ˜ì¸ì˜ê²¬ ëª¨ë“œ
        if mode == "report":
            return self._build_report_system_prompt(analyzed_files)

        # Exit í”„ë¡œì ì…˜ ëª¨ë“œ (ê¸°ë³¸)
        return f"""ë‹¹ì‹ ì€ **VC íˆ¬ìž ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸**ìž…ë‹ˆë‹¤.

## í˜„ìž¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {len(self.context["cached_results"])}ê°œ

## âš ï¸ ì ˆëŒ€ ê·œì¹™ (CRITICAL)

**ì ˆëŒ€ë¡œ ë„êµ¬ ì—†ì´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”!**

- ì—‘ì…€ íŒŒì¼ ë¶„ì„ â†’ ë°˜ë“œì‹œ read_excel_as_text ë˜ëŠ” analyze_excel ì‚¬ìš©
- Exit í”„ë¡œì ì…˜ ìƒì„± â†’ ë°˜ë“œì‹œ analyze_and_generate_projection ì‚¬ìš©
- ì¶”ì¸¡í•˜ê±°ë‚˜ ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€ â†’ ì‹¤ì œ ë„êµ¬ë¥¼ ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ ì–»ì–´ì•¼ í•¨
- í…ìŠ¤íŠ¸ë¡œë§Œ "ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤" ê°™ì€ ê±°ì§“ ì‘ë‹µ ì ˆëŒ€ ê¸ˆì§€

**ì‚¬ìš©ìžê°€ íŒŒì¼ ê²½ë¡œë¥¼ ì£¼ë©´ ì¦‰ì‹œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”!**

## í•µì‹¬ ì—­ëŸ‰

### 1. ìœ ì—°í•œ ì—‘ì…€ ë¶„ì„
- **read_excel_as_text**: ì—‘ì…€ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì½ê¸° (êµ¬ì¡°ê°€ ë‹¤ì–‘í•´ë„ OK)
- **analyze_excel**: ìžë™ íŒŒì‹± (íˆ¬ìžì¡°ê±´, ISìš”ì•½, Cap Table)
- ì—‘ì…€ êµ¬ì¡°ê°€ íŠ¹ì´í•˜ê±°ë‚˜ ë³µìž¡í•˜ë©´ read_excel_as_textë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ì„¸ìš”

### 2. ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
- PER, EV/Revenue, EV/EBITDA ë“± ëª¨ë“  ë°¸ë¥˜ì—ì´ì…˜ ë°©ë²•ë¡ 
- ì „ì²´ ë§¤ê°, ë¶€ë¶„ ë§¤ê°, SAFE ì „í™˜, ì½œì˜µì…˜ ë“±
- ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ì–´ë–¤ ì¡°í•©ë„ ê³„ì‚° ê°€ëŠ¥

### 3. Exit í”„ë¡œì ì…˜ ìƒì„±
- **analyze_and_generate_projection**: ì—‘ì…€ ë¶„ì„ í›„ ì¦‰ì‹œ Exit í”„ë¡œì ì…˜ ìƒì„±
- ì—°ë„, PER ë°°ìˆ˜, íšŒì‚¬ëª… ë“±ì„ ì§€ì •í•˜ì—¬ ë§žì¶¤í˜• ì—‘ì…€ ìƒì„±

## ìž‘ì—… ë°©ì‹

### ì—‘ì…€ íŒŒì¼ì„ ë°›ìœ¼ë©´:
1. **ì¦‰ì‹œ** read_excel_as_text ë„êµ¬ í˜¸ì¶œ (êµ¬ì¡° íŒŒì•…)
2. í…ìŠ¤íŠ¸ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ (íˆ¬ìžê¸ˆì•¡, ë‹¹ê¸°ìˆœì´ìµ, ì´ì£¼ì‹ìˆ˜ ë“±)
3. ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ë¶„ì„ ìˆ˜í–‰
4. **ì¦‰ì‹œ** analyze_and_generate_projection ë„êµ¬ í˜¸ì¶œ (Exit í”„ë¡œì ì…˜ ìƒì„±)
5. ê²°ê³¼ ì„¤ëª…

### ì˜ˆì‹œ ì›Œí¬í”Œë¡œìš°:
```
ì‚¬ìš©ìž: "temp/íŒŒì¼.xlsxë¥¼ 2030ë…„ PER 10,20,30ë°°ë¡œ ë¶„ì„í•´ì¤˜"

ìž˜ëª»ëœ ì‘ë‹µ:
"ë¶„ì„ì„ ì‹œìž‘í•˜ê² ìŠµë‹ˆë‹¤. ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"

ì˜¬ë°”ë¥¸ ì‘ë‹µ:
1. read_excel_as_text ë„êµ¬ë¥¼ ì¦‰ì‹œ í˜¸ì¶œ
2. ì‹¤ì œ ì—‘ì…€ ë‚´ìš©ì„ ì½ì–´ì„œ ì •ë³´ ì¶”ì¶œ
3. analyze_and_generate_projection ë„êµ¬ë¥¼ ì¦‰ì‹œ í˜¸ì¶œ
4. ìƒì„±ëœ íŒŒì¼ ê²½ë¡œì™€ ê²°ê³¼ë¥¼ ì‚¬ìš©ìžì—ê²Œ ì•Œë ¤ì¤Œ
```

## ì¤‘ìš” ì›ì¹™
- **ë„êµ¬ ìš°ì„ **: í•­ìƒ ë„êµ¬ë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ê³ , ì‹¤ì œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
- **ì¶”ì¸¡ ê¸ˆì§€**: ì—‘ì…€ ë‚´ìš©ì„ ëª¨ë¥´ë©´ read_excel_as_textë¡œ ì½ì–´ì•¼ í•¨
- **ì‹¤í–‰ í™•ì¸**: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„ì—ë§Œ ì„±ê³µ ì—¬ë¶€ë¥¼ ì•Œë ¤ì¤Œ
- **ëª…í™•í•œ ì„¤ëª…**: IRR, ë©€í‹°í”Œ, ê¸°ì—…ê°€ì¹˜ ë“±ì„ ì‹¤ì œ ìˆ«ìžë¡œ ì„¤ëª…

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬
{json.dumps([t["name"] for t in self.tools], ensure_ascii=False, indent=2)}

## ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

**ë§¤ìš° ì¤‘ìš”: ì´ ë¶„ì„ì€ íˆ¬ìžì‹¬ì‚¬ ë³´ê³ ì„œì— ì‚¬ìš©ë©ë‹ˆë‹¤.**

- **ì „ë¬¸ì ì´ê³  ì§„ì¤‘í•œ í†¤**: ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€ (âœ…âŒðŸ“ŠðŸ“ˆ ë“±)
- **ì •í™•í•œ ìˆ˜ì¹˜**: ëª¨ë“  ìž¬ë¬´ ì§€í‘œëŠ” ì •í™•í•œ ìˆ«ìžë¡œ ì œì‹œ
- **ê°ê´€ì  ë¶„ì„**: ê°ì •ì  í‘œí˜„ ë°°ì œ, ì‚¬ì‹¤ ê¸°ë°˜ ë¶„ì„
- **ëª…í™•í•œ êµ¬ì¡°**: ì œëª©, í•­ëª©, ìˆ˜ì¹˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
- **ë³´ê³ ì„œ í’ˆì§ˆ**: íˆ¬ìžì‹¬ì‚¬ì—­ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìžˆëŠ” ìˆ˜ì¤€ì˜ ë¶„ì„

ì˜ˆì‹œ:
- ë‚˜ìœ ì˜ˆ: "âœ… ë¶„ì„ ì™„ë£Œí–ˆì–´ìš”! ðŸ˜Š"
- ì¢‹ì€ ì˜ˆ: "ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."

- ë‚˜ìœ ì˜ˆ: "IRRì´ 35%ë„¤ìš”! ðŸ‘"
- ì¢‹ì€ ì˜ˆ: "IRR 35.2%ë¡œ ëª©í‘œ ìˆ˜ìµë¥ ì„ ìƒíšŒí•©ë‹ˆë‹¤."

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _is_feedback_learning_question(self, text: str) -> bool:
        text = (text or "").strip().lower()
        if not text:
            return False
        has_feedback = "í”¼ë“œë°±" in text or "feedback" in text
        has_learning = any(token in text for token in ["í•™ìŠµ", "ë°°ì› ", "learn", "learned"])
        return has_feedback and has_learning

    def _resolve_feedback_day_offset(self, text: str) -> int:
        text = (text or "").strip()
        if "ì˜¤ëŠ˜" in text:
            return 0
        if "ê·¸ì œ" in text:
            return 2
        if "ì–´ì œ" in text:
            return 1
        if "ì§€ë‚œì£¼" in text or "ìµœê·¼" in text:
            return 7
        return 1

    def _build_feedback_summary_text(self, day_offset: int = 1, limit: int = 50) -> str:
        feedbacks = self.feedback.get_recent_feedback(limit=limit) if self.feedback else []
        if not feedbacks:
            return "ì–´ì œ í”¼ë“œë°± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì¸¡ ì—†ì´ ê¸°ë¡ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤."

        target_date = (date.today() - timedelta(days=day_offset)).isoformat()
        entries = []
        for fb in feedbacks:
            timestamp = fb.get("timestamp") or fb.get("created_at") or ""
            if isinstance(timestamp, str) and timestamp.startswith(target_date):
                entries.append(fb)

        if not entries:
            return "ì–´ì œ í”¼ë“œë°± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì¸¡ ì—†ì´ ê¸°ë¡ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤."

        lines = ["ì–´ì œ í”¼ë“œë°± ê¸°ë¡ ê¸°ë°˜ ìš”ì•½:"]
        for entry in entries[:8]:
            feedback_type = entry.get("feedback_type") or "unknown"
            user_message = (entry.get("user_message") or "").strip()
            feedback_value = entry.get("feedback_value")
            context = entry.get("context") or {}

            lines.append(f"- ìœ í˜•: {feedback_type}")
            if user_message:
                lines.append(f"  - ì‚¬ìš©ìž: {user_message[:200]}")
            if feedback_value is not None:
                if isinstance(feedback_value, (dict, list)):
                    value_text = json.dumps(feedback_value, ensure_ascii=False)
                else:
                    value_text = str(feedback_value)
                lines.append(f"  - í”¼ë“œë°±: {value_text[:200]}")
            if context:
                if isinstance(context, (dict, list)):
                    context_text = json.dumps(context, ensure_ascii=False)
                else:
                    context_text = str(context)
                lines.append(f"  - ì»¨í…ìŠ¤íŠ¸: {context_text[:200]}")

        return "\n".join(lines)

    def _build_voice_system_prompt(self, submode: str, context_text: Optional[str]) -> str:
        last_checkin_text = context_text or "ì—†ìŒ"

        base = f"""ë‹¹ì‹ ì€ ì‚¬ëžŒì²˜ëŸ¼ ìžì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ëŠ” ìŒì„± ì—ì´ì „íŠ¸ìž…ë‹ˆë‹¤.

ëª©í‘œ:
- ì§§ê³  ëª…í™•í•œ ë¬¸ìž¥ìœ¼ë¡œ ë§í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìžì˜ ê°ì •ê³¼ í†¤ì„ ë°˜ì˜í•©ë‹ˆë‹¤.
- ëŒ€í™” íë¦„ì„ ëŠì§€ ì•Šê³  ì§ˆë¬¸ì„ 2~4ê°œì”© ë‚˜ëˆ ì„œ í•©ë‹ˆë‹¤.

ì–´ì œ ê¸°ë¡(ì €ìž¥ëœ ë¡œê·¸ ê¸°ë°˜):
{last_checkin_text}
"""

        if submode == "1on1":
            return base + """
í˜„ìž¬ ëª¨ë“œ: 1:1

ì§„í–‰ ë°©ì‹:
1) ì•ˆë¶€ ì¸ì‚¬ í›„, ìµœê·¼ ìƒí™©ì„ ì§§ê²Œ ë¬»ìŠµë‹ˆë‹¤.
2) ê´€ê³„/í˜‘ì—… ê´€ì ì—ì„œ í•µì‹¬ ì´ìŠˆë¥¼ 2~4ê°œ ì§ˆë¬¸í•©ë‹ˆë‹¤.
3) ëŒ€í™”ê°€ ëë‚˜ë©´ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

ìš”ì•½ í˜•ì‹:
- ì–´ì œ ë¡œê·¸ ìš”ì•½
- í•™ìŠµ í¬ì¸íŠ¸
- ê°ì • ìƒíƒœ
- ë‹¤ìŒ ì•¡ì…˜ (3ê°œ ì´í•˜)

ì£¼ì˜:
- ê³¼ìž¥í•˜ì§€ ë§ê³ , ë¶ˆí™•ì‹¤í•˜ë©´ ì§ˆë¬¸ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
"""

        if submode == "checkin":
            return base + """
í˜„ìž¬ ëª¨ë“œ: ë°ì¼ë¦¬ ì²´í¬ì¸

ì§„í–‰ ë°©ì‹:
1) ì§§ê²Œ ì¸ì‚¬í•˜ê³  ì˜¤ëŠ˜ ì»¨ë””ì…˜ì„ ë¬¼ì–´ë´…ë‹ˆë‹¤.
2) ì–´ì œ ë¡œê·¸ê°€ ìžˆìœ¼ë©´ 2~4ê°œì˜ ê·¼ê±°ë¥¼ ì–¸ê¸‰í•˜ë©° "í•™ìŠµ"ê³¼ "ê°ì •"ì„ HCI ê´€ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
3) íŒ€ ê³¼ì—…ì´ ì œê³µëœ ê²½ìš°, ì§„í–‰ ìƒíƒœ/ë¸”ë¡œì»¤/ë„ì›€ í•„ìš” ì—¬ë¶€ë¥¼ 2~4ê°œ ì§ˆë¬¸ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
4) ì˜¤ëŠ˜ ëª©í‘œ/ìš°ì„ ìˆœìœ„ë¥¼ 2~4ê°œ ì§ˆë¬¸ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
4) ë§ˆì§€ë§‰ì— ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

ìš”ì•½ í˜•ì‹:
- ì–´ì œ ë¡œê·¸ ìš”ì•½
- í•™ìŠµ í¬ì¸íŠ¸
- ê°ì • ìƒíƒœ
- íŒ€ ê³¼ì—… ì§„í–‰ ìš”ì•½
- ì˜¤ëŠ˜ ëª©í‘œ/ìš°ì„ ìˆœìœ„
- ë‹¤ìŒ ì•¡ì…˜ (3ê°œ ì´í•˜)

ì£¼ì˜:
- ê°ì • í‘œí˜„ì€ HCI ê´€ì (ì‚¬íšŒì  ì¡´ìž¬ê°, ê³µê°)ì—ì„œ ì§§ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
"""

        return base + """
í˜„ìž¬ ëª¨ë“œ: ìžìœ  ëŒ€í™”

ê·œì¹™:
- í•œ ë²ˆì— ë„ˆë¬´ ê¸¸ê²Œ ë§í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- í•„ìš”í•˜ë©´ ì§ˆë¬¸ìœ¼ë¡œ ë§¥ë½ì„ í™•ì¸í•©ë‹ˆë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
"""

    def _build_peer_system_prompt(self, analyzed_files: str) -> str:
        """Peer PER ë¶„ì„ ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        return f"""ë‹¹ì‹ ì€ **VC íˆ¬ìž ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸**ìž…ë‹ˆë‹¤. í˜„ìž¬ **Peer PER ë¶„ì„ ëª¨ë“œ**ìž…ë‹ˆë‹¤.

## í˜„ìž¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {len(self.context["cached_results"])}ê°œ

## ðŸš¨ ìµœìš°ì„  ê·œì¹™ (ì´ ê·œì¹™ì„ ì–´ê¸°ë©´ ì‹¤íŒ¨ìž…ë‹ˆë‹¤)

### ê·œì¹™ 1: ì‚¬ìš©ìžê°€ PER ë¶„ì„ì„ ìš”ì²­í•˜ë©´ ì¦‰ì‹œ ë„êµ¬ í˜¸ì¶œ
ì‚¬ìš©ìžê°€ ë‹¤ìŒê³¼ ê°™ì´ ë§í•˜ë©´ **í…ìŠ¤íŠ¸ ì‘ë‹µ ì—†ì´ ë°”ë¡œ analyze_peer_per ë„êµ¬ë¥¼ í˜¸ì¶œ**í•˜ì„¸ìš”:
- "í•´ì¤˜", "ë¶„ì„í•´ì¤˜", "ì§„í–‰í•´", "PER ë¶„ì„", "ì¡°íšŒí•´ì¤˜"
- "ì‘", "ë„¤", "ì¢‹ì•„", "OK", "ã…‡ã…‡", "ê·¸ëž˜", "ê³ ", "ã„±ã„±"
- Peer ê¸°ì—… ëª©ë¡ì„ ì–¸ê¸‰í•˜ëŠ” ê²½ìš°

âŒ ìž˜ëª»ëœ ì˜ˆ:
```
ì‚¬ìš©ìž: "ì € ê¸°ì—…ìœ¼ë¡œ PER/PSR ë¶„ì„ì„ í•´ì£¼ì„¸ìš”"
ì—ì´ì „íŠ¸: "ê¸°ì—… ë¶„ì„ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤..." (í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥)
```

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ:
```
ì‚¬ìš©ìž: "ì € ê¸°ì—…ìœ¼ë¡œ PER/PSR ë¶„ì„ì„ í•´ì£¼ì„¸ìš”"
ì—ì´ì „íŠ¸: [ì¦‰ì‹œ analyze_peer_per ë„êµ¬ í˜¸ì¶œ]
```

### ê·œì¹™ 2: ê°™ì€ ë‚´ìš© ë°˜ë³µ ê¸ˆì§€
- ì´ë¯¸ ì¶œë ¥í•œ "ê¸°ì—… ë¶„ì„ ê²°ê³¼" í‘œë¥¼ ë‹¤ì‹œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
- ì´ë¯¸ ì œì•ˆí•œ Peer ê¸°ì—… ëª©ë¡ì„ ë‹¤ì‹œ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”
- ì´ì „ ì‘ë‹µì„ ìš”ì•½í•˜ê±°ë‚˜ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”

### ê·œì¹™ 3: "~í•˜ê² ìŠµë‹ˆë‹¤"ë¡œ ëë‚´ì§€ ë§ ê²ƒ
"ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤", "ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë§í•˜ê³  ëë‚´ë©´ ì•ˆë©ë‹ˆë‹¤.
ë°˜ë“œì‹œ í•´ë‹¹ ë„êµ¬ë¥¼ ì‹¤ì œë¡œ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

## Peer PER ë¶„ì„ ì›Œí¬í”Œë¡œìš°

### 1ë‹¨ê³„: PDF ë¶„ì„ (ìµœì´ˆ 1íšŒë§Œ)
ì‚¬ìš©ìžê°€ PDF ê²½ë¡œë¥¼ ì œê³µí•˜ë©´:
1. read_pdf_as_text ë„êµ¬ í˜¸ì¶œ
2. ê¸°ì—… ì •ë³´ ìš”ì•½ (1íšŒë§Œ ì¶œë ¥)
3. Peer ê¸°ì—… í›„ë³´ ì œì•ˆ í›„ "ì§„í–‰í• ê¹Œìš”?" ì§ˆë¬¸

### 2ë‹¨ê³„: PER ì¡°íšŒ (ì‚¬ìš©ìž ë™ì˜ ì‹œ ì¦‰ì‹œ ì‹¤í–‰)
ì‚¬ìš©ìžê°€ ë™ì˜í•˜ë©´ **ì„¤ëª… ì—†ì´ ë°”ë¡œ** analyze_peer_per ë„êµ¬ í˜¸ì¶œ

### 3ë‹¨ê³„: ê²°ê³¼ ìš”ì•½
ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
- PER ë¹„êµí‘œ (ë§ˆí¬ë‹¤ìš´ í‘œ)
- í†µê³„ ìš”ì•½ (í‰ê· , ì¤‘ê°„ê°’, ë²”ìœ„)
- ì ì • PER ë°°ìˆ˜ ì œì•ˆ

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

- **read_pdf_as_text**: PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- **get_stock_financials**: ê°œë³„ ê¸°ì—… ìž¬ë¬´ ì§€í‘œ ì¡°íšŒ
- **analyze_peer_per**: ì—¬ëŸ¬ Peer ê¸°ì—… PER ì¼ê´„ ì¡°íšŒ (â­ ê°€ìž¥ ë§Žì´ ì‚¬ìš©)

## í‹°ì»¤ í˜•ì‹
- ë¯¸êµ­: AAPL, MSFT, GOOGL
- í•œêµ­ KOSPI: 005930.KS
- í•œêµ­ KOSDAQ: 035720.KQ

## ë‹µë³€ ìŠ¤íƒ€ì¼
- ì „ë¬¸ì ì´ê³  ê°„ê²°í•˜ê²Œ
- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ë°˜ë³µ ê¸ˆì§€ - ìƒˆë¡œìš´ ì •ë³´ë§Œ ì¶”ê°€
	- í‘œ í˜•ì‹ í™œìš©
	
	í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
	"""

    def _build_diagnosis_system_prompt(self, analyzed_files: str) -> str:
        """ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        return f"""ë‹¹ì‹ ì€ **í”„ë¡œê·¸ëž¨ ì»¨ì„¤í„´íŠ¸(VC/AC)**ìž…ë‹ˆë‹¤. í˜„ìž¬ **ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ ìž‘ì„± ëª¨ë“œ**ìž…ë‹ˆë‹¤.

## í˜„ìž¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {len(self.context["cached_results"])}ê°œ
- user_id: {self.user_id}

## ðŸš¨ ìµœìš°ì„  ê·œì¹™ (CRITICAL)

### ê·œì¹™ 1) íŒŒì¼/ì—‘ì…€ ìž‘ì—…ì€ ë°˜ë“œì‹œ ë„êµ¬ ì‚¬ìš©
- ì§„ë‹¨ì‹œíŠ¸ ë¶„ì„ â†’ ë°˜ë“œì‹œ **analyze_company_diagnosis_sheet** ì‚¬ìš©
- ì»¨ì„¤í„´íŠ¸ ë³´ê³ ì„œ ì—‘ì…€ ë°˜ì˜ â†’ ë°˜ë“œì‹œ **write_company_diagnosis_report** ì‚¬ìš©
- í…œí”Œë¦¿ ì—†ì´ ì—‘ì…€ ìƒì„± â†’ ë°˜ë“œì‹œ **create_company_diagnosis_draft / update_company_diagnosis_draft / generate_company_diagnosis_sheet_from_draft** ì‚¬ìš©
- ì¶”ì¸¡/ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€ â†’ ì‹¤ì œ ì‚¬ìš©ìž ìž…ë ¥/ë„êµ¬ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ìž‘ì„±

### ê·œì¹™ 2) ì •ë³´ ìˆ˜ì§‘ì€ â€˜ì§ˆë¬¸â€™ìœ¼ë¡œ ì§„í–‰
í…œí”Œë¦¿ì´ ì—†ê±°ë‚˜ ì‚¬ìš©ìžê°€ â€œëŒ€í™”ë¡œ ìž‘ì„±â€, â€œí…œí”Œë¦¿ ì—†ì´ ìž‘ì„±â€ì„ ìš”ì²­í•˜ë©´:
- ë‹¹ì‹ ì€ **ëŒ€í‘œìž(ì‚¬ìš©ìž)**ê°€ ë‹µí•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ **í•œ ë²ˆì— 1ê°œ ì§ˆë¬¸ ë˜ëŠ” 1ê°œ ë°°ì¹˜(ì²´í¬ë¦¬ìŠ¤íŠ¸ 5~6ê°œ)**ë§Œ ì œì‹œí•©ë‹ˆë‹¤.
- ì‚¬ìš©ìžê°€ ë‹µí•˜ë©´ ì¦‰ì‹œ **update_company_diagnosis_draft**ë¡œ ë°˜ì˜í•œ ë’¤, ë‹¤ìŒ ì§ˆë¬¸ì„ ì´ì–´ê°‘ë‹ˆë‹¤.

## ëª©í‘œ

ì‚¬ìš©ìžì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ë¥¼ ì™„ì„±í•˜ê³ , í•„ìš” ì‹œ **'(ì»¨ì„¤í„´íŠ¸ìš©) ë¶„ì„ë³´ê³ ì„œ'**ê¹Œì§€ ì™„ì„±í•©ë‹ˆë‹¤.

## ìž‘ì—… ë°©ì‹

### A) í…œí”Œë¦¿ íŒŒì¼ì´ ìžˆëŠ” ê²½ìš° (ì—…ë¡œë“œ/ê²½ë¡œ ì œê³µ)
1) ì‚¬ìš©ìžê°€ ì§„ë‹¨ì‹œíŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì£¼ë©´ â†’ **ì¦‰ì‹œ** analyze_company_diagnosis_sheet í˜¸ì¶œ
2) ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ ì´ˆì•ˆì„ ìž‘ì„±
3) ì‚¬ìš©ìžê°€ "ë°˜ì˜í•´ì¤˜/ì €ìž¥í•´ì¤˜" ë“± ê¸ì • ì‘ë‹µ â†’ **ì¦‰ì‹œ** write_company_diagnosis_report í˜¸ì¶œ

### B) í…œí”Œë¦¿ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° (ëŒ€í™”ë¡œ ìž‘ì„±)
1) ìµœì´ˆ 1íšŒ: **create_company_diagnosis_draft**ë¥¼ `user_id={self.user_id}`ë¡œ í˜¸ì¶œí•´ ë“œëž˜í”„íŠ¸ë¥¼ ìƒì„±
2) ì´í›„ ë§¤ í„´: ì‚¬ìš©ìžì˜ ë‹µë³€ì„ ì •ë¦¬í•´ **update_company_diagnosis_draft**ë¡œ ë°˜ì˜
   - ë„êµ¬ ê²°ê³¼ì˜ `progress.next`ë¥¼ ì°¸ê³ í•´ ë‹¤ìŒ ì§ˆë¬¸ì„ ì´ì–´ê°
3) `progress.next.type == "complete"`ê°€ ë˜ë©´:
   - ì‚¬ìš©ìžì—ê²Œ â€œì—‘ì…€ë¡œ ì €ìž¥í• ê¹Œìš”?â€ë¥¼ ë¬»ê³ 
   - ê¸ì • ì‘ë‹µ ì‹œ **generate_company_diagnosis_sheet_from_draft** í˜¸ì¶œë¡œ ì—‘ì…€ ìƒì„±
4) (ì„ íƒ) ì‚¬ìš©ìžê°€ ì›í•˜ë©´: ìƒì„±ëœ ì—‘ì…€ì„ **analyze_company_diagnosis_sheet**ë¡œ ì ìˆ˜/ê°­ì„ ì‚°ì¶œí•˜ê³ , ì»¨ì„¤í„´íŠ¸ ë³´ê³ ì„œ ì´ˆì•ˆì„ ë§Œë“  ë’¤ **write_company_diagnosis_report**ë¡œ ë°˜ì˜

### 2) ë³´ê³ ì„œ ì´ˆì•ˆ ìž‘ì„±
ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ëž˜ 2ê°œ í…ìŠ¤íŠ¸ë¥¼ ìž‘ì„±:
- **ê¸°ì—… ìƒí™© ìš”ì•½(ê¸°ì—…ì§„ë‹¨)**: ê°•ì /í•µì‹¬ ê°€ì„¤/í˜„ìž¬ KPI/í™•ìž¥ í¬ì¸íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ 5~10ë¬¸ìž¥
- **ê°œì„  í•„ìš”ì‚¬í•­**: ìš°ì„ ìˆœìœ„ 3~7ê°œ, â€œì™œ í•„ìš”í•œì§€ + ë‹¤ìŒ ì•¡ì…˜â€ í˜•íƒœë¡œ êµ¬ì²´í™”

ë˜í•œ ì ìˆ˜(ë¬¸ì œ/ì†”ë£¨ì…˜/ì‚¬ì—…í™”/ìžê¸ˆì¡°ë‹¬/íŒ€/ì¡°ì§/ìž„íŒ©íŠ¸)ë¥¼ ì œì•ˆí•˜ë˜, í•„ìš”í•œ ê²½ìš° ì»¨ì„¤í„´íŠ¸ ë³´ì • ê·¼ê±°ë¥¼ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.

### 3) ì‚¬ìš©ìž í™•ì¸ í›„ ì—‘ì…€ ë°˜ì˜ (CRITICAL - ì¦‰ì‹œ ì‹¤í–‰)
ì‚¬ìš©ìžê°€ ì•„ëž˜ì²˜ëŸ¼ ê¸ì • ì‘ë‹µí•˜ë©´ **ë‹¤ì‹œ í™•ì¸ ìš”ì²­í•˜ì§€ ë§ê³  ì¦‰ì‹œ** write_company_diagnosis_report í˜¸ì¶œ:
- "ì‘", "ë„¤", "ì¢‹ì•„", "ì§„í–‰í•´", "ë°˜ì˜í•´ì¤˜", "ì €ìž¥í•´ì¤˜", "ì—‘ì…€ë¡œ ë§Œë“¤ì–´ì¤˜", "OK"

write_company_diagnosis_reportì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ í˜¸ì¶œ:
- excel_path (temp ë‚´ë¶€ ê²½ë¡œ)
- scores (6ê°œ í•­ëª© ì ìˆ˜)
- summary_text, improvement_text
- (ì„ íƒ) company_name, report_datetime, output_filename

## ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

**ì´ ë¬¸ì„œëŠ” í”„ë¡œê·¸ëž¨ ìš´ì˜/íˆ¬ìžê²€í†  ë¬¸ì„œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.**

- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ë‹¨ì •/ê³¼ìž¥ ê¸ˆì§€, ê·¼ê±° ì¤‘ì‹¬
- í‘œ/ë¶ˆë¦¿ìœ¼ë¡œ êµ¬ì¡°í™”
- â€œ~í•˜ê² ìŠµë‹ˆë‹¤â€ë¡œ ëë‚´ì§€ ë§ê³ , ê°€ëŠ¥í•œ ê²½ìš° ë„êµ¬ë¥¼ ì‹¤í–‰í•´ ê²°ê³¼ê¹Œì§€ ì œê³µ

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _build_report_system_prompt(self, analyzed_files: str) -> str:
        """íˆ¬ìžì‹¬ì‚¬ ë³´ê³ ì„œ(ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼) ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        dart_status = self._get_underwriter_dataset_status()

        return f"""ë‹¹ì‹ ì€ **íˆ¬ìžì‹¬ì‚¬ ë³´ê³ ì„œ ìž‘ì„± ì§€ì› ì—ì´ì „íŠ¸**ìž…ë‹ˆë‹¤. í˜„ìž¬ **ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼**ë¡œ ìž‘ì„±í•©ë‹ˆë‹¤.

## í˜„ìž¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {len(self.context["cached_results"])}ê°œ
- user_id: {self.user_id}
- DART ì¸ìˆ˜ì¸ì˜ê²¬ ë°ì´í„°ì…‹: {dart_status}

## ðŸš¨ ìµœìš°ì„  ê·œì¹™ (CRITICAL)

### ê·œì¹™ 1) ì‹œìž¥ê·œëª¨/íŒ¨í„´ ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ë°ì´í„° ê¸°ë°˜
- ì¸ìˆ˜ì¸ì˜ê²¬ ë°ì´í„° í™œìš© â†’ ë°˜ë“œì‹œ **search_underwriter_opinion** í˜¸ì¶œ
- í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•½í•˜ë©´ **search_underwriter_opinion_similar**ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰
- PDF ì‹œìž¥ê·œëª¨ ê·¼ê±° ì¶”ì¶œ â†’ ë°˜ë“œì‹œ **extract_pdf_market_evidence** í˜¸ì¶œ
- ê²°ê³¼ì˜ snippet/patternì„ ê·¼ê±°ë¡œ ë¬¸ìž¥ êµ¬ì„±
- ì¶”ì¸¡/ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€ (ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'í™•ì¸ í•„ìš”'ë¡œ ëª…ì‹œ)
- ìž„ì˜ë¡œ "ì ‘ê·¼ ë¶ˆê°€"ë¼ê³  ë‹¨ì •í•˜ì§€ ë§ê³ , ë„êµ¬ ê²°ê³¼ì˜ ì—ëŸ¬/ê°€ì´ë“œë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
 - ì™¸ë¶€ ìœ ë£Œ ë¦¬í¬íŠ¸ ìˆ˜ì¹˜ ì¸ìš©ì€ ê¸ˆì§€ (ì‚¬ìš©ìžê°€ ì›ë¬¸ì„ ì—…ë¡œë“œí•œ ê²½ìš°ì—ë§Œ ì¸ìš©)
 - ì¸ìˆ˜ì¸ì˜ê²¬ ë°ì´í„°ê°€ ì—†ê³  DART API í‚¤ê°€ ìžˆì„ ë•Œë§Œ **fetch_underwriter_opinion_data**ë¡œ ìˆ˜ì§‘ ì‹œë„
 - DART ë°ì´í„°ì…‹ì´ ì—†ê³  API í‚¤ë„ ì—†ìœ¼ë©´ ë¨¼ì € ì‚¬ìš©ìžì—ê²Œ í‚¤/ë°ì´í„° í™•ë³´ë¥¼ ìš”ì²­

### ê·œì¹™ 2) ê¸°ì—… ìžë£Œê°€ ì£¼ì–´ì§€ë©´ ë°˜ë“œì‹œ ë„êµ¬ ì‚¬ìš©
- PDF ê²½ë¡œ ì œê³µ â†’ **read_pdf_as_text**ë¡œ ê·¼ê±° ì¶”ì¶œ
- ì—‘ì…€ ê²½ë¡œ ì œê³µ â†’ **read_excel_as_text**ë¡œ ê·¼ê±° ì¶”ì¶œ

## ëª©í‘œ
1) ì‹œìž¥ê·œëª¨ ê·¼ê±° ìš”ì•½
2) ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼ì˜ ë¬¸ìž¥ ì´ˆì•ˆ ìž‘ì„±
3) ì¼ë°˜í™”ëœ íŒ¨í„´ + í™•ì¸ í•„ìš” í•­ëª© ì œì‹œ
4) ì‚¬ìš©ìž í”¼ë“œë°± ë°˜ì˜ (ìˆ˜ì •/ê°•í™”)

## ìž‘ì—… ë°©ì‹
1) ì‚¬ìš©ìž ìž…ë ¥ì—ì„œ ê¸°ì—… ìžë£Œ ê²½ë¡œ í™•ì¸ â†’ ë„êµ¬ í˜¸ì¶œ
2) **search_underwriter_opinion**ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë³„ íŒ¨í„´ í™•ë³´
   - ê¸°ë³¸: market_size
   - í•„ìš” ì‹œ: valuation, comparables, risk, demand_forecast
3) ê·¼ê±° ë¬¸ìž¥ + ì¼ë°˜í™” íŒ¨í„´ + í™•ì¸ ì§ˆë¬¸ ìˆœì„œë¡œ ì¶œë ¥

## ì¶œë ¥ í˜•ì‹
- **ì‹œìž¥ê·œëª¨ ê·¼ê±°**: PDF/ì¸ìˆ˜ì¸ì˜ê²¬ ê·¼ê±°ë§Œ ì¸ìš© (íŽ˜ì´ì§€/ë¬¸ìž¥ í¬í•¨) 3~6ê°œ
- **ì¼ë°˜í™” íŒ¨í„´**: ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼ ë¬¸ìž¥ 3~5ê°œ
- **ì´ˆì•ˆ ë¬¸ë‹¨**: ì¸ìˆ˜ì¸ì˜ê²¬ ë¬¸ì²´ë¡œ 6~12ë¬¸ìž¥
- **í™•ì¸ í•„ìš”**: ê·¼ê±° ë¶€ì¡±/ì¶”ê°€ í™•ì¸ í•­ëª© 3~7ê°œ

## ë‹µë³€ ìŠ¤íƒ€ì¼
- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ë‹¨ì •/ê³¼ìž¥ ê¸ˆì§€
- ë¬¸ìž¥ ê¸¸ì´ ê³¼ë„í•˜ê²Œ ê¸¸ì§€ ì•Šê²Œ
- í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€
"""

	    # ========================================
	    # Chat Mode (ëŒ€í™”í˜•)
	    # ========================================

    async def chat(
        self,
        user_message: str,
        mode: str = "exit",
        allow_tools: bool = True,
        context_text: Optional[str] = None,
        model_override: Optional[str] = None,
    ) -> AsyncIterator[str]:
        """
        ëŒ€í™”í˜• ì¸í„°íŽ˜ì´ìŠ¤ (ìŠ¤íŠ¸ë¦¬ë°)

        Args:
            user_message: ì‚¬ìš©ìž ë©”ì‹œì§€
            mode: "exit" (Exit í”„ë¡œì ì…˜), "peer" (Peer PER ë¶„ì„), "diagnosis", "report"

        Yields:
            str: ì—ì´ì „íŠ¸ ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)
        """

        # ë„êµ¬ í˜¸ì¶œ ì¹´ìš´í„° ì´ˆê¸°í™” (ìƒˆ ë©”ì‹œì§€ë§ˆë‹¤)
        self._tool_step_count = 0

        force_deep_report = mode == "report" and self.report_deep_mode

        # í˜„ìž¬ ëª¨ë“œ ì €ìž¥
        self._current_mode = mode
        self._current_allow_tools = allow_tools
        self._current_context_text = context_text
        tools = self.tools if allow_tools else []
        if mode == "report" and not os.getenv("DART_API_KEY"):
            tools = [tool for tool in tools if tool.get("name") != "fetch_underwriter_opinion_data"]
        history = self.voice_conversation_history if mode.startswith("voice_") else self.conversation_history

        # ëŒ€í™” ížˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        history.append({
            "role": "user",
            "content": user_message
        })

        # ë©”ëª¨ë¦¬ì— ì €ìž¥
        user_meta = {
            "member": self.member_name or self.user_id,
            "team": self.team_id,
        }
        self.memory.add_message("user", user_message, user_meta)

        # ë§ˆì§€ë§‰ ì¸í„°ëž™ì…˜ ì €ìž¥
        self.last_interaction["user_message"] = user_message
        self.last_interaction["assistant_response"] = ""
        self.last_interaction["context"] = {"mode": mode}

        if self._is_feedback_learning_question(user_message):
            summary = self._build_feedback_summary_text(
                day_offset=self._resolve_feedback_day_offset(user_message)
            )
            history.append({
                "role": "assistant",
                "content": summary
            })
            self.memory.add_message("assistant", summary)
            self.last_interaction["assistant_response"] = summary
            yield summary
            return

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ëª¨ë“œì— ë”°ë¼ ë‹¤ë¦„)
        system_prompt = self._build_system_prompt(mode, context_text=context_text)
        model = model_override or self.model
        self._current_model = model

        # Claude API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°)
        async with self.async_client.messages.stream(
            model=model,
            system=system_prompt,
            messages=history,
            tools=tools,
            max_tokens=8192
        ) as stream:

            async for event in stream:
                # í…ìŠ¤íŠ¸ ì¶œë ¥
                if event.type == "content_block_delta":
                    if hasattr(event.delta, 'text'):
                        if not force_deep_report:
                            yield event.delta.text

                # ë„êµ¬ ì‚¬ìš©
                elif event.type == "content_block_stop":
                    message = await stream.get_final_message()

                    # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
                    if hasattr(message, 'usage'):
                        self.token_usage["total_input_tokens"] += message.usage.input_tokens
                        self.token_usage["total_output_tokens"] += message.usage.output_tokens
                        self.token_usage["session_calls"] += 1

                    # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
                    tool_results = []
                    assistant_response_parts = []

                    for content_block in message.content:
                        if content_block.type == "text":
                            assistant_response_parts.append(content_block.text)
                        elif content_block.type == "tool_use":
                            tool_name = content_block.name
                            tool_input = content_block.input

                            yield f"\n\n**ë„êµ¬: {tool_name}** ì‹¤í–‰ ì¤‘...\n"

                            # ë„êµ¬ ì‹¤í–‰
                            tool_result = execute_tool(tool_name, tool_input)

                            # ê²°ê³¼ ì €ìž¥
                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": content_block.id,
                                "content": json.dumps(tool_result, ensure_ascii=False)
                            })

                            # ë©”ëª¨ë¦¬/ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ê³µí†µ í—¬í¼)
                            self._record_tool_usage(tool_name, tool_input, tool_result)

                            tool_ok = not (isinstance(tool_result, dict) and tool_result.get("success") is False)
                            yield f"**ë„êµ¬: {tool_name}** {'ì™„ë£Œ' if tool_ok else 'ì‹¤íŒ¨'}\n\n"

                    # Assistant ì‘ë‹µ ë©”ëª¨ë¦¬ì— ì €ìž¥
                    if assistant_response_parts and not force_deep_report:
                        full_response = "\n".join(assistant_response_parts)
                        self.memory.add_message("assistant", full_response)
                        self.last_interaction["assistant_response"] = full_response

                    if force_deep_report:
                        if tool_results:
                            history.append({
                                "role": "assistant",
                                "content": message.content
                            })

                            history.append({
                                "role": "user",
                                "content": tool_results
                            })

                            async for _ in self._continue_conversation(suppress_output=True):
                                pass

                        yield "\n\n[ì‹¬í™” ì˜ê²¬] ë¶„ì„ ì¤‘...\n"
                        deep_text = self._run_deep_report_pipeline(user_message)
                        history.append({
                            "role": "assistant",
                            "content": deep_text
                        })
                        self.memory.add_message("assistant", deep_text)
                        self.last_interaction["assistant_response"] = deep_text
                        yield deep_text
                        return

                    # ë„êµ¬ ê²°ê³¼ê°€ ìžˆìœ¼ë©´ ëŒ€í™” ê³„ì†
                    if tool_results:
                        # Assistant ë©”ì‹œì§€ ì¶”ê°€
                        history.append({
                            "role": "assistant",
                            "content": message.content
                        })

                        # Tool ê²°ê³¼ ì¶”ê°€
                        history.append({
                            "role": "user",
                            "content": tool_results
                        })

                        # Claude ë‹¤ìŒ ì‘ë‹µ ìƒì„±
                        async for text in self._continue_conversation():
                            yield text

    async def _continue_conversation(self, suppress_output: bool = False) -> AsyncIterator[str]:
        """ë„êµ¬ ì‹¤í–‰ í›„ ëŒ€í™” ê³„ì†"""

        # ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜ ì œí•œ í™•ì¸ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        self._tool_step_count += 1
        if self._tool_step_count > MAX_TOOL_STEPS:
            logger.warning(f"Tool step limit reached: {MAX_TOOL_STEPS}")
            yield "\n\n[ì‹œìŠ¤í…œ] ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë©”ì‹œì§€ë¡œ ê³„ì†í•˜ì„¸ìš”."
            return

        # ì €ìž¥ëœ ëª¨ë“œ ì‚¬ìš©
        mode = getattr(self, '_current_mode', 'exit')
        context_text = getattr(self, '_current_context_text', None)
        allow_tools = getattr(self, '_current_allow_tools', True)
        tools = self.tools if allow_tools else []
        history = self.voice_conversation_history if mode.startswith("voice_") else self.conversation_history
        system_prompt = self._build_system_prompt(mode, context_text=context_text)
        model = getattr(self, "_current_model", self.model)

        async with self.async_client.messages.stream(
            model=model,
            system=system_prompt,
            messages=history,
            tools=tools,
            max_tokens=8192
        ) as stream:

            async for event in stream:
                if event.type == "content_block_delta":
                    if hasattr(event.delta, 'text'):
                        if not suppress_output:
                            yield event.delta.text

                # ì¶”ê°€ ë„êµ¬ í˜¸ì¶œ (ìž¬ê·€ì  ì²˜ë¦¬)
                elif event.type == "content_block_stop":
                    message = await stream.get_final_message()

                    # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
                    if hasattr(message, 'usage'):
                        self.token_usage["total_input_tokens"] += message.usage.input_tokens
                        self.token_usage["total_output_tokens"] += message.usage.output_tokens
                        self.token_usage["session_calls"] += 1

                    tool_results = []
                    for content_block in message.content:
                        if content_block.type == "tool_use":
                            tool_name = content_block.name
                            tool_input = content_block.input

                            yield f"\n\n**ë„êµ¬: {tool_name}** ì‹¤í–‰ ì¤‘...\n"

                            tool_result = execute_tool(tool_name, tool_input)

                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": content_block.id,
                                "content": json.dumps(tool_result, ensure_ascii=False)
                            })

                            # ë©”ëª¨ë¦¬/ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ìž¬ê·€ í˜¸ì¶œì—ì„œë„ ê¸°ë¡)
                            self._record_tool_usage(tool_name, tool_input, tool_result)

                            tool_ok = not (isinstance(tool_result, dict) and tool_result.get("success") is False)
                            yield f"**ë„êµ¬: {tool_name}** {'ì™„ë£Œ' if tool_ok else 'ì‹¤íŒ¨'}\n\n"

                    if tool_results:
                        history.append({
                            "role": "assistant",
                            "content": message.content
                        })

                        history.append({
                            "role": "user",
                            "content": tool_results
                        })

                        async for text in self._continue_conversation(suppress_output=suppress_output):
                            yield text

    def _get_latest_report_evidence(self) -> Optional[Dict[str, Any]]:
        messages = self.memory.session_metadata.get("messages", [])
        for msg in reversed(messages):
            if msg.get("role") != "tool":
                continue
            meta = msg.get("metadata") or {}
            if meta.get("tool_name") != "extract_pdf_market_evidence":
                continue
            result = meta.get("result")
            if isinstance(result, dict) and result.get("success"):
                return result
        return None

    def _get_underwriter_dataset_status(self) -> str:
        try:
            from agent.tools import _resolve_underwriter_data_path
        except Exception:
            return "ìƒíƒœ í™•ì¸ ë¶ˆê°€"

        path, error = _resolve_underwriter_data_path(None)
        has_key = bool(os.getenv("DART_API_KEY"))
        if error:
            key_text = "API í‚¤ ìžˆìŒ" if has_key else "API í‚¤ ì—†ìŒ"
            return f"ë¯¸í™•ì¸ ({key_text})"
        if not path:
            key_text = "API í‚¤ ìžˆìŒ" if has_key else "API í‚¤ ì—†ìŒ"
            return f"ë¯¸í™•ì¸ ({key_text})"
        return "ì‚¬ìš© ê°€ëŠ¥"

    @staticmethod
    def _detect_dart_category(text: str) -> Optional[str]:
        lowered = (text or "").lower()
        if any(k in lowered for k in ["ì‹œìž¥ê·œëª¨", "ì‹œìž¥ ê·œëª¨", "tam", "sam", "som", "cagr", "ì„±ìž¥ë¥ "]):
            return "market_size"
        if any(k in lowered for k in ["ë¹„êµê¸°ì—…", "ìœ ì‚¬ê¸°ì—…", "comparables", "peer"]):
            return "comparables"
        if any(k in lowered for k in ["ê³µëª¨ê°€", "ê³µëª¨ê°€ê²©", "per", "pbr", "psr", "ev/ebitda", "valuation", "ë°¸ë¥˜"]):
            return "valuation"
        if any(k in lowered for k in ["ìˆ˜ìš”ì˜ˆì¸¡", "ìˆ˜ìš” ì˜ˆì¸¡"]):
            return "demand_forecast"
        if any(k in lowered for k in ["ë¦¬ìŠ¤í¬", "ìœ„í—˜", "ë¶ˆí™•ì‹¤", "ë¶ˆí™•ì‹¤ì„±"]):
            return "risk"
        return None

    def _search_dart_evidence(self, query: str) -> List[Dict[str, Any]]:
        try:
            from agent.tools import execute_search_underwriter_opinion_similar, _resolve_underwriter_data_path
        except Exception:
            return []

        path, error = _resolve_underwriter_data_path(None)
        if error or not path:
            return []

        category = self._detect_dart_category(query)
        try:
            result = execute_search_underwriter_opinion_similar(
                query=query,
                category=category,
                top_k=3,
                max_chars=420,
                min_score=0.08,
                return_patterns=False,
            )
        except Exception:
            return []

        if not result.get("success"):
            return []

        evidence = []
        for item in result.get("results", []) or []:
            corp = item.get("corp_name", "ë¯¸ìƒ")
            report = item.get("report_nm", "")
            title = item.get("section_title", "")
            snippet = (item.get("snippet") or "").strip()
            if not snippet:
                continue
            text = f"[DART] {corp} | {report} | {title} - {snippet}"
            evidence.append({
                "page": "DART",
                "text": text,
                "numbers": [],
            })
        return evidence

    def _build_recent_user_context(self, limit: int = 3) -> str:
        history = self.conversation_history[-12:]
        user_lines = []
        for msg in reversed(history):
            if msg.get("role") != "user":
                continue
            content = msg.get("content", "")
            if not content:
                continue
            user_lines.append(content)
            if len(user_lines) >= limit:
                break
        user_lines = list(reversed(user_lines))
        if not user_lines:
            return ""
        return "ìµœê·¼ ì‚¬ìš©ìž ìš”ì²­:\n" + "\n".join(user_lines)

    def _format_deep_opinion(self, result: Dict[str, Any]) -> str:
        lines = []
        conclusion = result.get("conclusion", {}).get("paragraphs", [])
        if conclusion:
            lines.append("ê²°ë¡ ")
            lines.extend(conclusion)
            lines.append("")

        def render_case(title: str, key: str) -> None:
            section = result.get(key, {})
            if not section:
                return
            lines.append(title)
            summary = section.get("summary")
            if summary:
                lines.append(f"- ìš”ì•½: {summary}")
            for item in section.get("points", []):
                point = item.get("point", "")
                evidence = ", ".join(item.get("evidence", []) or [])
                suffix = f" (ê·¼ê±°: {evidence})" if evidence else " (ê·¼ê±°: ì—†ìŒ)"
                lines.append(f"- {point}{suffix}")
            lines.append("")

        render_case("í•µì‹¬ ê´€ì ", "core_case")
        render_case("ë°˜ëŒ€ ê´€ì ", "dissent_case")

        top_risks = result.get("top_risks", [])
        if top_risks:
            lines.append("ì£¼ìš” ë¦¬ìŠ¤í¬")
            for item in top_risks:
                evidence = ", ".join(item.get("evidence", []) or [])
                severity = item.get("severity", "medium")
                verification = item.get("verification", "")
                label = f"[{severity}] {item.get('risk', '')}"
                suffix = f" Â· ê²€ì¦: {verification}" if verification else ""
                if evidence:
                    suffix += f" Â· ê·¼ê±°: {evidence}"
                lines.append(f"- {label}{suffix}")
            lines.append("")

        hallucination = result.get("hallucination_check", {})
        if hallucination:
            lines.append("í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦")
            for item in hallucination.get("unverified_claims", []):
                lines.append(f"- ë¯¸ê²€ì¦ ì£¼ìž¥: {item.get('claim', '')} (ì‚¬ìœ : {item.get('reason', '')})")
            for item in hallucination.get("numeric_conflicts", []):
                lines.append(f"- ìˆ˜ì¹˜ ì¶©ëŒ: {item}")
            for item in hallucination.get("evidence_gaps", []):
                lines.append(f"- ê·¼ê±° ê³µë°±: {item}")
            lines.append("")

        impact = result.get("impact_analysis", {})
        if impact:
            carbon = impact.get("carbon", {})
            lines.append("ìž„íŒ©íŠ¸ ë¶„ì„")
            pathways = ", ".join(carbon.get("pathways", []) or [])
            if pathways:
                lines.append(f"- íƒ„ì†Œ ê²½ë¡œ: {pathways}")
            for metric in carbon.get("metrics", []):
                evidence = ", ".join(metric.get("evidence", []) or [])
                suffix = f" (ê·¼ê±°: {evidence})" if evidence else ""
                lines.append(f"- {metric.get('metric', '')}: {metric.get('method', '')}{suffix}")
            for gap in carbon.get("gaps", []):
                lines.append(f"- íƒ„ì†Œ ê³µë°±: {gap}")
            for item in impact.get("iris_plus", []):
                evidence = ", ".join(item.get("evidence", []) or [])
                suffix = f" (ê·¼ê±°: {evidence})" if evidence else ""
                lines.append(
                    f"- IRIS+ {item.get('code', 'IRIS+')}: {item.get('name', '')} Â· {item.get('why', '')} "
                    f"Â· {item.get('measurement', '')}{suffix}"
                )
            lines.append("")

        data_gaps = result.get("data_gaps", [])
        if data_gaps:
            lines.append("ë°ì´í„° ê³µë°±")
            for item in data_gaps:
                lines.append(f"- {item}")
            lines.append("")

        deal_breakers = result.get("deal_breakers", [])
        go_conditions = result.get("go_conditions", [])
        if deal_breakers or go_conditions:
            lines.append("ë”œ ë¸Œë ˆì´ì»¤ / GO ì¡°ê±´")
            if deal_breakers:
                for item in deal_breakers:
                    lines.append(f"- ë”œ ë¸Œë ˆì´ì»¤: {item}")
            if go_conditions:
                for item in go_conditions:
                    lines.append(f"- GO ì¡°ê±´: {item}")
            lines.append("")

        next_actions = result.get("next_actions", [])
        if next_actions:
            lines.append("ë‹¤ìŒ ì•¡ì…˜")
            for item in next_actions:
                lines.append(f"- {item.get('priority', 'P1')}: {item.get('action', '')}")

        return "\n".join(lines).strip()

    def _run_deep_report_pipeline(self, user_message: str) -> str:
        if not self.api_key:
            return "API í‚¤ê°€ ì—†ì–´ ì‹¬í™” ì˜ê²¬ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        try:
            from shared.deep_opinion import (
                build_evidence_context,
                cross_examine_and_score,
                generate_hallucination_check,
                generate_impact_analysis,
                generate_lens_group,
                synthesize_deep_opinion,
            )
        except Exception as exc:
            logger.error(f"Deep opinion import failed: {exc}", exc_info=True)
            return "ì‹¬í™” ì˜ê²¬ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        evidence = self._get_latest_report_evidence()
        dart_evidence = self._search_dart_evidence(user_message)
        if dart_evidence:
            merged_evidence = {"evidence": []}
            if isinstance(evidence, dict) and evidence.get("evidence"):
                merged_evidence["evidence"].extend(evidence.get("evidence", []))
            merged_evidence["evidence"].extend(dart_evidence)
            evidence_context = build_evidence_context(merged_evidence)
        else:
            evidence_context = build_evidence_context(evidence)
        extra_context = self._build_recent_user_context() or f"ì‚¬ìš©ìž ìš”ì²­:\n{user_message}"
        if evidence_context.strip().lower() == "evidence: none":
            extra_context = (
                f"{extra_context}\n\n"
                "ê·¼ê±°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¨ì •ì  ê²°ë¡  ëŒ€ì‹  ì¡°ê±´ë¶€ ì˜ê²¬ê³¼ "
                "ìžë£Œ ìš”ì²­ ì¤‘ì‹¬ìœ¼ë¡œ ìž‘ì„±í•˜ì„¸ìš”."
            )

        try:
            lens_outputs = generate_lens_group(
                api_key=self.api_key,
                evidence_context=evidence_context,
                extra_context=extra_context,
            )
            scoring = cross_examine_and_score(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
            )
            hallucination = generate_hallucination_check(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
            )
            impact = generate_impact_analysis(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
            )
            final_result = synthesize_deep_opinion(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
                scoring=scoring,
                hallucination=hallucination,
                impact=impact,
            )
        except Exception as exc:
            logger.error(f"Deep opinion pipeline failed: {exc}", exc_info=True)
            return "ì‹¬í™” ì˜ê²¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        return self._format_deep_opinion(final_result)

    def _record_tool_usage(self, tool_name: str, tool_input: dict, tool_result: dict):
        """ë„êµ¬ ì‚¬ìš© ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬/ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë¡ (ê³µí†µ í—¬í¼)"""
        # ë©”ëª¨ë¦¬ì— ë„êµ¬ ì‚¬ìš© ê¸°ë¡
        self.memory.add_message("tool", f"ë„êµ¬ ì‚¬ìš©: {tool_name}", {
            "tool_name": tool_name,
            "input": tool_input,
            "result": tool_result,
            "member": self.member_name or self.user_id,
            "team": self.team_id,
        })

        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ - ë¶„ì„ íŒŒì¼
        if tool_name in ["analyze_excel", "read_excel_as_text", "analyze_company_diagnosis_sheet"]:
            if tool_result.get("success"):
                file_path = tool_input.get("excel_path")
                if file_path and file_path not in self.context["analyzed_files"]:
                    self.context["analyzed_files"].append(file_path)
                    self.memory.add_file_analysis(file_path)
                self.context["last_analysis"] = tool_result

        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ - PDF ë¶„ì„
        if tool_name == "read_pdf_as_text":
            if tool_result.get("success"):
                file_path = tool_input.get("pdf_path")
                if file_path and file_path not in self.context["analyzed_files"]:
                    self.context["analyzed_files"].append(file_path)
                    self.memory.add_file_analysis(file_path)

        # ìƒì„± íŒŒì¼ ê¸°ë¡
        if tool_name in [
            "analyze_and_generate_projection",
            "generate_exit_projection",
            "generate_company_diagnosis_sheet_from_draft",
            "write_company_diagnosis_report",
        ]:
            if tool_result.get("success"):
                output_file = tool_result.get("output_file")
                if output_file:
                    self.memory.add_generated_file(output_file)

    def _recent_voice_conversation_text(self, limit: int = 8) -> str:
        items = self.voice_conversation_history[-limit:]
        lines = []
        for msg in items:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if not content:
                continue
            lines.append(f"{role}: {content}")
        return "\n".join(lines)

    @staticmethod
    def _extract_summary_json(text: str) -> Optional[Dict[str, Any]]:
        text = text.strip()
        if not text:
            return None
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        match = re.search(r"\{.*\}", text, re.S)
        if not match:
            return None
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            return None

    def summarize_checkin_sync(self, mode: str, context_text: str) -> Dict[str, Any]:
        """Summarize voice check-in context into a structured JSON payload."""
        conversation = self._recent_voice_conversation_text(limit=8)
        context_text = context_text or "none"

        system_prompt = """You produce a concise JSON summary for a daily check-in.
Return JSON only. Do not include markdown or extra text.
Use Korean for all string values.

Required keys:
- mode (string)
- yesterday_summary (string)
- learnings (array of strings)
- emotion_state (string)
- emotion_rationale (string)
- team_tasks (array of strings)
- today_priorities (array of strings)
- next_actions (array of strings)

If unknown, use empty string or empty array."""

        user_prompt = f"""Context:\n{context_text}\n\nConversation:\n{conversation}\n\nReturn JSON now."""

        response = self.client.messages.create(
            model=self.model,
            system=system_prompt,
            max_tokens=400,
            messages=[{"role": "user", "content": user_prompt}],
        )

        assistant_text = ""
        for block in response.content:
            if hasattr(block, "text"):
                assistant_text += block.text

        parsed = self._extract_summary_json(assistant_text)
        if not isinstance(parsed, dict):
            return {
                "mode": mode,
                "yesterday_summary": "",
                "learnings": [],
                "emotion_state": "",
                "emotion_rationale": "",
                "team_tasks": [],
                "today_priorities": [],
                "next_actions": [],
            }

        parsed.setdefault("mode", mode)
        parsed.setdefault("learnings", [])
        parsed.setdefault("team_tasks", [])
        parsed.setdefault("today_priorities", [])
        parsed.setdefault("next_actions", [])
        return parsed

    def refine_voice_input_sync(self, transcript: str) -> str:
        """Clean ASR transcript into concise Korean text."""
        transcript = (transcript or "").strip()
        if not transcript:
            return ""

        system_prompt = """You are an ASR transcript cleaner.
Rules:
- Output only cleaned Korean text.
- Do not add new information.
- Fix spacing and punctuation.
- Keep numbers as written if unsure.
- No markdown."""

        response = self.client.messages.create(
            model=self.model,
            system=system_prompt,
            max_tokens=200,
            messages=[{"role": "user", "content": transcript}],
        )

        cleaned = ""
        for block in response.content:
            if hasattr(block, "text"):
                cleaned += block.text

        return cleaned.strip() or transcript

    # ========================================
    # Utility Methods
    # ========================================

    def chat_sync(
        self,
        user_message: str,
        mode: str = "exit",
        allow_tools: bool = True,
        context_text: Optional[str] = None,
        model_override: Optional[str] = None,
    ) -> str:
        """ë™ê¸° ë²„ì „ chat (ê°„ë‹¨í•œ ì‚¬ìš©)

        Args:
            user_message: ì‚¬ìš©ìž ë©”ì‹œì§€
            mode: "exit" (Exit í”„ë¡œì ì…˜), "peer" (Peer PER ë¶„ì„), "diagnosis", "report"

        Returns:
            ì—ì´ì „íŠ¸ ì‘ë‹µ ë¬¸ìžì—´
        """
        import asyncio

        async def run():
            response = ""
            async for chunk in self.chat(
                user_message,
                mode=mode,
                allow_tools=allow_tools,
                context_text=context_text,
                model_override=model_override,
            ):
                response += chunk
            return response

        # Python 3.10+ compatible: asyncio.run() ì‚¬ìš©
        # ë‹¨, ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìžˆìœ¼ë©´ nest_asyncio í•„ìš”
        try:
            # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìžˆëŠ”ì§€ í™•ì¸
            loop = asyncio.get_running_loop()
            # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ìžˆìœ¼ë©´ (ì˜ˆ: Jupyter, Streamlit)
            # nest_asyncio ë˜ëŠ” ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as pool:
                future = pool.submit(asyncio.run, run())
                return future.result()
        except RuntimeError:
            # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ asyncio.run() ì‚¬ìš©
            return asyncio.run(run())

    def get_token_usage(self) -> Dict[str, Any]:
        """í† í° ì‚¬ìš©ëŸ‰ ë° ì˜ˆìƒ ë¹„ìš© ë°˜í™˜"""
        # Claude Opus 4.5 ê°€ê²© (2024ë…„ ê¸°ì¤€)
        INPUT_PRICE_PER_1M = 15.0   # $15 / 1M input tokens
        OUTPUT_PRICE_PER_1M = 75.0  # $75 / 1M output tokens

        input_cost = (self.token_usage["total_input_tokens"] / 1_000_000) * INPUT_PRICE_PER_1M
        output_cost = (self.token_usage["total_output_tokens"] / 1_000_000) * OUTPUT_PRICE_PER_1M
        total_cost = input_cost + output_cost

        return {
            "input_tokens": self.token_usage["total_input_tokens"],
            "output_tokens": self.token_usage["total_output_tokens"],
            "total_tokens": self.token_usage["total_input_tokens"] + self.token_usage["total_output_tokens"],
            "api_calls": self.token_usage["session_calls"],
            "estimated_cost_usd": round(total_cost, 4),
            "estimated_cost_krw": round(total_cost * 1400, 0)  # ëŒ€ëžµì  í™˜ìœ¨
        }

    def reset_token_usage(self):
        """í† í° ì‚¬ìš©ëŸ‰ ì´ˆê¸°í™”"""
        self.token_usage = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "session_calls": 0
        }

    def reset(self):
        """ì„¸ì…˜ ì´ˆê¸°í™”"""
        self.conversation_history = []
        self.voice_conversation_history = []
        self.context = {
            "analyzed_files": [],
            "cached_results": {},
            "last_analysis": None
        }
        self.reset_token_usage()
