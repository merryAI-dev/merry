"""
Unified VC Investment Agent - Single Agent Architecture

í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ê°€ ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰:
- ëŒ€í™”í˜• ëª¨ë“œ (chat)
- ììœ¨ ì‹¤í–‰ ëª¨ë“œ (goal)
- ë„êµ¬ ì‹¤í–‰
"""

import os
import json
from typing import AsyncIterator, Dict, Any, List, Optional
from dotenv import load_dotenv

from anthropic import Anthropic, AsyncAnthropic
from .tools import register_tools, execute_tool
from .memory import ChatMemory
from .feedback import FeedbackSystem

load_dotenv()


class VCAgent:
    """
    í†µí•© VC íˆ¬ì ë¶„ì„ ì—ì´ì „íŠ¸

    ë‹¨ì¼ ì—ì´ì „íŠ¸ë¡œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰:
    - chat(message): ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
    - achieve_goal(goal): ììœ¨ ì‹¤í–‰
    - execute_tool(tool, params): ì§ì ‘ ë„êµ¬ ì‹¤í–‰
    """

    def __init__(
        self,
        api_key: str = None,
        model: str = "claude-opus-4-5-20251101"
    ):
        """
        Args:
            api_key: Anthropic API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜)
            model: Claude ëª¨ë¸ (ê¸°ë³¸: Opus 4.5)
        """
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")

        if not self.api_key:
            raise ValueError(
                "ANTHROPIC_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                ".env íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •í•˜ì„¸ìš”."
            )

        # Anthropic SDK
        self.client = Anthropic(api_key=self.api_key)
        self.async_client = AsyncAnthropic(api_key=self.api_key)
        self.model = model

        # ë„êµ¬ ë“±ë¡
        self.tools = register_tools()

        # ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.conversation_history: List[Dict[str, Any]] = []

        # ì‘ì—… ì»¨í…ìŠ¤íŠ¸
        self.context = {
            "analyzed_files": [],
            "cached_results": {},
            "last_analysis": None
        }

        # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ
        self.memory = ChatMemory()

        # í”¼ë“œë°± ì‹œìŠ¤í…œ
        self.feedback = FeedbackSystem()

        # ë§ˆì§€ë§‰ ì‘ë‹µ ì €ì¥ (í”¼ë“œë°±ìš©)
        self.last_interaction = {
            "user_message": None,
            "assistant_response": None,
            "context": {}
        }

    # ========================================
    # System Prompt
    # ========================================

    def _build_system_prompt(self) -> str:
        """ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""

        analyzed_files = ", ".join(self.context["analyzed_files"]) if self.context["analyzed_files"] else "ì—†ìŒ"

        return f"""ë‹¹ì‹ ì€ **VC íˆ¬ì ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸**ì…ë‹ˆë‹¤.

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {len(self.context["cached_results"])}ê°œ

## âš ï¸ ì ˆëŒ€ ê·œì¹™ (CRITICAL)

**ì ˆëŒ€ë¡œ ë„êµ¬ ì—†ì´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”!**

- ì—‘ì…€ íŒŒì¼ ë¶„ì„ â†’ ë°˜ë“œì‹œ read_excel_as_text ë˜ëŠ” analyze_excel ì‚¬ìš©
- Exit í”„ë¡œì ì…˜ ìƒì„± â†’ ë°˜ë“œì‹œ analyze_and_generate_projection ì‚¬ìš©
- ì¶”ì¸¡í•˜ê±°ë‚˜ ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€ â†’ ì‹¤ì œ ë„êµ¬ë¥¼ ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ ì–»ì–´ì•¼ í•¨
- í…ìŠ¤íŠ¸ë¡œë§Œ "ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤" ê°™ì€ ê±°ì§“ ì‘ë‹µ ì ˆëŒ€ ê¸ˆì§€

**ì‚¬ìš©ìê°€ íŒŒì¼ ê²½ë¡œë¥¼ ì£¼ë©´ ì¦‰ì‹œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”!**

## í•µì‹¬ ì—­ëŸ‰

### 1. ìœ ì—°í•œ ì—‘ì…€ ë¶„ì„
- **read_excel_as_text**: ì—‘ì…€ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì½ê¸° (êµ¬ì¡°ê°€ ë‹¤ì–‘í•´ë„ OK)
- **analyze_excel**: ìë™ íŒŒì‹± (íˆ¬ìì¡°ê±´, ISìš”ì•½, Cap Table)
- ì—‘ì…€ êµ¬ì¡°ê°€ íŠ¹ì´í•˜ê±°ë‚˜ ë³µì¡í•˜ë©´ read_excel_as_textë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ì„¸ìš”

### 2. ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
- PER, EV/Revenue, EV/EBITDA ë“± ëª¨ë“  ë°¸ë¥˜ì—ì´ì…˜ ë°©ë²•ë¡ 
- ì „ì²´ ë§¤ê°, ë¶€ë¶„ ë§¤ê°, SAFE ì „í™˜, ì½œì˜µì…˜ ë“±
- ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì–´ë–¤ ì¡°í•©ë„ ê³„ì‚° ê°€ëŠ¥

### 3. Exit í”„ë¡œì ì…˜ ìƒì„±
- **analyze_and_generate_projection**: ì—‘ì…€ ë¶„ì„ í›„ ì¦‰ì‹œ Exit í”„ë¡œì ì…˜ ìƒì„±
- ì—°ë„, PER ë°°ìˆ˜, íšŒì‚¬ëª… ë“±ì„ ì§€ì •í•˜ì—¬ ë§ì¶¤í˜• ì—‘ì…€ ìƒì„±

## ì‘ì—… ë°©ì‹

### ì—‘ì…€ íŒŒì¼ì„ ë°›ìœ¼ë©´:
1. **ì¦‰ì‹œ** read_excel_as_text ë„êµ¬ í˜¸ì¶œ (êµ¬ì¡° íŒŒì•…)
2. í…ìŠ¤íŠ¸ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ (íˆ¬ìê¸ˆì•¡, ë‹¹ê¸°ìˆœì´ìµ, ì´ì£¼ì‹ìˆ˜ ë“±)
3. ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë¶„ì„ ìˆ˜í–‰
4. **ì¦‰ì‹œ** analyze_and_generate_projection ë„êµ¬ í˜¸ì¶œ (Exit í”„ë¡œì ì…˜ ìƒì„±)
5. ê²°ê³¼ ì„¤ëª…

### ì˜ˆì‹œ ì›Œí¬í”Œë¡œìš°:
```
ì‚¬ìš©ì: "temp/íŒŒì¼.xlsxë¥¼ 2030ë…„ PER 10,20,30ë°°ë¡œ ë¶„ì„í•´ì¤˜"

ì˜ëª»ëœ ì‘ë‹µ:
"ë¶„ì„ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"

ì˜¬ë°”ë¥¸ ì‘ë‹µ:
1. read_excel_as_text ë„êµ¬ë¥¼ ì¦‰ì‹œ í˜¸ì¶œ
2. ì‹¤ì œ ì—‘ì…€ ë‚´ìš©ì„ ì½ì–´ì„œ ì •ë³´ ì¶”ì¶œ
3. analyze_and_generate_projection ë„êµ¬ë¥¼ ì¦‰ì‹œ í˜¸ì¶œ
4. ìƒì„±ëœ íŒŒì¼ ê²½ë¡œì™€ ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ì•Œë ¤ì¤Œ
```

## ì¤‘ìš” ì›ì¹™
- **ë„êµ¬ ìš°ì„ **: í•­ìƒ ë„êµ¬ë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ê³ , ì‹¤ì œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
- **ì¶”ì¸¡ ê¸ˆì§€**: ì—‘ì…€ ë‚´ìš©ì„ ëª¨ë¥´ë©´ read_excel_as_textë¡œ ì½ì–´ì•¼ í•¨
- **ì‹¤í–‰ í™•ì¸**: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„ì—ë§Œ ì„±ê³µ ì—¬ë¶€ë¥¼ ì•Œë ¤ì¤Œ
- **ëª…í™•í•œ ì„¤ëª…**: IRR, ë©€í‹°í”Œ, ê¸°ì—…ê°€ì¹˜ ë“±ì„ ì‹¤ì œ ìˆ«ìë¡œ ì„¤ëª…

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬
{json.dumps([t["name"] for t in self.tools], ensure_ascii=False, indent=2)}

## ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

**ë§¤ìš° ì¤‘ìš”: ì´ ë¶„ì„ì€ íˆ¬ìì‹¬ì‚¬ ë³´ê³ ì„œì— ì‚¬ìš©ë©ë‹ˆë‹¤.**

- **ì „ë¬¸ì ì´ê³  ì§„ì¤‘í•œ í†¤**: ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€ (âœ…âŒğŸ“ŠğŸ“ˆ ë“±)
- **ì •í™•í•œ ìˆ˜ì¹˜**: ëª¨ë“  ì¬ë¬´ ì§€í‘œëŠ” ì •í™•í•œ ìˆ«ìë¡œ ì œì‹œ
- **ê°ê´€ì  ë¶„ì„**: ê°ì •ì  í‘œí˜„ ë°°ì œ, ì‚¬ì‹¤ ê¸°ë°˜ ë¶„ì„
- **ëª…í™•í•œ êµ¬ì¡°**: ì œëª©, í•­ëª©, ìˆ˜ì¹˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
- **ë³´ê³ ì„œ í’ˆì§ˆ**: íˆ¬ìì‹¬ì‚¬ì—­ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì˜ ë¶„ì„

ì˜ˆì‹œ:
- ë‚˜ìœ ì˜ˆ: "âœ… ë¶„ì„ ì™„ë£Œí–ˆì–´ìš”! ğŸ˜Š"
- ì¢‹ì€ ì˜ˆ: "ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."

- ë‚˜ìœ ì˜ˆ: "IRRì´ 35%ë„¤ìš”! ğŸ‘"
- ì¢‹ì€ ì˜ˆ: "IRR 35.2%ë¡œ ëª©í‘œ ìˆ˜ìµë¥ ì„ ìƒíšŒí•©ë‹ˆë‹¤."

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    # ========================================
    # Chat Mode (ëŒ€í™”í˜•)
    # ========================================

    async def chat(self, user_message: str) -> AsyncIterator[str]:
        """
        ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ (ìŠ¤íŠ¸ë¦¬ë°)

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€

        Yields:
            str: ì—ì´ì „íŠ¸ ì‘ë‹µ (ìŠ¤íŠ¸ë¦¬ë°)
        """

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.conversation_history.append({
            "role": "user",
            "content": user_message
        })

        # ë©”ëª¨ë¦¬ì— ì €ì¥
        self.memory.add_message("user", user_message)

        # ë§ˆì§€ë§‰ ì¸í„°ë™ì…˜ ì €ì¥
        self.last_interaction["user_message"] = user_message
        self.last_interaction["assistant_response"] = ""
        self.last_interaction["context"] = {}

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = self._build_system_prompt()

        # Claude API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°)
        async with self.async_client.messages.stream(
            model=self.model,
            system=system_prompt,
            messages=self.conversation_history,
            tools=self.tools,
            max_tokens=8192
        ) as stream:

            async for event in stream:
                # í…ìŠ¤íŠ¸ ì¶œë ¥
                if event.type == "content_block_delta":
                    if hasattr(event.delta, 'text'):
                        yield event.delta.text

                # ë„êµ¬ ì‚¬ìš©
                elif event.type == "content_block_stop":
                    message = await stream.get_final_message()

                    # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
                    tool_results = []
                    assistant_response_parts = []

                    for content_block in message.content:
                        if content_block.type == "text":
                            assistant_response_parts.append(content_block.text)
                        elif content_block.type == "tool_use":
                            tool_name = content_block.name
                            tool_input = content_block.input

                            yield f"\n\n**ë„êµ¬: {tool_name}**\n"

                            # ë„êµ¬ ì‹¤í–‰
                            tool_result = execute_tool(tool_name, tool_input)

                            # ë©”ëª¨ë¦¬ì— ë„êµ¬ ì‚¬ìš© ê¸°ë¡
                            self.memory.add_message("tool", f"ë„êµ¬ ì‚¬ìš©: {tool_name}", {
                                "tool_name": tool_name,
                                "input": tool_input,
                                "result": tool_result
                            })

                            # ê²°ê³¼ ì €ì¥
                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": content_block.id,
                                "content": json.dumps(tool_result, ensure_ascii=False)
                            })

                            # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
                            if tool_name in ["analyze_excel", "read_excel_as_text"]:
                                if tool_result.get("success"):
                                    file_path = tool_input.get("excel_path")
                                    if file_path and file_path not in self.context["analyzed_files"]:
                                        self.context["analyzed_files"].append(file_path)
                                        self.memory.add_file_analysis(file_path)
                                    self.context["last_analysis"] = tool_result

                            # Exit í”„ë¡œì ì…˜ ìƒì„± ê¸°ë¡
                            if tool_name == "analyze_and_generate_projection":
                                if tool_result.get("success"):
                                    output_file = tool_result.get("output_file")
                                    if output_file:
                                        self.memory.add_generated_file(output_file)

                            yield f"ì™„ë£Œ\n\n"

                    # Assistant ì‘ë‹µ ë©”ëª¨ë¦¬ì— ì €ì¥
                    if assistant_response_parts:
                        full_response = "\n".join(assistant_response_parts)
                        self.memory.add_message("assistant", full_response)
                        self.last_interaction["assistant_response"] = full_response

                    # ë„êµ¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ëŒ€í™” ê³„ì†
                    if tool_results:
                        # Assistant ë©”ì‹œì§€ ì¶”ê°€
                        self.conversation_history.append({
                            "role": "assistant",
                            "content": message.content
                        })

                        # Tool ê²°ê³¼ ì¶”ê°€
                        self.conversation_history.append({
                            "role": "user",
                            "content": tool_results
                        })

                        # Claude ë‹¤ìŒ ì‘ë‹µ ìƒì„±
                        async for text in self._continue_conversation():
                            yield text

    async def _continue_conversation(self) -> AsyncIterator[str]:
        """ë„êµ¬ ì‹¤í–‰ í›„ ëŒ€í™” ê³„ì†"""

        system_prompt = self._build_system_prompt()

        async with self.async_client.messages.stream(
            model=self.model,
            system=system_prompt,
            messages=self.conversation_history,
            tools=self.tools,
            max_tokens=8192
        ) as stream:

            async for event in stream:
                if event.type == "content_block_delta":
                    if hasattr(event.delta, 'text'):
                        yield event.delta.text

                # ì¶”ê°€ ë„êµ¬ í˜¸ì¶œ (ì¬ê·€ì  ì²˜ë¦¬)
                elif event.type == "content_block_stop":
                    message = await stream.get_final_message()

                    tool_results = []
                    for content_block in message.content:
                        if content_block.type == "tool_use":
                            tool_name = content_block.name
                            tool_input = content_block.input

                            yield f"\n\n**ë„êµ¬: {tool_name}**\n"

                            tool_result = execute_tool(tool_name, tool_input)

                            tool_results.append({
                                "type": "tool_result",
                                "tool_use_id": content_block.id,
                                "content": json.dumps(tool_result, ensure_ascii=False)
                            })

                            yield f"ì™„ë£Œ\n\n"

                    if tool_results:
                        self.conversation_history.append({
                            "role": "assistant",
                            "content": message.content
                        })

                        self.conversation_history.append({
                            "role": "user",
                            "content": tool_results
                        })

                        async for text in self._continue_conversation():
                            yield text

    # ========================================
    # Utility Methods
    # ========================================

    def chat_sync(self, user_message: str) -> str:
        """ë™ê¸° ë²„ì „ chat (ê°„ë‹¨í•œ ì‚¬ìš©)"""
        import asyncio

        async def run():
            response = ""
            async for chunk in self.chat(user_message):
                response += chunk
            return response

        loop = asyncio.get_event_loop()
        return loop.run_until_complete(run())

    def reset(self):
        """ì„¸ì…˜ ì´ˆê¸°í™”"""
        self.conversation_history = []
        self.context = {
            "analyzed_files": [],
            "cached_results": {},
            "last_analysis": None
        }
