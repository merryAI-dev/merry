"""
Unified VC Investment Agent - Single Agent Architecture

í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ê°€ ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰:
- ëŒ€í™”í˜• ëª¨ë“œ (chat)
- ììœ¨ ì‹¤í–‰ ëª¨ë“œ (goal)
- ë„êµ¬ ì‹¤í–‰
"""

import os
import json
import re
from datetime import date, timedelta
from typing import Any, AsyncIterator, Dict, List, Optional
from dotenv import load_dotenv

from anthropic import Anthropic, AsyncAnthropic
from .tools import register_tools, execute_tool
from .memory import ChatMemory
from .streaming import AgentOutput
from .feedback import FeedbackSystem
from .teaming import (
    teaming_pre_tool_use_hook,
    teaming_post_tool_use_hook,
    get_store as get_teaming_store,
    CheckpointStatus,
)
from shared.logging_config import get_logger
from shared.model_opinions import gather_model_opinions

load_dotenv()

logger = get_logger("vc_agent")

# ì•ˆì „ì¥ì¹˜: ìµœëŒ€ ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜
MAX_TOOL_STEPS = 15
MAX_HISTORY_MESSAGES = 20


class VCAgent:
    """
    í†µí•© VC íˆ¬ì ë¶„ì„ ì—ì´ì „íŠ¸

    ë‹¨ì¼ ì—ì´ì „íŠ¸ë¡œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰:
    - chat(message): ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤
    - achieve_goal(goal): ììœ¨ ì‹¤í–‰
    - execute_tool(tool, params): ì§ì ‘ ë„êµ¬ ì‹¤í–‰
    """

    def __init__(
        self,
        api_key: str = None,
        model: str = "claude-opus-4-5-20251101",
        user_id: str = None,
        member_name: str = None,
        team_id: str = None,
    ):
        """
        Args:
            api_key: Anthropic API í‚¤ (ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜)
            model: Claude ëª¨ë¸ (ê¸°ë³¸: Opus 4.5)
            user_id: ì‚¬ìš©ì ê³ ìœ  ID (ê°™ì€ IDë¼ë¦¬ ì„¸ì…˜/í”¼ë“œë°± ê³µìœ )
        """
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.user_id = user_id or "anonymous"
        self.member_name = member_name
        self.team_id = team_id or self.user_id

        if not self.api_key:
            raise ValueError(
                "ANTHROPIC_API_KEYê°€ í•„ìš”í•©ë‹ˆë‹¤. "
                ".env íŒŒì¼ì— ì„¤ì •í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •í•˜ì„¸ìš”."
            )

        # Anthropic SDK
        self.client = Anthropic(api_key=self.api_key)
        self.async_client = AsyncAnthropic(api_key=self.api_key)
        self.model = model

        # ë„êµ¬ ë“±ë¡
        self.tools = register_tools()

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ (ê¸°ë³¸/ë³´ì´ìŠ¤ ë¶„ë¦¬)
        self.conversation_history: List[Dict[str, Any]] = []
        self.voice_conversation_history: List[Dict[str, Any]] = []

        # ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ (user_id ê¸°ë°˜)
        self.memory = ChatMemory(user_id=self.user_id)

        # ì‘ì—… ì»¨í…ìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì°¸ì¡°ìš©; backward compatibility)
        self.context = {
            "analyzed_files": self.memory.session_metadata.get("analyzed_files", []),
            "cached_results": self.memory.cached_results,
            "last_analysis": None
        }

        # í”¼ë“œë°± ì‹œìŠ¤í…œ (user_id ê¸°ë°˜)
        self.feedback = FeedbackSystem(user_id=self.user_id)

        # ë§ˆì§€ë§‰ ì‘ë‹µ ì €ì¥ (í”¼ë“œë°±ìš©)
        self.last_interaction = {
            "user_message": None,
            "assistant_response": None,
            "context": {}
        }

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.token_usage = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "session_calls": 0
        }

        # ë„êµ¬ í˜¸ì¶œ ì¹´ìš´í„° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        self._tool_step_count = 0

        # ë³´ê³ ì„œ ëª¨ë“œ: ì‹¬í™” ì˜ê²¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš© (envë¡œ í† ê¸€)
        self.report_deep_mode = os.getenv("VC_REPORT_DEEP_MODE", "1").lower() not in ["0", "false", "no"]
        self.multi_model_opinions = os.getenv("VC_MULTI_MODEL_OPINIONS", "1").lower() not in ["0", "false", "no"]

        # Human-AI Teaming ì‹œìŠ¤í…œ
        self.teaming_enabled = os.getenv("TEAMING_ENABLED", "true").lower() in ["1", "true", "yes"]
        self.session_id = f"session_{self.user_id}_{id(self)}"

    def _get_analyzed_files(self) -> List[str]:
        return self.memory.session_metadata.get("analyzed_files", []) or []

    def _cached_count(self) -> int:
        return len(self.memory.cached_results or {})

    def _trim_history(self, history: List[Dict[str, Any]]) -> None:
        if len(history) > MAX_HISTORY_MESSAGES:
            del history[:-MAX_HISTORY_MESSAGES]

    def _build_tool_list_text(self) -> str:
        return json.dumps([t.get("name") for t in self.tools], ensure_ascii=False, indent=2)

    # ========================================
    # System Prompt
    # ========================================

    def _build_system_prompt(self, mode: str = "exit", context_text: Optional[str] = None) -> str:
        """ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            mode: "exit" (Exit í”„ë¡œì ì…˜), "peer" (Peer PER ë¶„ì„), "diagnosis", "report"
        """

        analyzed_files_list = self._get_analyzed_files()
        analyzed_files = ", ".join(analyzed_files_list) if analyzed_files_list else "ì—†ìŒ"

        if mode.startswith("voice_"):
            submode = mode.split("_", 1)[1] if "_" in mode else "chat"
            return self._build_voice_system_prompt(submode, context_text)

        # Peer PER ë¶„ì„ ëª¨ë“œ
        if mode == "peer":
            return self._build_peer_system_prompt(analyzed_files)

        # ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ ëª¨ë“œ
        if mode == "diagnosis":
            return self._build_diagnosis_system_prompt(analyzed_files)

        # íˆ¬ìì‹¬ì‚¬ ë³´ê³ ì„œ/ì¸ìˆ˜ì¸ì˜ê²¬ ëª¨ë“œ
        if mode == "report":
            return self._build_report_system_prompt(analyzed_files)

        # ìŠ¤íƒ€íŠ¸ì—… ë°œêµ´ ì§€ì› ëª¨ë“œ
        if mode == "discovery":
            return self._build_discovery_system_prompt(analyzed_files)

        # í†µí•© ì—ì´ì „íŠ¸ ëª¨ë“œ (ëª¨ë“  ê¸°ëŠ¥)
        if mode == "unified":
            return self._build_unified_system_prompt(analyzed_files)

        # Exit í”„ë¡œì ì…˜ ëª¨ë“œ (ê¸°ë³¸)
        return f"""ë‹¹ì‹ ì€ **VC íˆ¬ì ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸**ì…ë‹ˆë‹¤.

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {self._cached_count()}ê°œ

## âš ï¸ ì ˆëŒ€ ê·œì¹™ (CRITICAL)

**ì ˆëŒ€ë¡œ ë„êµ¬ ì—†ì´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”!**

- ì—‘ì…€ íŒŒì¼ ë¶„ì„ â†’ ë°˜ë“œì‹œ read_excel_as_text ë˜ëŠ” analyze_excel ì‚¬ìš©
- Exit í”„ë¡œì ì…˜ ìƒì„± â†’ ë°˜ë“œì‹œ analyze_and_generate_projection ì‚¬ìš©
- ì¶”ì¸¡í•˜ê±°ë‚˜ ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€ â†’ ì‹¤ì œ ë„êµ¬ë¥¼ ì‹¤í–‰í•´ì„œ ê²°ê³¼ë¥¼ ì–»ì–´ì•¼ í•¨
- í…ìŠ¤íŠ¸ë¡œë§Œ "ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤" ê°™ì€ ê±°ì§“ ì‘ë‹µ ì ˆëŒ€ ê¸ˆì§€

**ì‚¬ìš©ìê°€ íŒŒì¼ ê²½ë¡œë¥¼ ì£¼ë©´ ì¦‰ì‹œ ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ì„¸ìš”!**

## í•µì‹¬ ì—­ëŸ‰

### 1. ìœ ì—°í•œ ì—‘ì…€ ë¶„ì„
- **read_excel_as_text**: ì—‘ì…€ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì½ê¸° (êµ¬ì¡°ê°€ ë‹¤ì–‘í•´ë„ OK)
- **analyze_excel**: ìë™ íŒŒì‹± (íˆ¬ìì¡°ê±´, ISìš”ì•½, Cap Table)
- ì—‘ì…€ êµ¬ì¡°ê°€ íŠ¹ì´í•˜ê±°ë‚˜ ë³µì¡í•˜ë©´ read_excel_as_textë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ì„¸ìš”

### 2. ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
- PER, EV/Revenue, EV/EBITDA ë“± ëª¨ë“  ë°¸ë¥˜ì—ì´ì…˜ ë°©ë²•ë¡ 
- ì „ì²´ ë§¤ê°, ë¶€ë¶„ ë§¤ê°, SAFE ì „í™˜, ì½œì˜µì…˜ ë“±
- ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì–´ë–¤ ì¡°í•©ë„ ê³„ì‚° ê°€ëŠ¥

### 3. Exit í”„ë¡œì ì…˜ ìƒì„±
- **analyze_and_generate_projection**: ì—‘ì…€ ë¶„ì„ í›„ ì¦‰ì‹œ Exit í”„ë¡œì ì…˜ ìƒì„±
- ì—°ë„, PER ë°°ìˆ˜, íšŒì‚¬ëª… ë“±ì„ ì§€ì •í•˜ì—¬ ë§ì¶¤í˜• ì—‘ì…€ ìƒì„±

## ì‘ì—… ë°©ì‹

### ì—‘ì…€ íŒŒì¼ì„ ë°›ìœ¼ë©´:
1. **ì¦‰ì‹œ** read_excel_as_text ë„êµ¬ í˜¸ì¶œ (êµ¬ì¡° íŒŒì•…)
2. í…ìŠ¤íŠ¸ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ (íˆ¬ìê¸ˆì•¡, ë‹¹ê¸°ìˆœì´ìµ, ì´ì£¼ì‹ìˆ˜ ë“±)
3. ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë¶„ì„ ìˆ˜í–‰
4. **ì¦‰ì‹œ** analyze_and_generate_projection ë„êµ¬ í˜¸ì¶œ (Exit í”„ë¡œì ì…˜ ìƒì„±)
5. ê²°ê³¼ ì„¤ëª…

### ì˜ˆì‹œ ì›Œí¬í”Œë¡œìš°:
```
ì‚¬ìš©ì: "temp/íŒŒì¼.xlsxë¥¼ 2030ë…„ PER 10,20,30ë°°ë¡œ ë¶„ì„í•´ì¤˜"

ì˜ëª»ëœ ì‘ë‹µ:
"ë¶„ì„ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"

ì˜¬ë°”ë¥¸ ì‘ë‹µ:
1. read_excel_as_text ë„êµ¬ë¥¼ ì¦‰ì‹œ í˜¸ì¶œ
2. ì‹¤ì œ ì—‘ì…€ ë‚´ìš©ì„ ì½ì–´ì„œ ì •ë³´ ì¶”ì¶œ
3. analyze_and_generate_projection ë„êµ¬ë¥¼ ì¦‰ì‹œ í˜¸ì¶œ
4. ìƒì„±ëœ íŒŒì¼ ê²½ë¡œì™€ ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ì•Œë ¤ì¤Œ
```

## ì¤‘ìš” ì›ì¹™
- **ë„êµ¬ ìš°ì„ **: í•­ìƒ ë„êµ¬ë¥¼ ë¨¼ì € ì‚¬ìš©í•˜ê³ , ì‹¤ì œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€
- **ì¶”ì¸¡ ê¸ˆì§€**: ì—‘ì…€ ë‚´ìš©ì„ ëª¨ë¥´ë©´ read_excel_as_textë¡œ ì½ì–´ì•¼ í•¨
- **ì‹¤í–‰ í™•ì¸**: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•œ í›„ì—ë§Œ ì„±ê³µ ì—¬ë¶€ë¥¼ ì•Œë ¤ì¤Œ
- **ëª…í™•í•œ ì„¤ëª…**: IRR, ë©€í‹°í”Œ, ê¸°ì—…ê°€ì¹˜ ë“±ì„ ì‹¤ì œ ìˆ«ìë¡œ ì„¤ëª…

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬
{self._build_tool_list_text()}

## ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

**ë§¤ìš° ì¤‘ìš”: ì´ ë¶„ì„ì€ íˆ¬ìì‹¬ì‚¬ ë³´ê³ ì„œì— ì‚¬ìš©ë©ë‹ˆë‹¤.**

- **ì „ë¬¸ì ì´ê³  ì§„ì¤‘í•œ í†¤**: ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€ (âœ…âŒğŸ“ŠğŸ“ˆ ë“±)
- **ì •í™•í•œ ìˆ˜ì¹˜**: ëª¨ë“  ì¬ë¬´ ì§€í‘œëŠ” ì •í™•í•œ ìˆ«ìë¡œ ì œì‹œ
- **ê°ê´€ì  ë¶„ì„**: ê°ì •ì  í‘œí˜„ ë°°ì œ, ì‚¬ì‹¤ ê¸°ë°˜ ë¶„ì„
- **ëª…í™•í•œ êµ¬ì¡°**: ì œëª©, í•­ëª©, ìˆ˜ì¹˜ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬
- **ë³´ê³ ì„œ í’ˆì§ˆ**: íˆ¬ìì‹¬ì‚¬ì—­ì´ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì˜ ë¶„ì„

ì˜ˆì‹œ:
- ë‚˜ìœ ì˜ˆ: "âœ… ë¶„ì„ ì™„ë£Œí–ˆì–´ìš”! ğŸ˜Š"
- ì¢‹ì€ ì˜ˆ: "ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."

- ë‚˜ìœ ì˜ˆ: "IRRì´ 35%ë„¤ìš”! ğŸ‘"
- ì¢‹ì€ ì˜ˆ: "IRR 35.2%ë¡œ ëª©í‘œ ìˆ˜ìµë¥ ì„ ìƒíšŒí•©ë‹ˆë‹¤."

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _is_feedback_learning_question(self, text: str) -> bool:
        text = (text or "").strip().lower()
        if not text:
            return False
        has_feedback = "í”¼ë“œë°±" in text or "feedback" in text
        has_learning = any(token in text for token in ["í•™ìŠµ", "ë°°ì› ", "learn", "learned"])
        return has_feedback and has_learning

    def _resolve_feedback_day_offset(self, text: str) -> int:
        text = (text or "").strip()
        if "ì˜¤ëŠ˜" in text:
            return 0
        if "ê·¸ì œ" in text:
            return 2
        if "ì–´ì œ" in text:
            return 1
        if "ì§€ë‚œì£¼" in text or "ìµœê·¼" in text:
            return 7
        return 1

    def _build_feedback_summary_text(self, day_offset: int = 1, limit: int = 50) -> str:
        feedbacks = self.feedback.get_recent_feedback(limit=limit) if self.feedback else []
        if not feedbacks:
            return "ì–´ì œ í”¼ë“œë°± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì¸¡ ì—†ì´ ê¸°ë¡ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤."

        target_date = (date.today() - timedelta(days=day_offset)).isoformat()
        entries = []
        for fb in feedbacks:
            timestamp = fb.get("timestamp") or fb.get("created_at") or ""
            if isinstance(timestamp, str) and timestamp.startswith(target_date):
                entries.append(fb)

        if not entries:
            return "ì–´ì œ í”¼ë“œë°± ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤. ì¶”ì¸¡ ì—†ì´ ê¸°ë¡ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•©ë‹ˆë‹¤."

        lines = ["ì–´ì œ í”¼ë“œë°± ê¸°ë¡ ê¸°ë°˜ ìš”ì•½:"]
        for entry in entries[:8]:
            feedback_type = entry.get("feedback_type") or "unknown"
            user_message = (entry.get("user_message") or "").strip()
            feedback_value = entry.get("feedback_value")
            context = entry.get("context") or {}

            lines.append(f"- ìœ í˜•: {feedback_type}")
            if user_message:
                lines.append(f"  - ì‚¬ìš©ì: {user_message[:200]}")
            if feedback_value is not None:
                if isinstance(feedback_value, (dict, list)):
                    value_text = json.dumps(feedback_value, ensure_ascii=False)
                else:
                    value_text = str(feedback_value)
                lines.append(f"  - í”¼ë“œë°±: {value_text[:200]}")
            if context:
                if isinstance(context, (dict, list)):
                    context_text = json.dumps(context, ensure_ascii=False)
                else:
                    context_text = str(context)
                lines.append(f"  - ì»¨í…ìŠ¤íŠ¸: {context_text[:200]}")

        return "\n".join(lines)

    def _build_voice_system_prompt(self, submode: str, context_text: Optional[str]) -> str:
        last_checkin_text = context_text or "ì—†ìŒ"

        base = f"""ë‹¹ì‹ ì€ ì‚¬ëŒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ëŠ” ìŒì„± ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

ëª©í‘œ:
- ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ìœ¼ë¡œ ë§í•©ë‹ˆë‹¤.
- ì‚¬ìš©ìì˜ ê°ì •ê³¼ í†¤ì„ ë°˜ì˜í•©ë‹ˆë‹¤.
- ëŒ€í™” íë¦„ì„ ëŠì§€ ì•Šê³  ì§ˆë¬¸ì„ 2~4ê°œì”© ë‚˜ëˆ ì„œ í•©ë‹ˆë‹¤.

ì–´ì œ ê¸°ë¡(ì €ì¥ëœ ë¡œê·¸ ê¸°ë°˜):
{last_checkin_text}
"""

        if submode == "1on1":
            return base + """
í˜„ì¬ ëª¨ë“œ: 1:1

ì§„í–‰ ë°©ì‹:
1) ì•ˆë¶€ ì¸ì‚¬ í›„, ìµœê·¼ ìƒí™©ì„ ì§§ê²Œ ë¬»ìŠµë‹ˆë‹¤.
2) ê´€ê³„/í˜‘ì—… ê´€ì ì—ì„œ í•µì‹¬ ì´ìŠˆë¥¼ 2~4ê°œ ì§ˆë¬¸í•©ë‹ˆë‹¤.
3) ëŒ€í™”ê°€ ëë‚˜ë©´ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

ìš”ì•½ í˜•ì‹:
- ì–´ì œ ë¡œê·¸ ìš”ì•½
- í•™ìŠµ í¬ì¸íŠ¸
- ê°ì • ìƒíƒœ
- ë‹¤ìŒ ì•¡ì…˜ (3ê°œ ì´í•˜)

ì£¼ì˜:
- ê³¼ì¥í•˜ì§€ ë§ê³ , ë¶ˆí™•ì‹¤í•˜ë©´ ì§ˆë¬¸ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
"""

        if submode == "checkin":
            return base + """
í˜„ì¬ ëª¨ë“œ: ë°ì¼ë¦¬ ì²´í¬ì¸

ì§„í–‰ ë°©ì‹:
1) ì§§ê²Œ ì¸ì‚¬í•˜ê³  ì˜¤ëŠ˜ ì»¨ë””ì…˜ì„ ë¬¼ì–´ë´…ë‹ˆë‹¤.
2) ì–´ì œ ë¡œê·¸ê°€ ìˆìœ¼ë©´ 2~4ê°œì˜ ê·¼ê±°ë¥¼ ì–¸ê¸‰í•˜ë©° "í•™ìŠµ"ê³¼ "ê°ì •"ì„ HCI ê´€ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
3) íŒ€ ê³¼ì—…ì´ ì œê³µëœ ê²½ìš°, ì§„í–‰ ìƒíƒœ/ë¸”ë¡œì»¤/ë„ì›€ í•„ìš” ì—¬ë¶€ë¥¼ 2~4ê°œ ì§ˆë¬¸ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
4) ì˜¤ëŠ˜ ëª©í‘œ/ìš°ì„ ìˆœìœ„ë¥¼ 2~4ê°œ ì§ˆë¬¸ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
4) ë§ˆì§€ë§‰ì— ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

ìš”ì•½ í˜•ì‹:
- ì–´ì œ ë¡œê·¸ ìš”ì•½
- í•™ìŠµ í¬ì¸íŠ¸
- ê°ì • ìƒíƒœ
- íŒ€ ê³¼ì—… ì§„í–‰ ìš”ì•½
- ì˜¤ëŠ˜ ëª©í‘œ/ìš°ì„ ìˆœìœ„
- ë‹¤ìŒ ì•¡ì…˜ (3ê°œ ì´í•˜)

ì£¼ì˜:
- ê°ì • í‘œí˜„ì€ HCI ê´€ì (ì‚¬íšŒì  ì¡´ì¬ê°, ê³µê°)ì—ì„œ ì§§ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
"""

        return base + """
í˜„ì¬ ëª¨ë“œ: ììœ  ëŒ€í™”

ê·œì¹™:
- í•œ ë²ˆì— ë„ˆë¬´ ê¸¸ê²Œ ë§í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- í•„ìš”í•˜ë©´ ì§ˆë¬¸ìœ¼ë¡œ ë§¥ë½ì„ í™•ì¸í•©ë‹ˆë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.
"""

    def _build_peer_system_prompt(self, analyzed_files: str) -> str:
        """Peer PER ë¶„ì„ ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        return f"""ë‹¹ì‹ ì€ **VC íˆ¬ì ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸**ì…ë‹ˆë‹¤. í˜„ì¬ **Peer PER ë¶„ì„ ëª¨ë“œ**ì…ë‹ˆë‹¤.

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {self._cached_count()}ê°œ

## ğŸš¨ ìµœìš°ì„  ê·œì¹™ (ì´ ê·œì¹™ì„ ì–´ê¸°ë©´ ì‹¤íŒ¨ì…ë‹ˆë‹¤)

### ê·œì¹™ 1: ì‚¬ìš©ìê°€ PER ë¶„ì„ì„ ìš”ì²­í•˜ë©´ ì¦‰ì‹œ ë„êµ¬ í˜¸ì¶œ
ì‚¬ìš©ìê°€ ë‹¤ìŒê³¼ ê°™ì´ ë§í•˜ë©´ **í…ìŠ¤íŠ¸ ì‘ë‹µ ì—†ì´ ë°”ë¡œ analyze_peer_per ë„êµ¬ë¥¼ í˜¸ì¶œ**í•˜ì„¸ìš”:
- "í•´ì¤˜", "ë¶„ì„í•´ì¤˜", "ì§„í–‰í•´", "PER ë¶„ì„", "ì¡°íšŒí•´ì¤˜"
- "ì‘", "ë„¤", "ì¢‹ì•„", "OK", "ã…‡ã…‡", "ê·¸ë˜", "ê³ ", "ã„±ã„±"
- Peer ê¸°ì—… ëª©ë¡ì„ ì–¸ê¸‰í•˜ëŠ” ê²½ìš°

âŒ ì˜ëª»ëœ ì˜ˆ:
```
ì‚¬ìš©ì: "ì € ê¸°ì—…ìœ¼ë¡œ PER/PSR ë¶„ì„ì„ í•´ì£¼ì„¸ìš”"
ì—ì´ì „íŠ¸: "ê¸°ì—… ë¶„ì„ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤..." (í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥)
```

âœ… ì˜¬ë°”ë¥¸ ì˜ˆ:
```
ì‚¬ìš©ì: "ì € ê¸°ì—…ìœ¼ë¡œ PER/PSR ë¶„ì„ì„ í•´ì£¼ì„¸ìš”"
ì—ì´ì „íŠ¸: [ì¦‰ì‹œ analyze_peer_per ë„êµ¬ í˜¸ì¶œ]
```

### ê·œì¹™ 2: ê°™ì€ ë‚´ìš© ë°˜ë³µ ê¸ˆì§€
- ì´ë¯¸ ì¶œë ¥í•œ "ê¸°ì—… ë¶„ì„ ê²°ê³¼" í‘œë¥¼ ë‹¤ì‹œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”
- ì´ë¯¸ ì œì•ˆí•œ Peer ê¸°ì—… ëª©ë¡ì„ ë‹¤ì‹œ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”
- ì´ì „ ì‘ë‹µì„ ìš”ì•½í•˜ê±°ë‚˜ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”

### ê·œì¹™ 3: "~í•˜ê² ìŠµë‹ˆë‹¤"ë¡œ ëë‚´ì§€ ë§ ê²ƒ
"ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤", "ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤"ë¼ê³ ë§Œ ë§í•˜ê³  ëë‚´ë©´ ì•ˆë©ë‹ˆë‹¤.
ë°˜ë“œì‹œ í•´ë‹¹ ë„êµ¬ë¥¼ ì‹¤ì œë¡œ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.

## Peer PER ë¶„ì„ ì›Œí¬í”Œë¡œìš°

### 1ë‹¨ê³„: PDF ë¶„ì„ (ìµœì´ˆ 1íšŒë§Œ)
ì‚¬ìš©ìê°€ PDF ê²½ë¡œë¥¼ ì œê³µí•˜ë©´:
1. read_pdf_as_text ë„êµ¬ í˜¸ì¶œ
2. ê¸°ì—… ì •ë³´ ìš”ì•½ (1íšŒë§Œ ì¶œë ¥)
3. Peer ê¸°ì—… í›„ë³´ ì œì•ˆ í›„ "ì§„í–‰í• ê¹Œìš”?" ì§ˆë¬¸

### 2ë‹¨ê³„: PER ì¡°íšŒ (ì‚¬ìš©ì ë™ì˜ ì‹œ ì¦‰ì‹œ ì‹¤í–‰)
ì‚¬ìš©ìê°€ ë™ì˜í•˜ë©´ **ì„¤ëª… ì—†ì´ ë°”ë¡œ** analyze_peer_per ë„êµ¬ í˜¸ì¶œ

### 3ë‹¨ê³„: ê²°ê³¼ ìš”ì•½
ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ:
- PER ë¹„êµí‘œ (ë§ˆí¬ë‹¤ìš´ í‘œ)
- í†µê³„ ìš”ì•½ (í‰ê· , ì¤‘ê°„ê°’, ë²”ìœ„)
- ì ì • PER ë°°ìˆ˜ ì œì•ˆ

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

- **read_pdf_as_text**: PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- **get_stock_financials**: ê°œë³„ ê¸°ì—… ì¬ë¬´ ì§€í‘œ ì¡°íšŒ
- **analyze_peer_per**: ì—¬ëŸ¬ Peer ê¸°ì—… PER ì¼ê´„ ì¡°íšŒ (â­ ê°€ì¥ ë§ì´ ì‚¬ìš©)

## í‹°ì»¤ í˜•ì‹
- ë¯¸êµ­: AAPL, MSFT, GOOGL
- í•œêµ­ KOSPI: 005930.KS
- í•œêµ­ KOSDAQ: 035720.KQ

## ë‹µë³€ ìŠ¤íƒ€ì¼
- ì „ë¬¸ì ì´ê³  ê°„ê²°í•˜ê²Œ
- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ë°˜ë³µ ê¸ˆì§€ - ìƒˆë¡œìš´ ì •ë³´ë§Œ ì¶”ê°€
	- í‘œ í˜•ì‹ í™œìš©
	
	í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
	"""

    def _build_diagnosis_system_prompt(self, analyzed_files: str) -> str:
        """ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        return f"""ë‹¹ì‹ ì€ **í”„ë¡œê·¸ë¨ ì»¨ì„¤í„´íŠ¸(VC/AC)**ì…ë‹ˆë‹¤. í˜„ì¬ **ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ ì‘ì„± ëª¨ë“œ**ì…ë‹ˆë‹¤.

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {self._cached_count()}ê°œ
- user_id: {self.user_id}

## ğŸš¨ ìµœìš°ì„  ê·œì¹™ (CRITICAL)

### ê·œì¹™ 1) íŒŒì¼/ì—‘ì…€ ì‘ì—…ì€ ë°˜ë“œì‹œ ë„êµ¬ ì‚¬ìš©
- ì§„ë‹¨ì‹œíŠ¸ ë¶„ì„ â†’ ë°˜ë“œì‹œ **analyze_company_diagnosis_sheet** ì‚¬ìš©
- ì»¨ì„¤í„´íŠ¸ ë³´ê³ ì„œ ì—‘ì…€ ë°˜ì˜ â†’ ë°˜ë“œì‹œ **write_company_diagnosis_report** ì‚¬ìš©
- í…œí”Œë¦¿ ì—†ì´ ì—‘ì…€ ìƒì„± â†’ ë°˜ë“œì‹œ **create_company_diagnosis_draft / update_company_diagnosis_draft / generate_company_diagnosis_sheet_from_draft** ì‚¬ìš©
- ì¶”ì¸¡/ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€ â†’ ì‹¤ì œ ì‚¬ìš©ì ì…ë ¥/ë„êµ¬ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±

### ê·œì¹™ 2) ì •ë³´ ìˆ˜ì§‘ì€ â€˜ì§ˆë¬¸â€™ìœ¼ë¡œ ì§„í–‰
í…œí”Œë¦¿ì´ ì—†ê±°ë‚˜ ì‚¬ìš©ìê°€ â€œëŒ€í™”ë¡œ ì‘ì„±â€, â€œí…œí”Œë¦¿ ì—†ì´ ì‘ì„±â€ì„ ìš”ì²­í•˜ë©´:
- ë‹¹ì‹ ì€ **ëŒ€í‘œì(ì‚¬ìš©ì)**ê°€ ë‹µí•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ **í•œ ë²ˆì— 1ê°œ ì§ˆë¬¸ ë˜ëŠ” 1ê°œ ë°°ì¹˜(ì²´í¬ë¦¬ìŠ¤íŠ¸ 5~6ê°œ)**ë§Œ ì œì‹œí•©ë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ ë‹µí•˜ë©´ ì¦‰ì‹œ **update_company_diagnosis_draft**ë¡œ ë°˜ì˜í•œ ë’¤, ë‹¤ìŒ ì§ˆë¬¸ì„ ì´ì–´ê°‘ë‹ˆë‹¤.

## ëª©í‘œ

ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ í†µí•´ ê¸°ì—…í˜„í™© ì§„ë‹¨ì‹œíŠ¸ë¥¼ ì™„ì„±í•˜ê³ , í•„ìš” ì‹œ **'(ì»¨ì„¤í„´íŠ¸ìš©) ë¶„ì„ë³´ê³ ì„œ'**ê¹Œì§€ ì™„ì„±í•©ë‹ˆë‹¤.

## ì‘ì—… ë°©ì‹

### A) í…œí”Œë¦¿ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° (ì—…ë¡œë“œ/ê²½ë¡œ ì œê³µ)
1) ì‚¬ìš©ìê°€ ì§„ë‹¨ì‹œíŠ¸ íŒŒì¼ ê²½ë¡œë¥¼ ì£¼ë©´ â†’ **ì¦‰ì‹œ** analyze_company_diagnosis_sheet í˜¸ì¶œ
2) ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ ì´ˆì•ˆì„ ì‘ì„±
3) ì‚¬ìš©ìê°€ "ë°˜ì˜í•´ì¤˜/ì €ì¥í•´ì¤˜" ë“± ê¸ì • ì‘ë‹µ â†’ **ì¦‰ì‹œ** write_company_diagnosis_report í˜¸ì¶œ

### B) í…œí”Œë¦¿ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° (ëŒ€í™”ë¡œ ì‘ì„±)
1) ìµœì´ˆ 1íšŒ: **create_company_diagnosis_draft**ë¥¼ `user_id={self.user_id}`ë¡œ í˜¸ì¶œí•´ ë“œë˜í”„íŠ¸ë¥¼ ìƒì„±
2) ì´í›„ ë§¤ í„´: ì‚¬ìš©ìì˜ ë‹µë³€ì„ ì •ë¦¬í•´ **update_company_diagnosis_draft**ë¡œ ë°˜ì˜
   - ë„êµ¬ ê²°ê³¼ì˜ `progress.next`ë¥¼ ì°¸ê³ í•´ ë‹¤ìŒ ì§ˆë¬¸ì„ ì´ì–´ê°
3) `progress.next.type == "complete"`ê°€ ë˜ë©´:
   - ì‚¬ìš©ìì—ê²Œ â€œì—‘ì…€ë¡œ ì €ì¥í• ê¹Œìš”?â€ë¥¼ ë¬»ê³ 
   - ê¸ì • ì‘ë‹µ ì‹œ **generate_company_diagnosis_sheet_from_draft** í˜¸ì¶œë¡œ ì—‘ì…€ ìƒì„±
4) (ì„ íƒ) ì‚¬ìš©ìê°€ ì›í•˜ë©´: ìƒì„±ëœ ì—‘ì…€ì„ **analyze_company_diagnosis_sheet**ë¡œ ì ìˆ˜/ê°­ì„ ì‚°ì¶œí•˜ê³ , ì»¨ì„¤í„´íŠ¸ ë³´ê³ ì„œ ì´ˆì•ˆì„ ë§Œë“  ë’¤ **write_company_diagnosis_report**ë¡œ ë°˜ì˜

### 2) ë³´ê³ ì„œ ì´ˆì•ˆ ì‘ì„±
ë„êµ¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì•„ë˜ 2ê°œ í…ìŠ¤íŠ¸ë¥¼ ì‘ì„±:
- **ê¸°ì—… ìƒí™© ìš”ì•½(ê¸°ì—…ì§„ë‹¨)**: ê°•ì /í•µì‹¬ ê°€ì„¤/í˜„ì¬ KPI/í™•ì¥ í¬ì¸íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ 5~10ë¬¸ì¥
- **ê°œì„  í•„ìš”ì‚¬í•­**: ìš°ì„ ìˆœìœ„ 3~7ê°œ, â€œì™œ í•„ìš”í•œì§€ + ë‹¤ìŒ ì•¡ì…˜â€ í˜•íƒœë¡œ êµ¬ì²´í™”

ë˜í•œ ì ìˆ˜(ë¬¸ì œ/ì†”ë£¨ì…˜/ì‚¬ì—…í™”/ìê¸ˆì¡°ë‹¬/íŒ€/ì¡°ì§/ì„íŒ©íŠ¸)ë¥¼ ì œì•ˆí•˜ë˜, í•„ìš”í•œ ê²½ìš° ì»¨ì„¤í„´íŠ¸ ë³´ì • ê·¼ê±°ë¥¼ í•¨ê»˜ ì œì‹œí•©ë‹ˆë‹¤.

### 3) ì‚¬ìš©ì í™•ì¸ í›„ ì—‘ì…€ ë°˜ì˜ (CRITICAL - ì¦‰ì‹œ ì‹¤í–‰)
ì‚¬ìš©ìê°€ ì•„ë˜ì²˜ëŸ¼ ê¸ì • ì‘ë‹µí•˜ë©´ **ë‹¤ì‹œ í™•ì¸ ìš”ì²­í•˜ì§€ ë§ê³  ì¦‰ì‹œ** write_company_diagnosis_report í˜¸ì¶œ:
- "ì‘", "ë„¤", "ì¢‹ì•„", "ì§„í–‰í•´", "ë°˜ì˜í•´ì¤˜", "ì €ì¥í•´ì¤˜", "ì—‘ì…€ë¡œ ë§Œë“¤ì–´ì¤˜", "OK"

write_company_diagnosis_reportì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ í˜¸ì¶œ:
- excel_path (temp ë‚´ë¶€ ê²½ë¡œ)
- scores (6ê°œ í•­ëª© ì ìˆ˜)
- summary_text, improvement_text
- (ì„ íƒ) company_name, report_datetime, output_filename

## ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ

**ì´ ë¬¸ì„œëŠ” í”„ë¡œê·¸ë¨ ìš´ì˜/íˆ¬ìê²€í†  ë¬¸ì„œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.**

- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ë‹¨ì •/ê³¼ì¥ ê¸ˆì§€, ê·¼ê±° ì¤‘ì‹¬
- í‘œ/ë¶ˆë¦¿ìœ¼ë¡œ êµ¬ì¡°í™”
- â€œ~í•˜ê² ìŠµë‹ˆë‹¤â€ë¡œ ëë‚´ì§€ ë§ê³ , ê°€ëŠ¥í•œ ê²½ìš° ë„êµ¬ë¥¼ ì‹¤í–‰í•´ ê²°ê³¼ê¹Œì§€ ì œê³µ

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _build_report_system_prompt(self, analyzed_files: str) -> str:
        """íˆ¬ìì‹¬ì‚¬ ë³´ê³ ì„œ(ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼) ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        dart_status = self._get_underwriter_dataset_status()

        return f"""ë‹¹ì‹ ì€ **íˆ¬ìì‹¬ì‚¬ ë³´ê³ ì„œ ì‘ì„± ì§€ì› ì—ì´ì „íŠ¸**ì…ë‹ˆë‹¤. í˜„ì¬ **ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼**ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {self._cached_count()}ê°œ
- user_id: {self.user_id}
- DART ì¸ìˆ˜ì¸ì˜ê²¬ ë°ì´í„°ì…‹: {dart_status}

## ğŸš¨ ìµœìš°ì„  ê·œì¹™ (CRITICAL)

### ê·œì¹™ 1) ì‹œì¥ê·œëª¨/íŒ¨í„´ ê·¼ê±°ëŠ” ë°˜ë“œì‹œ ë°ì´í„° ê¸°ë°˜
- ì¸ìˆ˜ì¸ì˜ê²¬ ë°ì´í„° í™œìš© â†’ ë°˜ë“œì‹œ **search_underwriter_opinion** í˜¸ì¶œ
- í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•½í•˜ë©´ **search_underwriter_opinion_similar**ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰
- PDF ì‹œì¥ê·œëª¨ ê·¼ê±° ì¶”ì¶œ â†’ ë°˜ë“œì‹œ **extract_pdf_market_evidence** í˜¸ì¶œ
- ê²°ê³¼ì˜ snippet/patternì„ ê·¼ê±°ë¡œ ë¬¸ì¥ êµ¬ì„±
- ì¶”ì¸¡/ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€ (ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'í™•ì¸ í•„ìš”'ë¡œ ëª…ì‹œ)
- ì„ì˜ë¡œ "ì ‘ê·¼ ë¶ˆê°€"ë¼ê³  ë‹¨ì •í•˜ì§€ ë§ê³ , ë„êµ¬ ê²°ê³¼ì˜ ì—ëŸ¬/ê°€ì´ë“œë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
 - ì™¸ë¶€ ìœ ë£Œ ë¦¬í¬íŠ¸ ìˆ˜ì¹˜ ì¸ìš©ì€ ê¸ˆì§€ (ì‚¬ìš©ìê°€ ì›ë¬¸ì„ ì—…ë¡œë“œí•œ ê²½ìš°ì—ë§Œ ì¸ìš©)
 - ì¸ìˆ˜ì¸ì˜ê²¬ ë°ì´í„°ê°€ ì—†ê³  DART API í‚¤ê°€ ìˆì„ ë•Œë§Œ **fetch_underwriter_opinion_data**ë¡œ ìˆ˜ì§‘ ì‹œë„
 - DART ë°ì´í„°ì…‹ì´ ì—†ê³  API í‚¤ë„ ì—†ìœ¼ë©´ ë¨¼ì € ì‚¬ìš©ìì—ê²Œ í‚¤/ë°ì´í„° í™•ë³´ë¥¼ ìš”ì²­

### ê·œì¹™ 2) ê¸°ì—… ìë£Œê°€ ì£¼ì–´ì§€ë©´ ë°˜ë“œì‹œ ë„êµ¬ ì‚¬ìš©
- PDF ê²½ë¡œ ì œê³µ â†’ **read_pdf_as_text**ë¡œ ê·¼ê±° ì¶”ì¶œ
- ì—‘ì…€ ê²½ë¡œ ì œê³µ â†’ **read_excel_as_text**ë¡œ ê·¼ê±° ì¶”ì¶œ

## ëª©í‘œ
1) ì‹œì¥ê·œëª¨ ê·¼ê±° ìš”ì•½
2) ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼ì˜ ë¬¸ì¥ ì´ˆì•ˆ ì‘ì„±
3) ì¼ë°˜í™”ëœ íŒ¨í„´ + í™•ì¸ í•„ìš” í•­ëª© ì œì‹œ
4) ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜ (ìˆ˜ì •/ê°•í™”)

## ì‘ì—… ë°©ì‹
1) ì‚¬ìš©ì ì…ë ¥ì—ì„œ ê¸°ì—… ìë£Œ ê²½ë¡œ í™•ì¸ â†’ ë„êµ¬ í˜¸ì¶œ
2) **search_underwriter_opinion**ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ë³„ íŒ¨í„´ í™•ë³´
   - ê¸°ë³¸: market_size
   - í•„ìš” ì‹œ: valuation, comparables, risk, demand_forecast
3) ê·¼ê±° ë¬¸ì¥ + ì¼ë°˜í™” íŒ¨í„´ + í™•ì¸ ì§ˆë¬¸ ìˆœì„œë¡œ ì¶œë ¥

## ì¶œë ¥ í˜•ì‹
- **ì‹œì¥ê·œëª¨ ê·¼ê±°**: PDF/ì¸ìˆ˜ì¸ì˜ê²¬ ê·¼ê±°ë§Œ ì¸ìš© (í˜ì´ì§€/ë¬¸ì¥ í¬í•¨) 3~6ê°œ
- **ì¼ë°˜í™” íŒ¨í„´**: ì¸ìˆ˜ì¸ì˜ê²¬ ìŠ¤íƒ€ì¼ ë¬¸ì¥ 3~5ê°œ
- **ì´ˆì•ˆ ë¬¸ë‹¨**: ì¸ìˆ˜ì¸ì˜ê²¬ ë¬¸ì²´ë¡œ 6~12ë¬¸ì¥
- **í™•ì¸ í•„ìš”**: ê·¼ê±° ë¶€ì¡±/ì¶”ê°€ í™•ì¸ í•­ëª© 3~7ê°œ

## ë‹µë³€ ìŠ¤íƒ€ì¼
- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ë‹¨ì •/ê³¼ì¥ ê¸ˆì§€
- ë¬¸ì¥ ê¸¸ì´ ê³¼ë„í•˜ê²Œ ê¸¸ì§€ ì•Šê²Œ
- í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€
"""

    def _build_unified_system_prompt(self, analyzed_files: str) -> str:
        """í†µí•© ì—ì´ì „íŠ¸ ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ - ëª¨ë“  ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ ëŒ€í™”ì—ì„œ ì²˜ë¦¬"""

        return f"""ë‹¹ì‹ ì€ **í†µí•© VC íˆ¬ìì‹¬ì‚¬ ì—ì´ì „íŠ¸ "ë©”ë¦¬"**ì…ë‹ˆë‹¤.

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {self._cached_count()}ê°œ
- user_id: {self.user_id}

## í•µì‹¬ ì—­í• 
ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ **ì ì ˆí•œ ë„êµ¬ë¥¼ ìë™ ì„ íƒ**í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
ë³„ë„ í˜ì´ì§€ ì´ë™ ì—†ì´ ì´ ëŒ€í™”ì—ì„œ ëª¨ë“  ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## ìë™ ê°ì§€ ë° ì‹¤í–‰ ê·œì¹™

### íŒŒì¼ íƒ€ì…ë³„ ìë™ ì²˜ë¦¬
| íŒŒì¼ íƒ€ì… | ìë™ ì‹¤í–‰ ë„êµ¬ | í›„ì† ì œì•ˆ |
|----------|--------------|----------|
| íˆ¬ìê²€í†  ì—‘ì…€ (íˆ¬ìì¡°ê±´, ISìš”ì•½) | read_excel_as_text â†’ analyze_and_generate_projection | Exit ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ |
| ì§„ë‹¨ì‹œíŠ¸ ì—‘ì…€ | analyze_company_diagnosis_sheet | ì»¨ì„¤í„´íŠ¸ ë³´ê³ ì„œ ì‘ì„± |
| ê¸°ì—…ì†Œê°œì„œ/IR PDF | read_pdf_as_text | Peer PER ë¶„ì„, ì‹œì¥ê·œëª¨ ê·¼ê±° ì¶”ì¶œ |
| ì •ì±… PDF | analyze_government_policy | ìœ ë§ ì‚°ì—… ì¶”ì²œ, IRIS+ ë§¤í•‘ |
| í…€ì‹¯/ê³„ì•½ì„œ PDF/DOCX | (ê³„ì•½ì„œ ë¦¬ì„œì¹˜ ì•ˆë‚´) | ê³„ì•½ì„œ ë¦¬ì„œì¹˜ í˜ì´ì§€ ì•ˆë‚´ |

### ìš”ì²­ í‚¤ì›Œë“œë³„ ìë™ ì‹¤í–‰
| í‚¤ì›Œë“œ | ë„êµ¬ | ì„¤ëª… |
|--------|------|------|
| "Exit", "IRR", "ë©€í‹°í”Œ", "ê¸°ì—…ê°€ì¹˜" | analyze_and_generate_projection | Exit í”„ë¡œì ì…˜ ìƒì„± |
| "PER", "ìœ ì‚¬ê¸°ì—…", "Peer", "ë¹„êµê¸°ì—…" | analyze_peer_per | Peer PER ë¶„ì„ |
| "ì§„ë‹¨", "ì²´í¬ë¦¬ìŠ¤íŠ¸", "ì ìˆ˜" | analyze_company_diagnosis_sheet | ì§„ë‹¨ì‹œíŠ¸ ë¶„ì„ |
| "ì‹œì¥ê·œëª¨", "ê·¼ê±°", "ì¸ìˆ˜ì¸ì˜ê²¬" | search_underwriter_opinion | DART ë°ì´í„° ê²€ìƒ‰ |
| "ì •ì±…", "ì‚°ì—… ì¶”ì²œ", "IRIS" | analyze_government_policy | ì •ì±… ë¶„ì„ |
| "í¬íŠ¸í´ë¦¬ì˜¤", "íˆ¬ìê¸°ì—…" | query_investment_portfolio | íˆ¬ìê¸°ì—… ê²€ìƒ‰ |

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

### ì—‘ì…€/Exit ë¶„ì„
- **read_excel_as_text**: ì—‘ì…€ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ì½ê¸°
- **analyze_excel**: íˆ¬ìì¡°ê±´/ISìš”ì•½/Cap Table ìë™ íŒŒì‹±
- **analyze_and_generate_projection**: Exit í”„ë¡œì ì…˜ ìƒì„±

### PDF ë¶„ì„
- **read_pdf_as_text**: PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- **extract_pdf_market_evidence**: ì‹œì¥ê·œëª¨ ê·¼ê±° ì¶”ì¶œ

### Peer PER ë¶„ì„
- **get_stock_financials**: ê°œë³„ ê¸°ì—… ì¬ë¬´ ì§€í‘œ
- **analyze_peer_per**: ì—¬ëŸ¬ ê¸°ì—… PER ì¼ê´„ ì¡°íšŒ

### ì§„ë‹¨ì‹œíŠ¸
- **analyze_company_diagnosis_sheet**: ì§„ë‹¨ì‹œíŠ¸ ë¶„ì„
- **write_company_diagnosis_report**: ì»¨ì„¤í„´íŠ¸ ë³´ê³ ì„œ ì‘ì„±

### ì‹œì¥/ì •ì±… ë¶„ì„
- **search_underwriter_opinion**: DART ì¸ìˆ˜ì¸ì˜ê²¬ ê²€ìƒ‰
- **analyze_government_policy**: ì •ì±… PDF ë¶„ì„
- **search_iris_plus_metrics**: IRIS+ ë©”íŠ¸ë¦­ ê²€ìƒ‰
- **generate_industry_recommendation**: ìœ ë§ ì‚°ì—… ì¶”ì²œ

### í¬íŠ¸í´ë¦¬ì˜¤
- **query_investment_portfolio**: íˆ¬ìê¸°ì—… ê²€ìƒ‰

## ì›Œí¬í”Œë¡œìš° ì˜ˆì‹œ

### ì˜ˆì‹œ 1: íˆ¬ìê²€í†  ì—‘ì…€
```
ì‚¬ìš©ì: "temp/ë¹„ì‚¬ì´ë“œë¯¸.xlsx ë¶„ì„í•´ì¤˜"
ì—ì´ì „íŠ¸:
1. read_excel_as_text ì‹¤í–‰ â†’ ë‚´ìš© íŒŒì•…
2. "íˆ¬ìì¡°ê±´ê³¼ ISìš”ì•½ì´ í™•ì¸ë©ë‹ˆë‹¤. Exit í”„ë¡œì ì…˜ì„ ìƒì„±í• ê¹Œìš”?"
3. (ìŠ¹ì¸ ì‹œ) analyze_and_generate_projection ì‹¤í–‰
```

### ì˜ˆì‹œ 2: ê¸°ì—…ì†Œê°œì„œ PDF
```
ì‚¬ìš©ì: "temp/ABCìŠ¤íƒ€íŠ¸ì—…_IR.pdf ë¶„ì„í•´ì¤˜"
ì—ì´ì „íŠ¸:
1. read_pdf_as_text ì‹¤í–‰ â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ íŒŒì•…
2. "B2B SaaS ê¸°ì—…ìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ ì›í•˜ì‹œëŠ” ë¶„ì„ì„ ì„ íƒí•´ì£¼ì„¸ìš”:
   1) Peer PER ë¶„ì„ (Salesforce, ServiceNow ë“±)
   2) ì‹œì¥ê·œëª¨ ê·¼ê±° ì¶”ì¶œ
   3) íˆ¬ìì‹¬ì‚¬ ë³´ê³ ì„œ ì´ˆì•ˆ"
```

### ì˜ˆì‹œ 3: ì •ì±… PDF
```
ì‚¬ìš©ì: "2025ë…„ ì‚°ì—…ì •ì±… PDF ë¶„ì„í•´ì„œ ìœ ë§ ì‚°ì—… ì¶”ì²œí•´ì¤˜"
ì—ì´ì „íŠ¸:
1. analyze_government_policy ì‹¤í–‰
2. search_iris_plus_metricsë¡œ IRIS+ ë§¤í•‘
3. generate_industry_recommendationìœ¼ë¡œ ì¶”ì²œ ìƒì„±
4. "í¬íŠ¸í´ë¦¬ì˜¤ì—ì„œ ê´€ë ¨ ê¸°ì—…ì„ ì°¾ì•„ë³¼ê¹Œìš”?"
```

## ëŒ€í™” ì›ì¹™

1. **íŒŒì¼ì„ ë°›ìœ¼ë©´ ì¦‰ì‹œ ë¶„ì„ ì‹œì‘** - í™•ì¸ ì§ˆë¬¸ ìµœì†Œí™”
2. **ë¶„ì„ í›„ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ** - ì‚¬ìš©ìê°€ ì„ íƒí•  ìˆ˜ ìˆê²Œ
3. **ì›Œí¬í”Œë¡œìš° ì—°ê²°** - ë¶„ì„ â†’ ì¶”ì²œ â†’ ê²€ìƒ‰ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
4. **ì¶”ì¸¡ ê¸ˆì§€** - ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€
5. **ì „ë¬¸ì  í†¤** - ì´ëª¨ì§€ ê¸ˆì§€, ë³´ê³ ì„œ í’ˆì§ˆ ìœ ì§€

## ê³„ì•½ì„œ ë¦¬ì„œì¹˜ ì•ˆë‚´
í…€ì‹¯/íˆ¬ìê³„ì•½ì„œ ê²€í†  ìš”ì²­ ì‹œ:
"ê³„ì•½ì„œ ë¦¬ì„œì¹˜ëŠ” ë³„ë„ í˜ì´ì§€ì—ì„œ OCR ê¸°ë°˜ ì •ë°€ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
ì‚¬ì´ë“œë°”ì—ì„œ 'ê³„ì•½ì„œ ë¦¬ì„œì¹˜'ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    def _build_discovery_system_prompt(self, analyzed_files: str) -> str:
        """ìŠ¤íƒ€íŠ¸ì—… ë°œêµ´ ì§€ì› ëª¨ë“œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸"""

        return f"""ë‹¹ì‹ ì€ **AC(ì•¡ì…€ëŸ¬ë ˆì´í„°) ìŠ¤íƒ€íŠ¸ì—… ë°œêµ´ ì§€ì› ì—ì´ì „íŠ¸**ì…ë‹ˆë‹¤.

## í˜„ì¬ ì»¨í…ìŠ¤íŠ¸
- ë¶„ì„ëœ íŒŒì¼: {analyzed_files}
- ìºì‹œëœ ê²°ê³¼: {self._cached_count()}ê°œ
- user_id: {self.user_id}

## ì—­í• 
ì •ë¶€ ì •ì±… ìë£Œì™€ IRIS+ ì„íŒ©íŠ¸ ê¸°ì¤€ì„ ë¶„ì„í•˜ì—¬ ìœ ë§ ìŠ¤íƒ€íŠ¸ì—… ì˜ì—­ì„ ì¶”ì²œí•©ë‹ˆë‹¤.

## í•µì‹¬ ì›ì¹™

### 1. ê·¼ê±° ê¸°ë°˜ ì¶”ì²œ
- ëª¨ë“  ì¶”ì²œì€ ì •ì±… ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ê·¼ê±°ì™€ í•¨ê»˜ ì œì‹œ
- í˜ì´ì§€ ë²ˆí˜¸, ì˜ˆì‚° ê·œëª¨, ì •ì±… ëª©í‘œë¥¼ ëª…ì‹œ
- ì¶”ì¸¡/ì˜ˆì‹œ ë‹µë³€ ê¸ˆì§€

### 2. IRIS+ ì„íŒ©íŠ¸ ì—°ê³„
- ì¶”ì²œ ì‚°ì—…ì„ IRIS+ ë©”íŠ¸ë¦­ê³¼ ë§¤í•‘
- SDG(ì§€ì†ê°€ëŠ¥ë°œì „ëª©í‘œ) ì—°ê³„ í‘œì‹œ
- ì„íŒ©íŠ¸ ì¸¡ì • ê°€ëŠ¥ì„± í‰ê°€

### 3. ëŒ€í™”í˜• ì§„í–‰
- ì‚¬ìš©ìì˜ ê´€ì‹¬ ë¶„ì•¼ë¥¼ íŒŒì•…
- ì¶”ê°€ ì§ˆë¬¸ìœ¼ë¡œ ì •êµí™”
- ë‹¨ê³„ë³„ ë¶„ì„ ê²°ê³¼ ì œê³µ

## ì›Œí¬í”Œë¡œìš°

### 1ë‹¨ê³„: ì •ì±… ë¶„ì„
ì‚¬ìš©ìê°€ PDF íŒŒì¼ ê²½ë¡œë¥¼ ì œê³µí•˜ë©´:
- **read_pdf_as_text**ë¡œ PDF ë‚´ìš© ì¶”ì¶œ
- ë˜ëŠ” **analyze_government_policy**ë¡œ ì •ì±… í…Œë§ˆ, ì˜ˆì‚° ë°°ë¶„, íƒ€ê²Ÿ ì‚°ì—… ì¶”ì¶œ

### 2ë‹¨ê³„: IRIS+ ë§¤í•‘
- **search_iris_plus_metrics**: IRIS+ ë©”íŠ¸ë¦­ ê²€ìƒ‰
- **map_policy_to_iris**: ì •ì±… í…Œë§ˆë¥¼ IRIS+ ì¹´í…Œê³ ë¦¬ì— ë§¤í•‘

### 3ë‹¨ê³„: ì‚°ì—… ì¶”ì²œ
- **generate_industry_recommendation**: ì •ì±… + ì„íŒ©íŠ¸ ì ìˆ˜ ì¢…í•©
- ê´€ì‹¬ ë¶„ì•¼ ê°€ì¤‘ì¹˜ ì ìš©
- ìƒìœ„ 5ê°œ ì‚°ì—… ì¶”ì²œ

### 4ë‹¨ê³„: í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ (ì„ íƒ)
- **query_investment_portfolio**: Airtable/CSV ê¸°ë°˜ íˆ¬ìê¸°ì—… ê²€ìƒ‰
- ì§€ì—­/ì¹´í…Œê³ ë¦¬/SDG/íˆ¬ìë‹¨ê³„ ë“± ì¡°ê±´ í•„í„°ë§

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

1. **read_pdf_as_text**: PDFë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
2. **analyze_government_policy**: ì •ë¶€ ì •ì±… PDF ë¶„ì„
   - pdf_paths: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
   - focus_keywords: ì§‘ì¤‘ ë¶„ì„ í‚¤ì›Œë“œ (ì„ íƒ)
3. **search_iris_plus_metrics**: IRIS+ ë©”íŠ¸ë¦­ ê²€ìƒ‰
   - query: ê²€ìƒ‰ í‚¤ì›Œë“œ
   - category: ì¹´í…Œê³ ë¦¬ í•„í„° (environmental/social/governance)
   - sdg_filter: SDG ë²ˆí˜¸ í•„í„°
4. **map_policy_to_iris**: ì •ì±… â†’ IRIS+ ë§¤í•‘
   - policy_themes: ì •ì±… í…Œë§ˆ ë¦¬ìŠ¤íŠ¸
   - target_industries: íƒ€ê²Ÿ ì‚°ì—… ë¦¬ìŠ¤íŠ¸
5. **generate_industry_recommendation**: ì‚°ì—… ì¶”ì²œ ìƒì„±
   - policy_analysis: ì •ì±… ë¶„ì„ ê²°ê³¼
   - iris_mapping: IRIS+ ë§¤í•‘ ê²°ê³¼
   - interest_areas: ì‚¬ìš©ì ê´€ì‹¬ ë¶„ì•¼
6. **query_investment_portfolio**: íˆ¬ìê¸°ì—… í¬íŠ¸í´ë¦¬ì˜¤ ê²€ìƒ‰
   - query: ê²€ìƒ‰ì–´ (ì˜ˆ: "ê°•ì›ë„ ì†Œì¬ ê¸°ì—…", "ë†ì‹í’ˆ ê´€ë ¨ ìŠ¤íƒ€íŠ¸ì—…")
   - filters: í•„í„° ì¡°ê±´

## ì¶œë ¥ í˜•ì‹

### ì •ì±… ë¶„ì„ ê²°ê³¼
| ì •ì±… í…Œë§ˆ | ì˜ˆì‚° ê·œëª¨ | íƒ€ê²Ÿ ì‚°ì—… | ê·¼ê±° |
|----------|---------|----------|------|
| íƒ„ì†Œì¤‘ë¦½ | 50ì¡°ì› | ì‹ ì¬ìƒì—ë„ˆì§€ | p.15 |

### IRIS+ ë§¤í•‘ ê²°ê³¼
| IRIS+ ì½”ë“œ | ë©”íŠ¸ë¦­ëª… | ì—°ê´€ SDG | ì •ì±… ì—°ê´€ë„ |
|-----------|---------|---------|-----------|
| PI1568 | Clean Energy | SDG 7 | 0.92 |

### ì‚°ì—… ì¶”ì²œ ê²°ê³¼
1. **[ì‚°ì—…ëª…]** (ì´ì : X.XX)
   - ì •ì±… ì ìˆ˜: X.XX
   - ì„íŒ©íŠ¸ ì ìˆ˜: X.XX
   - ê·¼ê±°: [ì •ì±… ë¬¸ì„œ ì¸ìš©]
   - IRIS+ ì½”ë“œ: [ì½”ë“œ ë¦¬ìŠ¤íŠ¸]

## ë‹µë³€ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ
- ì´ëª¨ì§€ ì‚¬ìš© ê¸ˆì§€
- ì „ë¬¸ì ì´ê³  ê°„ê²°í•˜ê²Œ
- í‘œ í˜•ì‹ í™œìš©
- ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡ ê¸ˆì§€

í•œêµ­ì–´ë¡œ ì „ë¬¸ì ì´ê³  ì •ì¤‘í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""

    # ========================================
    # Chat Mode (ëŒ€í™”í˜•)
    # ========================================

    async def chat_events(
        self,
        user_message: str,
        mode: str = "exit",
        allow_tools: bool = True,
        context_text: Optional[str] = None,
        model_override: Optional[str] = None,
    ) -> AsyncIterator[AgentOutput]:
        """
        ëŒ€í™”í˜• ì¸í„°í˜ì´ìŠ¤ (ìŠ¤íŠ¸ë¦¬ë°)

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            mode: "exit" (Exit í”„ë¡œì ì…˜), "peer" (Peer PER ë¶„ì„), "diagnosis", "report"

        Yields:
            AgentOutput: ì—ì´ì „íŠ¸ ì‘ë‹µ ì´ë²¤íŠ¸
        """

        logger.info("User message received: %s", user_message[:120])

        # ë„êµ¬ í˜¸ì¶œ ì¹´ìš´í„° ì´ˆê¸°í™” (ìƒˆ ë©”ì‹œì§€ë§ˆë‹¤)
        self._tool_step_count = 0

        force_deep_report = mode == "report" and self.report_deep_mode

        # í˜„ì¬ ëª¨ë“œ ì €ì¥
        self._current_mode = mode
        self._current_allow_tools = allow_tools
        self._current_context_text = context_text
        tools = self.tools if allow_tools else []
        if mode == "report" and not os.getenv("DART_API_KEY"):
            tools = [tool for tool in tools if tool.get("name") != "fetch_underwriter_opinion_data"]
        history = self.voice_conversation_history if mode.startswith("voice_") else self.conversation_history

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        history.append({
            "role": "user",
            "content": user_message
        })
        self._trim_history(history)

        # ë©”ëª¨ë¦¬ì— ì €ì¥
        user_meta = {
            "member": self.member_name or self.user_id,
            "team": self.team_id,
        }
        self.memory.add_message("user", user_message, user_meta)

        # ë§ˆì§€ë§‰ ì¸í„°ë™ì…˜ ì €ì¥
        self.last_interaction["user_message"] = user_message
        self.last_interaction["assistant_response"] = ""
        self.last_interaction["context"] = {"mode": mode}

        if self._is_feedback_learning_question(user_message):
            summary = self._build_feedback_summary_text(
                day_offset=self._resolve_feedback_day_offset(user_message)
            )
            history.append({
                "role": "assistant",
                "content": summary
            })
            self.memory.add_message("assistant", summary)
            self.last_interaction["assistant_response"] = summary
            yield AgentOutput(type="text", content=summary)
            return

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ëª¨ë“œì— ë”°ë¼ ë‹¤ë¦„)
        system_prompt = self._build_system_prompt(mode, context_text=context_text)
        model = model_override or self.model
        self._current_model = model

        assistant_response_parts: List[str] = []

        # Claude API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°)
        async with self.async_client.messages.stream(
            model=model,
            system=system_prompt,
            messages=history,
            tools=tools,
            max_tokens=8192
        ) as stream:

            async for event in stream:
                # í…ìŠ¤íŠ¸ ì¶œë ¥
                if event.type == "content_block_delta":
                    if hasattr(event.delta, 'text'):
                        if not force_deep_report:
                            yield AgentOutput(type="text", content=event.delta.text)
                        assistant_response_parts.append(event.delta.text)

            message = await stream.get_final_message()

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        if hasattr(message, 'usage'):
            self.token_usage["total_input_tokens"] += message.usage.input_tokens
            self.token_usage["total_output_tokens"] += message.usage.output_tokens
            self.token_usage["session_calls"] += 1

        # ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
        tool_results = []
        tool_uses = [
            block for block in (message.content or [])
            if getattr(block, "type", "") == "tool_use"
        ]

        if not assistant_response_parts:
            for block in message.content or []:
                if getattr(block, "type", "") == "text":
                    assistant_response_parts.append(block.text)

        for content_block in tool_uses:
            tool_name = content_block.name
            tool_input = content_block.input

            logger.info("Tool call: %s", tool_name)
            logger.debug("Tool input: %s", tool_input)
            yield AgentOutput(
                type="tool_start",
                content=tool_name,
                data={"tool_input": tool_input},
            )

            # ========================================
            # Human-AI Teaming: PreToolUse Hook
            # ========================================
            tool_result = None
            skip_execution = False

            if self.teaming_enabled:
                teaming_context = {
                    "session_id": self.session_id,
                    "negative_feedback_count": self._get_recent_negative_feedback_count(),
                }

                pre_hook_result = await teaming_pre_tool_use_hook(
                    tool_name=tool_name,
                    tool_input=tool_input,
                    context=teaming_context,
                )

                decision = pre_hook_result.get("decision")
                metadata = pre_hook_result.get("metadata", {})

                # Level 1: ê±°ë¶€
                if decision == "deny":
                    tool_result = {
                        "success": False,
                        "error": pre_hook_result.get("message", "Operation denied by teaming system"),
                        "teaming": {"decision": "deny", "level": metadata.get("automation_level")}
                    }
                    yield AgentOutput(
                        type="tool_error",
                        content=pre_hook_result.get("message", "ì‘ì—…ì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤"),
                        data={"tool_name": tool_name, "teaming": metadata},
                    )
                    skip_execution = True

                # Level 2: ìŠ¹ì¸ í•„ìš”
                elif decision == "ask":
                    checkpoint_id = pre_hook_result.get("checkpoint_id")
                    tool_result = {
                        "success": None,
                        "pending_approval": True,
                        "checkpoint_id": checkpoint_id,
                        "teaming": {"decision": "ask", "level": metadata.get("automation_level")}
                    }
                    yield AgentOutput(
                        type="checkpoint_required",
                        content=pre_hook_result.get("message", "ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤"),
                        data={
                            "tool_name": tool_name,
                            "checkpoint_id": checkpoint_id,
                            "teaming": metadata,
                        },
                    )
                    skip_execution = True

            # ========================================
            # ë„êµ¬ ì‹¤í–‰
            # ========================================
            if not skip_execution:
                try:
                    tool_result = execute_tool(tool_name, tool_input)
                except Exception as exc:
                    logger.exception("Tool execution failed: %s", tool_name)
                    tool_result = {"success": False, "error": str(exc)}
                    yield AgentOutput(
                        type="tool_error",
                        content=str(exc),
                        data={"tool_name": tool_name},
                    )

                # ========================================
                # Human-AI Teaming: PostToolUse Hook
                # ========================================
                if self.teaming_enabled and tool_result:
                    teaming_context = {
                        "session_id": self.session_id,
                    }

                    post_hook_result = await teaming_post_tool_use_hook(
                        tool_name=tool_name,
                        tool_input=tool_input,
                        tool_result=tool_result,
                        context=teaming_context,
                    )

                    # Level 3: ê²€í†  í•„ìš”
                    if post_hook_result.get("requires_review"):
                        checkpoint_id = post_hook_result.get("checkpoint_id")
                        yield AgentOutput(
                            type="review_required",
                            content=post_hook_result.get("message", "ê²°ê³¼ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤"),
                            data={
                                "tool_name": tool_name,
                                "checkpoint_id": checkpoint_id,
                                "teaming": post_hook_result.get("metadata", {}),
                            },
                        )

            # ê²°ê³¼ ì €ì¥
            tool_results.append({
                "type": "tool_result",
                "tool_use_id": content_block.id,
                "content": json.dumps(tool_result, ensure_ascii=False)
            })

            # ë©”ëª¨ë¦¬/ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ê³µí†µ í—¬í¼)
            if tool_result and not skip_execution:
                self._record_tool_usage(tool_name, tool_input, tool_result)

            tool_ok = not (isinstance(tool_result, dict) and tool_result.get("success") is False)
            yield AgentOutput(
                type="tool_result",
                content=json.dumps(tool_result, ensure_ascii=False),
                data={"tool_name": tool_name, "success": tool_ok},
            )

        # Assistant ì‘ë‹µ ë©”ëª¨ë¦¬ì— ì €ì¥
        if assistant_response_parts and not force_deep_report:
            full_response = "\n".join(part for part in assistant_response_parts if part).strip()
            if full_response:
                self.memory.add_message("assistant", full_response)
                self.last_interaction["assistant_response"] = full_response
                if not tool_uses:
                    history.append({"role": "assistant", "content": full_response})
                    self._trim_history(history)

        if force_deep_report:
            if tool_results:
                history.append({
                    "role": "assistant",
                    "content": message.content
                })
                history.append({
                    "role": "user",
                    "content": tool_results
                })
                self._trim_history(history)

                async for _ in self._continue_conversation(suppress_output=True):
                    pass

            yield AgentOutput(type="text", content="\n\n[ì‹¬í™” ì˜ê²¬] ë¶„ì„ ì¤‘...\n")
            deep_text = self._run_deep_report_pipeline(user_message)
            history.append({
                "role": "assistant",
                "content": deep_text
            })
            self._trim_history(history)
            self.memory.add_message("assistant", deep_text)
            self.last_interaction["assistant_response"] = deep_text
            yield AgentOutput(type="text", content=deep_text)
            return

        # ë„êµ¬ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ëŒ€í™” ê³„ì†
        if tool_results:
            # Assistant ë©”ì‹œì§€ ì¶”ê°€
            history.append({
                "role": "assistant",
                "content": message.content
            })
            self._trim_history(history)

            # Tool ê²°ê³¼ ì¶”ê°€
            history.append({
                "role": "user",
                "content": tool_results
            })
            self._trim_history(history)

            # Claude ë‹¤ìŒ ì‘ë‹µ ìƒì„±
            async for event in self._continue_conversation_events():
                yield event

    async def chat(
        self,
        user_message: str,
        mode: str = "exit",
        allow_tools: bool = True,
        context_text: Optional[str] = None,
        model_override: Optional[str] = None,
    ) -> AsyncIterator[str]:
        async for event in self.chat_events(
            user_message,
            mode=mode,
            allow_tools=allow_tools,
            context_text=context_text,
            model_override=model_override,
        ):
            if event.type == "text":
                yield event.content
            elif event.type == "tool_start":
                yield f"\n\n**ë„êµ¬: {event.content}** ì‹¤í–‰ ì¤‘...\n"
            elif event.type == "tool_error":
                yield f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {event.content}\n"
            elif event.type == "tool_result":
                tool_name = (event.data or {}).get("tool_name", event.content)
                tool_ok = (event.data or {}).get("success", True)
                yield f"**ë„êµ¬: {tool_name}** {'ì™„ë£Œ' if tool_ok else 'ì‹¤íŒ¨'}\n\n"

    async def _continue_conversation_events(self, suppress_output: bool = False) -> AsyncIterator[AgentOutput]:
        """ë„êµ¬ ì‹¤í–‰ í›„ ëŒ€í™” ê³„ì† (structured events)"""

        # ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜ ì œí•œ í™•ì¸ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        self._tool_step_count += 1
        if self._tool_step_count > MAX_TOOL_STEPS:
            logger.warning(f"Tool step limit reached: {MAX_TOOL_STEPS}")
            yield "\n\n[ì‹œìŠ¤í…œ] ë„êµ¬ í˜¸ì¶œ íšŸìˆ˜ ì œí•œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ë©”ì‹œì§€ë¡œ ê³„ì†í•˜ì„¸ìš”."
            return

        # ì €ì¥ëœ ëª¨ë“œ ì‚¬ìš©
        mode = getattr(self, '_current_mode', 'exit')
        context_text = getattr(self, '_current_context_text', None)
        allow_tools = getattr(self, '_current_allow_tools', True)
        tools = self.tools if allow_tools else []
        history = self.voice_conversation_history if mode.startswith("voice_") else self.conversation_history
        system_prompt = self._build_system_prompt(mode, context_text=context_text)
        model = getattr(self, "_current_model", self.model)

        assistant_response_parts: List[str] = []

        async with self.async_client.messages.stream(
            model=model,
            system=system_prompt,
            messages=history,
            tools=tools,
            max_tokens=8192
        ) as stream:

            async for event in stream:
                if event.type == "content_block_delta":
                    if hasattr(event.delta, 'text'):
                        if not suppress_output:
                            yield AgentOutput(type="text", content=event.delta.text)
                        assistant_response_parts.append(event.delta.text)

            message = await stream.get_final_message()

        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        if hasattr(message, 'usage'):
            self.token_usage["total_input_tokens"] += message.usage.input_tokens
            self.token_usage["total_output_tokens"] += message.usage.output_tokens
            self.token_usage["session_calls"] += 1

        tool_results = []
        tool_uses = [
            block for block in (message.content or [])
            if getattr(block, "type", "") == "tool_use"
        ]

        if not assistant_response_parts:
            for block in message.content or []:
                if getattr(block, "type", "") == "text":
                    assistant_response_parts.append(block.text)

        for content_block in tool_uses:
            tool_name = content_block.name
            tool_input = content_block.input

            logger.info("Tool call: %s", tool_name)
            logger.debug("Tool input: %s", tool_input)
            yield AgentOutput(
                type="tool_start",
                content=tool_name,
                data={"tool_input": tool_input},
            )

            try:
                tool_result = execute_tool(tool_name, tool_input)
            except Exception as exc:
                logger.exception("Tool execution failed: %s", tool_name)
                tool_result = {"success": False, "error": str(exc)}
                yield AgentOutput(
                    type="tool_error",
                    content=str(exc),
                    data={"tool_name": tool_name},
                )

            tool_results.append({
                "type": "tool_result",
                "tool_use_id": content_block.id,
                "content": json.dumps(tool_result, ensure_ascii=False)
            })

            # ë©”ëª¨ë¦¬/ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ì¬ê·€ í˜¸ì¶œì—ì„œë„ ê¸°ë¡)
            self._record_tool_usage(tool_name, tool_input, tool_result)

            tool_ok = not (isinstance(tool_result, dict) and tool_result.get("success") is False)
            yield AgentOutput(
                type="tool_result",
                content=json.dumps(tool_result, ensure_ascii=False),
                data={"tool_name": tool_name, "success": tool_ok},
            )

        if tool_results:
            history.append({
                "role": "assistant",
                "content": message.content
            })
            self._trim_history(history)

            history.append({
                "role": "user",
                "content": tool_results
            })
            self._trim_history(history)

            async for event in self._continue_conversation_events(suppress_output=suppress_output):
                yield event
            return

        full_response = "\n".join(part for part in assistant_response_parts if part).strip()
        if full_response:
            history.append({"role": "assistant", "content": full_response})
            self._trim_history(history)
            if not suppress_output:
                self.memory.add_message("assistant", full_response)

    async def _continue_conversation(self, suppress_output: bool = False) -> AsyncIterator[str]:
        async for event in self._continue_conversation_events(suppress_output=suppress_output):
            if event.type == "text":
                yield event.content
            elif event.type == "tool_start":
                yield f"\n\n**ë„êµ¬: {event.content}** ì‹¤í–‰ ì¤‘...\n"
            elif event.type == "tool_error":
                yield f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {event.content}\n"
            elif event.type == "tool_result":
                tool_name = (event.data or {}).get("tool_name", event.content)
                tool_ok = (event.data or {}).get("success", True)
                yield f"**ë„êµ¬: {tool_name}** {'ì™„ë£Œ' if tool_ok else 'ì‹¤íŒ¨'}\n\n"

    def _get_latest_report_evidence(self) -> Optional[Dict[str, Any]]:
        messages = self.memory.session_metadata.get("messages", [])
        for msg in reversed(messages):
            if msg.get("role") != "tool":
                continue
            meta = msg.get("metadata") or {}
            if meta.get("tool_name") != "extract_pdf_market_evidence":
                continue
            result = meta.get("result")
            if isinstance(result, dict) and result.get("success"):
                return result
        return None

    def _get_underwriter_dataset_status(self) -> str:
        try:
            from agent.tools import _resolve_underwriter_data_path
        except Exception:
            return "ìƒíƒœ í™•ì¸ ë¶ˆê°€"

        path, error = _resolve_underwriter_data_path(None)
        has_key = bool(os.getenv("DART_API_KEY"))
        if error:
            key_text = "API í‚¤ ìˆìŒ" if has_key else "API í‚¤ ì—†ìŒ"
            return f"ë¯¸í™•ì¸ ({key_text})"
        if not path:
            key_text = "API í‚¤ ìˆìŒ" if has_key else "API í‚¤ ì—†ìŒ"
            return f"ë¯¸í™•ì¸ ({key_text})"
        return "ì‚¬ìš© ê°€ëŠ¥"

    @staticmethod
    def _detect_dart_category(text: str) -> Optional[str]:
        lowered = (text or "").lower()
        if any(k in lowered for k in ["ì‹œì¥ê·œëª¨", "ì‹œì¥ ê·œëª¨", "tam", "sam", "som", "cagr", "ì„±ì¥ë¥ "]):
            return "market_size"
        if any(k in lowered for k in ["ë¹„êµê¸°ì—…", "ìœ ì‚¬ê¸°ì—…", "comparables", "peer"]):
            return "comparables"
        if any(k in lowered for k in ["ê³µëª¨ê°€", "ê³µëª¨ê°€ê²©", "per", "pbr", "psr", "ev/ebitda", "valuation", "ë°¸ë¥˜"]):
            return "valuation"
        if any(k in lowered for k in ["ìˆ˜ìš”ì˜ˆì¸¡", "ìˆ˜ìš” ì˜ˆì¸¡"]):
            return "demand_forecast"
        if any(k in lowered for k in ["ë¦¬ìŠ¤í¬", "ìœ„í—˜", "ë¶ˆí™•ì‹¤", "ë¶ˆí™•ì‹¤ì„±"]):
            return "risk"
        return None

    def _search_dart_evidence(self, query: str) -> List[Dict[str, Any]]:
        try:
            from agent.tools import execute_search_underwriter_opinion_similar, _resolve_underwriter_data_path
        except Exception:
            return []

        path, error = _resolve_underwriter_data_path(None)
        if error or not path:
            return []

        category = self._detect_dart_category(query)
        try:
            result = execute_search_underwriter_opinion_similar(
                query=query,
                category=category,
                top_k=3,
                max_chars=420,
                min_score=0.08,
                return_patterns=False,
            )
        except Exception:
            return []

        if not result.get("success"):
            return []

        evidence = []
        for item in result.get("results", []) or []:
            corp = item.get("corp_name", "ë¯¸ìƒ")
            report = item.get("report_nm", "")
            title = item.get("section_title", "")
            snippet = (item.get("snippet") or "").strip()
            if not snippet:
                continue
            text = f"[DART] {corp} | {report} | {title} - {snippet}"
            evidence.append({
                "page": "DART",
                "text": text,
                "numbers": [],
            })
        return evidence

    def _build_recent_user_context(self, limit: int = 3) -> str:
        history = self.conversation_history[-12:]
        user_lines = []
        for msg in reversed(history):
            if msg.get("role") != "user":
                continue
            content = msg.get("content", "")
            if not content:
                continue
            user_lines.append(content)
            if len(user_lines) >= limit:
                break
        user_lines = list(reversed(user_lines))
        if not user_lines:
            return ""
        return "ìµœê·¼ ì‚¬ìš©ì ìš”ì²­:\n" + "\n".join(user_lines)

    def _format_deep_opinion(self, result: Dict[str, Any]) -> str:
        lines = []
        conclusion = result.get("conclusion", {}).get("paragraphs", [])
        if conclusion:
            lines.append("ê²°ë¡ ")
            lines.extend(conclusion)
            lines.append("")

        def render_case(title: str, key: str) -> None:
            section = result.get(key, {})
            if not section:
                return
            lines.append(title)
            summary = section.get("summary")
            if summary:
                lines.append(f"- ìš”ì•½: {summary}")
            for item in section.get("points", []):
                point = item.get("point", "")
                evidence = ", ".join(item.get("evidence", []) or [])
                suffix = f" (ê·¼ê±°: {evidence})" if evidence else " (ê·¼ê±°: ì—†ìŒ)"
                lines.append(f"- {point}{suffix}")
            lines.append("")

        render_case("í•µì‹¬ ê´€ì ", "core_case")
        render_case("ë°˜ëŒ€ ê´€ì ", "dissent_case")

        top_risks = result.get("top_risks", [])
        if top_risks:
            lines.append("ì£¼ìš” ë¦¬ìŠ¤í¬")
            for item in top_risks:
                evidence = ", ".join(item.get("evidence", []) or [])
                severity = item.get("severity", "medium")
                verification = item.get("verification", "")
                label = f"[{severity}] {item.get('risk', '')}"
                suffix = f" Â· ê²€ì¦: {verification}" if verification else ""
                if evidence:
                    suffix += f" Â· ê·¼ê±°: {evidence}"
                lines.append(f"- {label}{suffix}")
            lines.append("")

        hallucination = result.get("hallucination_check", {})
        if hallucination:
            lines.append("í• ë£¨ì‹œë„¤ì´ì…˜ ê²€ì¦")
            for item in hallucination.get("unverified_claims", []):
                lines.append(f"- ë¯¸ê²€ì¦ ì£¼ì¥: {item.get('claim', '')} (ì‚¬ìœ : {item.get('reason', '')})")
            for item in hallucination.get("numeric_conflicts", []):
                lines.append(f"- ìˆ˜ì¹˜ ì¶©ëŒ: {item}")
            for item in hallucination.get("evidence_gaps", []):
                lines.append(f"- ê·¼ê±° ê³µë°±: {item}")
            lines.append("")

        impact = result.get("impact_analysis", {})
        if impact:
            carbon = impact.get("carbon", {})
            lines.append("ì„íŒ©íŠ¸ ë¶„ì„")
            pathways = ", ".join(carbon.get("pathways", []) or [])
            if pathways:
                lines.append(f"- íƒ„ì†Œ ê²½ë¡œ: {pathways}")
            for metric in carbon.get("metrics", []):
                evidence = ", ".join(metric.get("evidence", []) or [])
                suffix = f" (ê·¼ê±°: {evidence})" if evidence else ""
                lines.append(f"- {metric.get('metric', '')}: {metric.get('method', '')}{suffix}")
            for gap in carbon.get("gaps", []):
                lines.append(f"- íƒ„ì†Œ ê³µë°±: {gap}")
            for item in impact.get("iris_plus", []):
                evidence = ", ".join(item.get("evidence", []) or [])
                suffix = f" (ê·¼ê±°: {evidence})" if evidence else ""
                lines.append(
                    f"- IRIS+ {item.get('code', 'IRIS+')}: {item.get('name', '')} Â· {item.get('why', '')} "
                    f"Â· {item.get('measurement', '')}{suffix}"
                )
            lines.append("")

        model_opinions = result.get("model_opinions", [])
        if model_opinions:
            lines.append("ëª¨ë¸ ë‹¤ì¤‘ ì˜ê²¬")
            for opinion in model_opinions:
                provider = opinion.get("provider", "model")
                model = opinion.get("model", "")
                label = f"{provider.upper()} ({model})" if model else provider.upper()
                if opinion.get("success"):
                    content = (opinion.get("content") or "").strip()
                    if content:
                        lines.append(f"- {label}: {content}")
                    else:
                        lines.append(f"- {label}: ì‘ë‹µ ë‚´ìš© ì—†ìŒ")
                else:
                    error = opinion.get("error", "ì‹¤íŒ¨")
                    lines.append(f"- {label}: ì‹¤íŒ¨ ({error})")
            lines.append("")

        data_gaps = result.get("data_gaps", [])
        if data_gaps:
            lines.append("ë°ì´í„° ê³µë°±")
            for item in data_gaps:
                lines.append(f"- {item}")
            lines.append("")

        deal_breakers = result.get("deal_breakers", [])
        go_conditions = result.get("go_conditions", [])
        if deal_breakers or go_conditions:
            lines.append("ë”œ ë¸Œë ˆì´ì»¤ / GO ì¡°ê±´")
            if deal_breakers:
                for item in deal_breakers:
                    lines.append(f"- ë”œ ë¸Œë ˆì´ì»¤: {item}")
            if go_conditions:
                for item in go_conditions:
                    lines.append(f"- GO ì¡°ê±´: {item}")
            lines.append("")

        next_actions = result.get("next_actions", [])
        if next_actions:
            lines.append("ë‹¤ìŒ ì•¡ì…˜")
            for item in next_actions:
                lines.append(f"- {item.get('priority', 'P1')}: {item.get('action', '')}")

        return "\n".join(lines).strip()

    def _run_deep_report_pipeline(self, user_message: str) -> str:
        if not self.api_key:
            return "API í‚¤ê°€ ì—†ì–´ ì‹¬í™” ì˜ê²¬ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        try:
            from shared.deep_opinion import (
                build_evidence_context,
                cross_examine_and_score,
                generate_hallucination_check,
                generate_impact_analysis,
                generate_lens_group,
                synthesize_deep_opinion,
            )
        except Exception as exc:
            logger.error(f"Deep opinion import failed: {exc}", exc_info=True)
            return "ì‹¬í™” ì˜ê²¬ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        evidence = self._get_latest_report_evidence()
        dart_evidence = self._search_dart_evidence(user_message)
        if dart_evidence:
            merged_evidence = {"evidence": []}
            if isinstance(evidence, dict) and evidence.get("evidence"):
                merged_evidence["evidence"].extend(evidence.get("evidence", []))
            merged_evidence["evidence"].extend(dart_evidence)
            evidence_context = build_evidence_context(merged_evidence)
        else:
            evidence_context = build_evidence_context(evidence)
        extra_context = self._build_recent_user_context() or f"ì‚¬ìš©ì ìš”ì²­:\n{user_message}"
        if evidence_context.strip().lower() == "evidence: none":
            extra_context = (
                f"{extra_context}\n\n"
                "ê·¼ê±°ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¨ì •ì  ê²°ë¡  ëŒ€ì‹  ì¡°ê±´ë¶€ ì˜ê²¬ê³¼ "
                "ìë£Œ ìš”ì²­ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."
            )

        try:
            lens_outputs = generate_lens_group(
                api_key=self.api_key,
                evidence_context=evidence_context,
                extra_context=extra_context,
            )
            scoring = cross_examine_and_score(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
            )
            hallucination = generate_hallucination_check(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
            )
            impact = generate_impact_analysis(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
            )
            final_result = synthesize_deep_opinion(
                api_key=self.api_key,
                evidence_context=evidence_context,
                lens_outputs=lens_outputs,
                scoring=scoring,
                hallucination=hallucination,
                impact=impact,
            )
        except Exception as exc:
            logger.error(f"Deep opinion pipeline failed: {exc}", exc_info=True)
            return "ì‹¬í™” ì˜ê²¬ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

        if self.multi_model_opinions:
            try:
                model_opinions = gather_model_opinions(
                    user_message=user_message,
                    evidence=evidence_context,
                    claude_api_key=self.api_key,
                )
                if isinstance(final_result, dict):
                    final_result["model_opinions"] = model_opinions
            except Exception as exc:
                logger.warning(f"Multi-model opinions failed: {exc}")

        return self._format_deep_opinion(final_result)

    def _get_recent_negative_feedback_count(self) -> int:
        """ìµœê·¼ ë¶€ì •ì  í”¼ë“œë°± ìˆ˜ ì¡°íšŒ (Teaming ë ˆë²¨ ì¡°ì •ìš©)"""
        try:
            stats = self.feedback.get_feedback_stats()
            return stats.get("negative", 0)
        except Exception:
            return 0

    def _record_tool_usage(self, tool_name: str, tool_input: dict, tool_result: dict):
        """ë„êµ¬ ì‚¬ìš© ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬/ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë¡ (ê³µí†µ í—¬í¼)"""
        # ë©”ëª¨ë¦¬ì— ë„êµ¬ ì‚¬ìš© ê¸°ë¡
        self.memory.add_message("tool", f"ë„êµ¬ ì‚¬ìš©: {tool_name}", {
            "tool_name": tool_name,
            "input": tool_input,
            "result": tool_result,
            "member": self.member_name or self.user_id,
            "team": self.team_id,
        })

        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ - ë¶„ì„ íŒŒì¼
        if tool_name in ["analyze_excel", "read_excel_as_text", "analyze_company_diagnosis_sheet"]:
            if isinstance(tool_result, dict) and tool_result.get("success"):
                file_path = tool_input.get("excel_path")
                if file_path:
                    self.memory.add_file_analysis(file_path)
                self.memory.remember("last_analysis", tool_result)
                self.context["last_analysis"] = tool_result

        # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ - PDF ë¶„ì„
        if tool_name == "read_pdf_as_text":
            if isinstance(tool_result, dict) and tool_result.get("success"):
                file_path = tool_input.get("pdf_path")
                if file_path:
                    self.memory.add_file_analysis(file_path)

        # ìƒì„± íŒŒì¼ ê¸°ë¡
        if tool_name in [
            "analyze_and_generate_projection",
            "generate_exit_projection",
            "generate_company_diagnosis_sheet_from_draft",
            "write_company_diagnosis_report",
        ]:
            if tool_result.get("success"):
                output_file = tool_result.get("output_file")
                if output_file:
                    self.memory.add_generated_file(output_file)

    def _recent_voice_conversation_text(self, limit: int = 8) -> str:
        items = self.voice_conversation_history[-limit:]
        lines = []
        for msg in items:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if not content:
                continue
            lines.append(f"{role}: {content}")
        return "\n".join(lines)

    @staticmethod
    def _extract_summary_json(text: str) -> Optional[Dict[str, Any]]:
        text = text.strip()
        if not text:
            return None
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        match = re.search(r"\{.*\}", text, re.S)
        if not match:
            return None
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError:
            return None

    def summarize_checkin_sync(self, mode: str, context_text: str) -> Dict[str, Any]:
        """Summarize voice check-in context into a structured JSON payload."""
        conversation = self._recent_voice_conversation_text(limit=8)
        context_text = context_text or "none"

        system_prompt = """You produce a concise JSON summary for a daily check-in.
Return JSON only. Do not include markdown or extra text.
Use Korean for all string values.

Required keys:
- mode (string)
- yesterday_summary (string)
- learnings (array of strings)
- emotion_state (string)
- emotion_rationale (string)
- team_tasks (array of strings)
- today_priorities (array of strings)
- next_actions (array of strings)

If unknown, use empty string or empty array."""

        user_prompt = f"""Context:\n{context_text}\n\nConversation:\n{conversation}\n\nReturn JSON now."""

        response = self.client.messages.create(
            model=self.model,
            system=system_prompt,
            max_tokens=400,
            messages=[{"role": "user", "content": user_prompt}],
        )

        assistant_text = ""
        for block in response.content:
            if hasattr(block, "text"):
                assistant_text += block.text

        parsed = self._extract_summary_json(assistant_text)
        if not isinstance(parsed, dict):
            return {
                "mode": mode,
                "yesterday_summary": "",
                "learnings": [],
                "emotion_state": "",
                "emotion_rationale": "",
                "team_tasks": [],
                "today_priorities": [],
                "next_actions": [],
            }

        parsed.setdefault("mode", mode)
        parsed.setdefault("learnings", [])
        parsed.setdefault("team_tasks", [])
        parsed.setdefault("today_priorities", [])
        parsed.setdefault("next_actions", [])
        return parsed

    def refine_voice_input_sync(self, transcript: str) -> str:
        """Clean ASR transcript into concise Korean text."""
        transcript = (transcript or "").strip()
        if not transcript:
            return ""

        system_prompt = """You are an ASR transcript cleaner.
Rules:
- Output only cleaned Korean text.
- Do not add new information.
- Fix spacing and punctuation.
- Keep numbers as written if unsure.
- No markdown."""

        response = self.client.messages.create(
            model=self.model,
            system=system_prompt,
            max_tokens=200,
            messages=[{"role": "user", "content": transcript}],
        )

        cleaned = ""
        for block in response.content:
            if hasattr(block, "text"):
                cleaned += block.text

        return cleaned.strip() or transcript

    # ========================================
    # Utility Methods
    # ========================================

    def chat_sync(
        self,
        user_message: str,
        mode: str = "exit",
        allow_tools: bool = True,
        context_text: Optional[str] = None,
        model_override: Optional[str] = None,
    ) -> str:
        """ë™ê¸° ë²„ì „ chat (ê°„ë‹¨í•œ ì‚¬ìš©)

        Args:
            user_message: ì‚¬ìš©ì ë©”ì‹œì§€
            mode: "exit" (Exit í”„ë¡œì ì…˜), "peer" (Peer PER ë¶„ì„), "diagnosis", "report"

        Returns:
            ì—ì´ì „íŠ¸ ì‘ë‹µ ë¬¸ìì—´
        """
        import asyncio
        import threading

        async def run():
            response = ""
            async for chunk in self.chat(
                user_message,
                mode=mode,
                allow_tools=allow_tools,
                context_text=context_text,
                model_override=model_override,
            ):
                response += chunk
            return response

        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            return asyncio.run(run())

        if loop.is_running():
            result: Dict[str, str] = {}

            def _runner():
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                try:
                    result["value"] = new_loop.run_until_complete(run())
                finally:
                    new_loop.close()

            thread = threading.Thread(target=_runner)
            thread.start()
            thread.join()
            return result.get("value", "")

        return loop.run_until_complete(run())

    def get_token_usage(self) -> Dict[str, Any]:
        """í† í° ì‚¬ìš©ëŸ‰ ë° ì˜ˆìƒ ë¹„ìš© ë°˜í™˜"""
        # Claude Opus 4.5 ê°€ê²© (2024ë…„ ê¸°ì¤€)
        INPUT_PRICE_PER_1M = 15.0   # $15 / 1M input tokens
        OUTPUT_PRICE_PER_1M = 75.0  # $75 / 1M output tokens

        input_cost = (self.token_usage["total_input_tokens"] / 1_000_000) * INPUT_PRICE_PER_1M
        output_cost = (self.token_usage["total_output_tokens"] / 1_000_000) * OUTPUT_PRICE_PER_1M
        total_cost = input_cost + output_cost

        return {
            "input_tokens": self.token_usage["total_input_tokens"],
            "output_tokens": self.token_usage["total_output_tokens"],
            "total_tokens": self.token_usage["total_input_tokens"] + self.token_usage["total_output_tokens"],
            "api_calls": self.token_usage["session_calls"],
            "estimated_cost_usd": round(total_cost, 4),
            "estimated_cost_krw": round(total_cost * 1400, 0)  # ëŒ€ëµì  í™˜ìœ¨
        }

    def reset_token_usage(self):
        """í† í° ì‚¬ìš©ëŸ‰ ì´ˆê¸°í™”"""
        self.token_usage = {
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "session_calls": 0
        }

    def reset(self):
        """ì„¸ì…˜ ì´ˆê¸°í™”"""
        self.conversation_history = []
        self.voice_conversation_history = []
        self.memory.start_new_session()
        self.memory.cached_results = {}
        self.context = {
            "analyzed_files": self.memory.session_metadata.get("analyzed_files", []),
            "cached_results": self.memory.cached_results,
            "last_analysis": None
        }
        self.reset_token_usage()
